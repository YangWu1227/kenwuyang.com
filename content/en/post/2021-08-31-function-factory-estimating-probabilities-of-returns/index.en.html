---
title: 'Estimating Probabilities of Asset Returns in R'
author: Yang Wu
date: '2021-08-31'
slug: function-factory-estimating-probabilities-of-returns
categories:
  - R
tags:
  - Finance
  - Data Visualization
  - Probability
  - Portfolio Analytics
subtitle: ''
summary: 'Using Base R function factory to estimate probabilities of asset returns'
authors: []
featured: no
image:
  caption: 'Photo by Ray_Shrewsberry on Pixabay'
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>
<script src="{{< blogdown/postref >}}index.en_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index.en_files/lightable/lightable.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#the-problem">The problem</a></li>
<li><a href="#not-theoretical-but-empirical">Not theoretical, but empirical</a></li>
<li><a href="#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li><a href="#estimating-probabilities-in-r">Estimating probabilities in R</a></li>
<li><a href="#some-words-of-caution">Some words of caution</a></li>
</ul>
</div>

<p>       In a previous <a href="https://www.kenwuyang.com/en/post/visualizing-asset-returns/">post</a>, we covered some techniques for visualizing financial data— specifically, asset returns— using various types of visualization and R graphics libraries. We explored line charts, histograms, and density plots, which are all very useful ways to visualize asset returns. Nevertheless, we were left with a question— <em>how could one estimate the probabilities of these asset returns?</em> We are interested in this question since stakeholders may need more information than what visualizations could provide. Simply put, stakeholders may need us to provide some ways to ascertain uncertainty, as difficult as this may seem, and we must answer the call.</p>
<hr />
<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>       The last time we examined financial data, we focused on a class of assets called the Exchange Traded Funds. We took a sample of daily prices from 2012-12-31 to 2021-7-31 and converted them to monthly log returns. This ultimately left us with a sample of 103 monthly log returns. The last section of my <a href="https://www.kenwuyang.com/en/post/visualizing-asset-returns/">post</a> on asset visualization ended with this following visualization:</p>
<p><img src="densities.png" width="100%" height="100%" style="display: block; margin: auto;" /></p>
<p>       We also established that <strong>the heights of the curve (the y-axis) do not represent probabilities.</strong> To convert to an actual probability, we need to find the area under the curve for a specific interval of returns. One way of accomplishing this task is to fit some type of probability distribution to our data, and the options are plentiful. The difficulty, however, is that data from the world of finance are often messy and none of these options are ever consistently adequate. The problem, as <a href="https://www.investopedia.com/contributors/46/">David Harper</a> once described in a blog post, is precisely this:</p>
<blockquote>
<p>Finance, a social science, is not as clean as physical sciences. Gravity, for example, has an elegant formula that we can depend on, time and again. Financial asset returns, on the other hand, cannot be replicated so consistently. A staggering amount of money has been lost over the years by clever people who confused the accurate distributions (i.e., as if derived from physical sciences) with the messy, unreliable approximations that try to depict financial returns. In finance, probability distributions are little more than crude pictorial representations.</p>
</blockquote>
<p>       I quote David’s blog post here simply to emphasize that it is generally difficult to capture uncertainty accurately and consistently in finance. But this is not a reason to give up, and, in my humble opinion, we certainly should not disregard all the mathematical and statistical models in <em>toto</em>. Why? Perhaps, it is because having some information is usually more helpful than having no information. As the aphorism goes:</p>
<blockquote>
<p>All models are wrong, but some are useful.</p>
</blockquote>
<p>In light of this spirit, we will try to tackle the problem set forth in my last post to the best of our abilities.</p>
<hr />
</div>
<div id="not-theoretical-but-empirical" class="section level2">
<h2>Not theoretical, but empirical</h2>
<p>       A common practice is to use the normal distribution to approximate returns data, but empirical evidence often suggest sub-optimal goodness-of-fit due to skewness and excess kurtosis. Examine the skewness measures and excess kurtosis of our sample of returns data:</p>
<p><strong>Skewness</strong></p>
<pre class="r"><code># Skewness
asset_returns_tq %&gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::skewness,
    method = &quot;moment&quot;
  )</code></pre>
<pre><code>       SPY        EFA        DIA        QQQ        AGG 
-0.7011322 -0.5213137 -0.7557169 -0.2351347 -0.0665614 </code></pre>
<p><strong>Excess Kurtosis</strong></p>
<pre class="r"><code># Excess Kurtosis
asset_returns_tq %&gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::kurtosis,
    method = &quot;excess&quot;
  )</code></pre>
<pre><code>      SPY       EFA       DIA       QQQ       AGG 
2.1505474 2.1227652 2.3110488 0.3138191 0.3451339 </code></pre>
<hr />
<p>       In addition, take a look at the results for a test of normality:</p>
<table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-6">Table 1: </span>Anderson-Darling Test of Normality
</caption>
<thead>
<tr>
<th style="text-align:center;">
SPY
</th>
<th style="text-align:center;">
EFA
</th>
<th style="text-align:center;">
DIA
</th>
<th style="text-align:center;">
QQQ
</th>
<th style="text-align:center;">
AGG
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
3e-04
</td>
<td style="text-align:center;">
0.0886
</td>
<td style="text-align:center;">
7e-04
</td>
<td style="text-align:center;">
0.129
</td>
<td style="text-align:center;">
0.6993
</td>
</tr>
</tbody>
</table>
<p>Controlling the Type I error rate at <span class="math inline">\(\alpha=0.1\)</span>, only the returns from the Invesco QQQ and iShares Core U.S. Aggregate Bond ETF’s could be considered normally distributed for this particular sample. All of the empirical evidence above suggest that, in practice, the normal distribution may not be a good fit for asset returns. Further, violation of the assumptions that the monthly returns are independently and identically distributed are frequent. Examine the shapes of the densities above, are there any good reasons to believe that these shapes are constant across time and that each random draw of monthly return will come from these <em>exact</em> distributions? Plus, we usually cannot argue <strong>a priori</strong> that monthly asset returns are independent.</p>
<p>       Various other theoretical distributions such as the log-normal distribution, beta distribution, t-distribution have been used as alternatives. To the extent that these theoretical distributions work well with specific random samples of returns data, it may be helpful to employ them in our estimations of probabilities. Nevertheless, we may choose a normal or a log-normal distribution and discover later on that it misled us on the likelihood of left-tail losses. Sometimes we employ a skewed distribution that fits the sample very well only to have the data in the next period prove us wrong. Consider the returns series for the SPDR Dow Jones Industrial Average ETF. Let us plot the skewness-kurtosis graph:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre><code>summary statistics
------
min:  -0.1464204   max:  0.1147059 
median:  0.01368791 
mean:  0.0113744 
estimated sd:  0.0398583 
estimated skewness:  -0.7669313 
estimated kurtosis:  5.487882 </code></pre>
<p>Examine the blue dot (labeled <strong>observation</strong>) in the plot above, does it look like our sample of returns is anywhere close to any of the theoretical distributions? The orange dots, which represent bootstrapped samples of the DIA returns series, are scattered all over the place; we simply cannot be certain if any of the theoretical distributions would be a good fit for our data. And we only have one sample in this case!</p>
<p>       So for all of the reasons above, I opt to approach the task of estimating returns probabilities empirically. This approach entails the use of the <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution function</a>, commonly referred to as the eCDF. One advantage of this nonparametric approach is that we are depending on our data for estimation. Simply put, an empirical distribution is determined by the sample, whereas a theoretical distribution can only determine samples drawn from it. When the parametric conditions of validity are far from being met, we have to use the data itself to create a cumulative distribution in order to estimate probabilities.</p>
<hr />
</div>
<div id="the-empirical-cumulative-distribution-function" class="section level2">
<h2>The Empirical Cumulative Distribution Function</h2>
<p>       Before we discuss the R implementations, however, a bit of theory on the eCDF and its properties certainly would not hurt. Suppose that <span class="math inline">\(x_{1}, \ldots, x_{n}\)</span> is a batch of observations (the word batch implies no commitment to an i.i.d stochastic model). The empirical cumulative distribution function is defined as</p>
<p><span class="math display">\[\begin{align*}
F_{n}(x)=\frac{1}{n}\left(\text{number of} x_{i} \leq x\right)
\end{align*}\]</span></p>
<p>Next, we order the batch of observations by <span class="math inline">\(x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}\)</span>, so</p>
<ul>
<li><p>if <span class="math inline">\(x&lt;x_{(1)}\)</span>, the probability is defined as <span class="math inline">\(F_{n}(x)=0\)</span></p></li>
<li><p>if <span class="math inline">\(x_{(1)} \leq x&lt;x_{(2)}\)</span>, the probability is defined as <span class="math inline">\(F_{n}(x)=\frac{1}{n}\)</span></p></li>
<li><p>if <span class="math inline">\(x_{(k)} \leq x&lt;x_{(k+1)}\)</span>, the probability is defined as <span class="math inline">\(F_{n}(x)=\frac{k}{n}\)</span></p></li>
</ul>
<p>If there is a single observation with value <span class="math inline">\(x\)</span>, then <span class="math inline">\(F_{n}\)</span> has a jump of height <span class="math inline">\(\frac{1}{n}\)</span> at <span class="math inline">\(x\)</span>; if there are <span class="math inline">\(t\)</span> observations with the same value <span class="math inline">\(x\)</span>, then <span class="math inline">\(F_{n}\)</span> has a jump of height <span class="math inline">\(\frac{t}{n}\)</span> at <span class="math inline">\(x\)</span>. The eCDF is analogue to the cumulative distribution function of a random variable in a sense— <span class="math inline">\(F(x)\)</span> gives the probability that <span class="math inline">\(X \leq x\)</span> and <span class="math inline">\(F_{n}(x)\)</span> gives the proportion of observations less than or equal to <span class="math inline">\(x\)</span>.</p>
<p>       In the case where <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span> is a random sample from a continuous distribution function, <span class="math inline">\(F\)</span>, we can express <span class="math inline">\(F_{n}\)</span> as follows:
<span class="math display">\[
F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I_{(-\infty, x]}\left(X_{i}\right)
\]</span>
where
<span class="math display">\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp; \text { if } X_{i} \leq x \\
0, &amp; \text { if } X_{i}&gt;x
\end{array}\right.
\]</span>
By the definition of CDF, the probability of <span class="math inline">\(X_{i}\leq x\)</span> is <span class="math inline">\(F(x)\)</span> and the probability of <span class="math inline">\(X_{i}&gt;x\)</span> is <span class="math inline">\(1-F(x)\)</span>. <strong>Note</strong>: <span class="math inline">\(F(x)\)</span> is the true unknown cdf we intend to estimate. The random variables <span class="math inline">\(I_{(-\infty, x]}\left(X_{i}\right)\)</span> are independent Bernoulli random variables:
<span class="math display">\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp; \text { with probability } F(x) \\
0, &amp; \text { with probability } 1-F(x)
\end{array}\right.
\]</span>
Thus, <span class="math inline">\(n F_{n}(x)\)</span> is a binomial random variable with
<span class="math display">\[
\begin{aligned}
E\left[F_{n}(x)\right] &amp;=F(x) \\
\operatorname{Var}\left[F_{n}(x)\right] &amp;=\frac{1}{n} F(x)[1-F(x)]
\end{aligned}
\]</span>
In other words, <span class="math inline">\(F_{n}(x)\)</span> is an unbiased estimator of <span class="math inline">\(F(x)\)</span> and it has a maximum variance at the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F(x)=.5\)</span>, which is the empirical median. As <span class="math inline">\(x\)</span> becomes very large or very small, the variance tends toward zero.</p>
<hr />
</div>
<div id="estimating-probabilities-in-r" class="section level2">
<h2>Estimating probabilities in R</h2>
<p>       The <code>stats</code> package provides a <a href="https://www.kenwuyang.com/post/function-factories-part-i/">function factory</a>, <code>ecdf</code>, that returns an empirical cumulative distribution function given a vector of observations. Let us create a manufactured function using the returns series of the SPDR Dow Jones Industrial Average ETF. As can be seen from earlier sections, the returns series for this ETF is far from normal, and, for that matter, from any other theoretical distributions.</p>
<pre class="r"><code># Create a function using the function factory
ecdf_DIA &lt;- stats::ecdf(x = asset_returns_tq[[&quot;DIA&quot;]])</code></pre>
<p>Next, we may plot the eCDF using <code>ggplot2</code>:</p>
<pre class="r"><code># Plot data
tibble::tibble(
  x = asset_returns_tq[[&quot;DIA&quot;]],
  y = ecdf_DIA(v = asset_returns_tq[[&quot;DIA&quot;]])
) %&gt;% 
  # Plot using geom_step
  ggplot(data = ., mapping = aes(x = x, y = y)) +
  geom_step(color = &quot;orange&quot;) +
  labs(title = &quot;Empirical Cumulative Distribution&quot;,
       x = &quot;Monthly Log Returns&quot;,
       y = latex2exp::TeX(string = &quot;$\\F_{n}(X)$&quot;)) + 
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = &quot;white&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-9-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>       The advantage of using a function factory is that we may now compute probabilities based on the eCDF. For instance, we may wish to know the probability that a monthly log return would fall below <span class="math inline">\(0\%\)</span>:</p>
<pre class="r"><code># Probability P(x &lt; 0.0)
ecdf_DIA(v = 0)</code></pre>
<pre><code>[1] 0.2815534</code></pre>
<p>In other words, there is about a <span class="math inline">\(28\%\)</span> chance that a monthly log return would fall below <span class="math inline">\(0\%\)</span>. The probabilities that monthly log returns would fall within other intervals of values could be computed using the same method. <strong>Note:</strong> We could never estimate the probability that a monthly log return would take on a <em>specific</em> value. Because asset (or portfolio) returns are continuous, the probability that a monthly log return would take on any one specific value is zero.</p>
<hr />
</div>
<div id="some-words-of-caution" class="section level2">
<h2>Some words of caution</h2>
<p>       Viola! We now have a method, though an imperfect one, for estimating probabilities of asset returns. The empirical approach is not without its limitations. For instance, it is usually the case that a large amount of data is needed to accurately estimate a distribution nonparametrically, especially a continuous one. In this post, we employ of sample of 103 monthly returns from 2012 to 2021. For better accuracy, we could certainly switch to weekly or even daily frequency. The availability of historical data also ensures that we can expand our sample easily.</p>
<p>       Still, some questions remain as we often need to make assumptions in order to interpolate between observed values of returns (What if we have yearly, weekly, or even daily frequencies?) and extrapolate outside the observed data range (What about the probabilities of returns intervals that are beyond the minimum and maximum values of our sample?). In addition, the reliability and convergence rate of the empirical approach in multivariate analyses decrease as data become more scattered in higher dimensions. Fortunately, in the context of asset returns, we often find ourselves in the case of univariate analysis.</p>
<p>       In short, there is no perfect approach to estimating probabilities of asset returns. And, in reality, the focus is often placed on analyzing portfolio returns. To this end, there are are many alternative ways to quantify uncertainty outside of simple probabilities— value-at-risk, expected shortfalls, downside deviation, etc. In this post, we have demonstrated that perhaps no approach is ever completely <em>correct</em> but we must always try our best to do what we can to bring value. Fortunately, there is a variety of topics to cover in those avenues and we will certainly tackle them in future posts.</p>
</div>
