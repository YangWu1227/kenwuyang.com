---
title: Statistical Factories in R (Part II)
author: Yang Wu
date: '2021-09-04'
slug: statistical-factories-part-ii
categories:
  - R
tags:
  - Application
  - Statistics
  - R Programming
subtitle: ''
summary: 'Application of R function factories in statistics'
authors: []
featured: no
image:
  caption: '<a href="https://commons.wikimedia.org/wiki/File:Sampling_with_replacement_and_out-of-bag_dataset.jpg">IlIlIIlIIIlIlll</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons'
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#box-cox-transformation">Box-Cox Transformation</a>
<ul>
<li><a href="#implementation-using-function-factories">Implementation using function factories</a></li>
</ul></li>
<li><a href="#bootstrap-generator">Bootstrap Generator</a>
<ul>
<li><a href="#implementation-using-function-factories-1">Implementation using function factories</a></li>
</ul></li>
<li><a href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a>
<ul>
<li><a href="#implementation-using-function-factories-2">Implementation using function factories</a></li>
</ul></li>
</ul>
</div>

<p>       In my <a href="https://www.kenwuyang.com/post/function-factories-part-i/">post on function factories</a>, we discussed the R data structure that powers function factories— environments. Now, we will focus on some applications of function factories in statistics. Again, the content of this post is inspired by <a href="https://adv-r.hadley.nz/">Advance R</a>; those who may be interested in learning more could turn to Hadley’s book for more information. Here we go!</p>
<hr />
<div id="box-cox-transformation" class="section level2">
<h2>Box-Cox Transformation</h2>
<p>       The Box-Cox procedure is used in statistical analysis to identify a transformation from the family of power transformations on a univariate variable, <span class="math inline">\(Y\)</span>, in order to address the skewness of its distribution. The family of power transformation is of the form:</p>
<p><span class="math display">\[\begin{align*}
Y^{\prime}=Y^{\lambda}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a parameter to be determined from the data or taken as given. For univariate data <span class="math inline">\(\{Y_{1}, Y_{2},...,Y_{n}\}\)</span>, the transformation has the following form:
<span class="math display">\[
Y_{i}(\lambda)=\left\{\begin{array}{ll}
\frac{Y_{i}^{\lambda}-1}{\lambda}, &amp;  \lambda \neq 0 \\
\log_{e} Y_{i}, &amp; \lambda=0
\end{array}\right.
\]</span></p>
<div id="implementation-using-function-factories" class="section level3">
<h3>Implementation using function factories</h3>
<p>       Here is the function factory:</p>
<pre class="r"><code>box_cox_factory &lt;- function(lambda) {
  if (!is.double(lambda)) {
    rlang::abort(message = &quot;The argument lambda must be a numeric vector.&quot;)
  }

  if (lambda == 0) {
    function(y) log(y, base = exp(1))
  } else {
    function(y) ((y^lambda) - 1) / lambda
  }
}</code></pre>
<p>Let us see it in action:</p>
<pre class="r"><code># Create a transformation function where lambda = 5
box_cox_5 &lt;- box_cox_factory(lambda = 5)
# Create a vector of non-normal data
y &lt;- rbeta(n = 500, shape1 = 6, shape2 = 1)
# Visualize
ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &quot;black&quot;, fill = &quot;orange&quot;
  ) +
  labs(title = &quot;Pre-transformation&quot;) +
  theme(
    panel.background = element_rect(fill = &quot;white&quot;),
    panel.grid = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code># After transformation
ggplot(data = tibble::tibble(&quot;new_y&quot; = box_cox_5(y)), mapping = aes(x = new_y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &quot;black&quot;, fill = &quot;orange&quot;
  ) +
  labs(title = &quot;Post-transformation&quot;) +
  theme(
    panel.background = element_rect(fill = &quot;white&quot;),
    panel.grid = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-2.png" width="576" style="display: block; margin: auto;" /></p>
<p>       This is by no means the <em>optimal</em> transformation, but it allows us to see the significant change to the distribution of our data. An added benefit to using the function factory is that we can visually explore how different values of <span class="math inline">\(\lambda\)</span> transform our data. This can be carried out with the help of <code>ggplot2::stat_function</code>:</p>
<pre class="r"><code>box_cox_plot_function &lt;- function(lambda) {
  # Change line color based on lambda value
  ggplot2::stat_function(
    mapping = aes(color = lambda),
    # Use our function factory, which will return different manufactured functions based on &quot;lambda&quot;
    fun = box_cox_factory(lambda = lambda),
    size = 1
  )
}</code></pre>
<p>Let us see how different values of <span class="math inline">\(\lambda=\{0, 1, 2, 3, 4, 5\}\)</span> change our data:</p>
<pre class="r"><code>ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  purrr::map(.x = c(0, 1, 2, 3, 4, 5), .f = box_cox_plot_function) +
  scale_color_viridis_c(limits = c(0, 5)) +
  labs(
    y = &quot;Transformed Y&quot;,
    x = &quot;Original Y&quot;
  ) +
  theme(
    panel.background = element_rect(fill = &quot;white&quot;),
    panel.grid = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-4-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>       The transformation can be applied in linear regression analysis as a remedial measure when model conditions— linearity, constancy of error variance— are not met. I will also cover this topic in my future posts as well.</p>
<hr />
</div>
</div>
<div id="bootstrap-generator" class="section level2">
<h2>Bootstrap Generator</h2>
<p>       The bootstrap is a powerful statistical technique that allows us to quantify the uncertainty associated with a given estimator or statistic. Ideally, to quantify the uncertainty of an estimator or statistic, we would obtain new samples of data from the population, obtaining estimators or statistics for each individual sample. In reality, however, obtaining repeated samples from a given population can be costly. The bootstrap method emulates the process of obtaining new samples, allowing us to estimate the variability of estimators or statistics without actually generating additional samples. <em>Rather than repeatedly obtaining independent samples from the population, the method instead obtains distinct samples by repeatedly sampling observations from the original sample.</em> The following diagram illustrates the essence of bootstrapping:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="bootstrap.png" alt="Diagram from [An Introduction to Statistical Learning](https://www.statlearning.com/)" width="489" />
<p class="caption">
Figure 1: Diagram from <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>
</p>
</div>
<p>       Let us examine an application. Suppose we have a data set with two variables <code>x</code> and <code>y</code>; we wish to fit the simple linear regression model (<span class="math inline">\(E\{Y\}=\beta_{0}+\beta_{1}X\)</span>) to the data. The point estimator we are interested is <span class="math inline">\(\hat{\beta}_{1}\)</span> and we wish to quantify the variability of this estimator. We can solve analytically for the point estimator of the variance of the sampling distribution of <span class="math inline">\(\hat{\beta}_{1}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\hat{\sigma}^{2}\{\hat{\hat{\beta}_{1}}\}=\frac{MSE}{\sum(X_{1}-\bar{X})^{2}}
\end{align*}\]</span></p>
<p>However, we are not satisfied with just one point estimator. We would like to generate bootstrap samples of the data, fit the simple regression model to each sample, and obtain a point estimator <span class="math inline">\(\hat{\beta}_{1}\)</span> for each of the models. Then, we will empirically obtain the variance of the sampling distribution of all of these estimators, <span class="math inline">\(\hat{\beta}_{1}\)</span>.</p>
<div id="implementation-using-function-factories-1" class="section level3">
<h3>Implementation using function factories</h3>
<p>       To solve this problem in R, we can combine function factories and functionals:</p>
<pre class="r"><code># Generate a random data frame
data &lt;- tibble::tibble(
  y = sample(x = 1:200, size = 100, replace = FALSE),
  x = sample(x = 1:200, size = 100, replace = FALSE)
)
# Find the point estimator of the variance of the sampling distribution of beta hat
lm(y ~ x, data = data) %&gt;%
  summary() %&gt;%
  purrr::pluck(.x = ., &quot;coefficients&quot;) %&gt;%
  `[`(4)</code></pre>
<pre><code>[1] 0.09828928</code></pre>
<p>Next, we create a function factory, and it takes as input a given data frame and a variable from which the bootstrap sample is to be generated. Ultimately, the function factory will return a manufactured function:</p>
<pre class="r"><code>bootstrap_factory &lt;- function(df, var) {

  # Initialize n
  n &lt;- nrow(df)
  # Force evaluation of var
  base::force(var)

  # Manufactured function
  function() {
    # Select the variables and modify them
    # Use &quot;drop&quot; to prevent data frame from being simplified to a matrix
    df[var] &lt;- df[var][sample(x = 1:n, replace = TRUE), , drop = FALSE]
    df
  }
}</code></pre>
<p>       The benefit of creating a function factory is that this bootstrap generator is data frame and variable-agnostic. If we wish to create a bootstrapping function for another data frame or other variables, we could simply create another manufactured function for that task. In the simple linear regression case with one independent variable, it may not be obvious why a function factory is beneficial. However, imagine now we have a 20-variable data frame that we need to bootstrap. This function factory will save us a lot of time in terms of copying-and-pasting, minimizing code length and the amount of typing. Next, let us generate 1000 bootstrapped data frames based on the original data frame:</p>
<pre class="r"><code># Create a list of 1000 data frames
# Each list element is a bootstrapped data frame
list_of_data_frames &lt;- purrr::map(
  .x = 1:1000,
  # Create a manufactured function
  .f = ~ bootstrap_factory(df = data, var = &quot;y&quot;)()
)
# Next fit the model to each of the bootstrapped data frames and extract the coefficient
vector_of_betas &lt;- list_of_data_frames %&gt;%
  # Fit the model to each of the 1000 data frames
  # This is a list of &quot;lm&quot; model objects
  purrr::map(.x = ., .f = ~ lm(y ~ x, data = .x)) %&gt;%
  # Now extract the estimator of the coefficient on x from each of the model summary output
  purrr::map_dbl(.x = ., .f = ~ stats::coef(.x)[[2]])</code></pre>
<p>       As can be seen, the whole process only takes about 6 lines of code. Here’s the sampling distribution of the estimator <span class="math inline">\(\hat{\beta}_{1}\)</span>:</p>
<pre class="r"><code>ggplot(data = tibble::tibble(x = vector_of_betas), mapping = aes(x = x)) +
  geom_histogram(
    binwidth = function(x) (max(x) - min(x)) / nclass.FD(x),
    color = &quot;black&quot;,
    fill = &quot;orange&quot;
  ) +
  theme(
    panel.background = element_rect(fill = &quot;white&quot;),
    panel.grid = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-10-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>And the standard deviation of this sampling distribution is 0.095925. Does this confirm our standard error calculation from earlier or refute it?</p>
<hr />
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2>Maximum Likelihood Estimation</h2>
<p>       Another application of function factories is the estimation of the parameters of the normal error regression model:</p>
<p><span class="math display">\[\begin{align*}
Y_{i}=\beta_{0}+\beta_{1}X_{i} + \varepsilon_{i}
\end{align*}\]</span></p>
<p>For this model, each <span class="math inline">\(Y_{i}\)</span> observation is normally distributed with mean <span class="math inline">\(E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. The method of maximum likelihood uses the density of the probability distribution at <span class="math inline">\(Y_{i}\)</span> as a measure of consistency for the observation <span class="math inline">\(Y_{i}\)</span>. In general, the density of an observation <span class="math inline">\(Y_{i}\)</span> for the normal error model is given as:</p>
<p><span class="math display">\[\begin{align*}
f_{i}=\frac{1}{\sqrt{2\sigma}}\text{exp}\bigg[-\frac{1}{2}\bigg(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma}\bigg)^2\bigg]
\end{align*}\]</span></p>
<p>       In R, this density can be computed using the <code>dnorm</code> function. We will specify the following variables:</p>
<ul>
<li><p><span class="math inline">\(y=Y_{i}\)</span></p></li>
<li><p><span class="math inline">\(\text{expected_y}=E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)</span></p></li>
<li><p><span class="math inline">\(\text{sigma}=\sigma\)</span></p></li>
</ul>
<pre class="r"><code>dnorm(x = y, mean = expected_y, sd = sigma)</code></pre>
<p><strong>The maximum likelihood function uses the product of the densities of all the <span class="math inline">\(Y_{i}\)</span> observations as the measure of consistency of the parameter values with the sample data.</strong> In other words, the likelihood function for <span class="math inline">\(n\)</span> observations <span class="math inline">\(Y_{1},Y_{2},...Y_{n}\)</span> is the product of the individual densities <span class="math inline">\(f_{i}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    L(\beta_{o},\beta_{1},\sigma)=\prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]
\end{align*}\]</span></p>
<p>We may work with the natural log of <span class="math inline">\(L(\beta_{o},\beta_{1},\sigma)\)</span> since both <span class="math inline">\(L\)</span> and <span class="math inline">\(\log_{e}L\)</span> are maximized for the same values of <span class="math inline">\(\beta_{o},\beta_{1},\)</span> and <span class="math inline">\(\sigma\)</span>. Plus, we can use the product rule such that the natural log of a product of is the sum of the natural logs, <span class="math inline">\(\ln(xyz)=\ln(x)+\ln(y)+\ln(z)\)</span>. Therefore, taking the natural log of the likelihood function means that the right hand side of the equation becomes the sum of the log densities:</p>
<p><span class="math display">\[\begin{align*}
    \log_{e}L(\beta_{o},\beta_{1},\sigma)=\sum_{i = 1}^{n}\log_{e}\Bigg[ \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]\Bigg]
\end{align*}\]</span></p>
<p>This will simplify our implementation in R.</p>
<div id="implementation-using-function-factories-2" class="section level3">
<h3>Implementation using function factories</h3>
<p>       For R implementation, we first need to generate some random data:</p>
<pre class="r"><code># Generate random data
data &lt;- tibble(
  x = rnorm(n = 150, mean = 2, sd = 24),
  y = rnorm(n = 150, mean = 45, sd = 7)
)</code></pre>
<p>For comparison, let us also obtain the estimators of the parameters using the least squares method:</p>
<pre class="r"><code># Model
model &lt;- lm(y ~ x, data = data) %&gt;%
  summary()
# Least squares estimators
purrr::pluck(.x = model, &quot;coefficients&quot;) %&gt;%
  `[`(c(1, 2))</code></pre>
<pre><code>[1] 45.34230635 -0.01160914</code></pre>
<pre class="r"><code># Sigma
purrr::pluck(.x = model, &quot;sigma&quot;)</code></pre>
<pre><code>[1] 7.75527</code></pre>
<p>Next, create the function factory:</p>
<pre class="r"><code>likelihood_factory &lt;- function(x, y) {

  # Initialize y and x
  y &lt;- force(y)
  x &lt;- force(x)

  # Manufactured function
  function(beta0, beta1, sigma) {

    # Linear regression model
    # The value of &quot;x&quot; is scoped from the enclosing environment of this manufactured function
    expected_y &lt;- beta0 + beta1 * x
    # Negative log-likelihood function
    # The value of &quot;y&quot; is scoped from the enclosing environment of this manufactured function
    # Negatively scale the log-likelihood values since we want to maximize
    -sum(dnorm(x = y, mean = expected_y, sd = sigma, log = TRUE))
  }
}</code></pre>
<p>       The advantage of creating a function factory is that we initialize “x” and “y” in the execution environment of <code>likelihood_factory</code>, which is the enclosing environment of the manufactured function and where it scopes for the values of “x” and “y”. Without the captured and encapsulated environment of a factory function, “x” and “y” will have to be stored in the global environment. Here they can be overwritten or deleted as well as interfere with other bindings. So, in a sense, the function factory here provides an extra layer of safeguarding. To find the maximum likelihood estimators, we supply a manufactured function to the <code>bbmle::mle2</code> function from the <code>bbmle</code> package:</p>
<pre class="r"><code>bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), 
            start = list(beta0 = 0, beta1 = 0, sigma = 50))</code></pre>
<pre><code>
Call:
bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), start = list(beta0 = 0, 
    beta1 = 0, sigma = 50))

Coefficients:
     beta0      beta1      sigma 
45.3423018 -0.0116091  7.7033944 

Log-likelihood: -519.09 </code></pre>
<p>How do the maximum likelihood estimators compare to those of the least squares method? (Hint: For normal data, they should be consistent with each other.)</p>
<hr />
<p>       And that is all for this post. We have covered three interesting applications of function factories in statistics. Among all of R’s functional programming tool-kits, function factories are perhaps the least utilized features as far as data science is concerned. This is likely because functional factories tend to have more of a mathematical or statistical flavor to them. However, as we have seen in this post and <a href="https://www.kenwuyang.com/post/function-factory-estimating-probabilities-of-returns/">others</a>, there is a class of problems, particular in mathematical statistics, to which function factories could offer elegant solutions in R. Therefore, having them in our R toolkit may certainly reap some long term benefits, allowing us to solve a variety of problems beyond those that are typically encountered in data analysis.</p>
</div>
</div>
