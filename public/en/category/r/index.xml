<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Yang (Ken) Wu</title>
    <link>https://www.kenwuyang.com/en/category/r/</link>
      <atom:link href="https://www.kenwuyang.com/en/category/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Yang Wu</copyright><lastBuildDate>Tue, 07 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.kenwuyang.com/media/Sharing.png</url>
      <title>R</title>
      <link>https://www.kenwuyang.com/en/category/r/</link>
    </image>
    
    <item>
      <title>Visualizing My Life In Months</title>
      <link>https://www.kenwuyang.com/en/post/my-life-in-months/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/my-life-in-months/</guid>
      <description>&lt;h3 id=&#34;how-much-time-do-i-have-left&#34;&gt;How much time do I have left?&lt;/h3&gt;
&lt;img src=&#34;my_life_in_months.png&#34; width=&#34;500&#34; height=&#34;250&#34;&gt;
&lt;p&gt;Inspired by &lt;a href=&#34;https://waitbutwhy.com/2014/05/life-weeks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tim Urban&lt;/a&gt; and &lt;a href=&#34;https://github.com/isabellabenabaye/life-chart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Isabella Benabaye&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Factories in R (Part II)</title>
      <link>https://www.kenwuyang.com/en/post/statistical-factories-part-ii/</link>
      <pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/statistical-factories-part-ii/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#box-cox-transformation&#34;&gt;Box-Cox Transformation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bootstrap-generator&#34;&gt;Bootstrap Generator&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories-1&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories-2&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In my &lt;a href=&#34;https://www.kenwuyang.com/en/post/function-factories-part-i/&#34;&gt;post on function factories&lt;/a&gt;, we discussed the R data structure that powers function factories— environments. Now, we will focus on some applications of function factories in statistics. Again, the content of this post is inspired by &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;; those who may be interested in learning more could turn to Hadley’s book for more information. Here we go!&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;box-cox-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Cox Transformation&lt;/h2&gt;
&lt;p&gt;       The Box-Cox procedure is used in statistical analysis to identify a transformation from the family of power transformations on a univariate variable, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, in order to address the skewness of its distribution. The family of power transformation is of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Y^{\prime}=Y^{\lambda}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a parameter to be determined from the data or taken as given. For univariate data &lt;span class=&#34;math inline&#34;&gt;\(\{Y_{1}, Y_{2},...,Y_{n}\}\)&lt;/span&gt;, the transformation has the following form:
&lt;span class=&#34;math display&#34;&gt;\[
Y_{i}(\lambda)=\left\{\begin{array}{ll}
\frac{Y_{i}^{\lambda}-1}{\lambda}, &amp;amp;  \lambda \neq 0 \\
\log_{e} Y_{i}, &amp;amp; \lambda=0
\end{array}\right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       Here is the function factory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;box_cox_factory &amp;lt;- function(lambda) {
  if (!is.double(lambda)) {
    rlang::abort(message = &amp;quot;The argument lambda must be a numeric vector.&amp;quot;)
  }

  if (lambda == 0) {
    function(y) log(y, base = exp(1))
  } else {
    function(y) ((y^lambda) - 1) / lambda
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a transformation function where lambda = 5
box_cox_5 &amp;lt;- box_cox_factory(lambda = 5)
# Create a vector of non-normal data
y &amp;lt;- rbeta(n = 500, shape1 = 6, shape2 = 1)
# Visualize
ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &amp;quot;black&amp;quot;, fill = &amp;quot;orange&amp;quot;
  ) +
  labs(title = &amp;quot;Pre-transformation&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# After transformation
ggplot(data = tibble::tibble(&amp;quot;new_y&amp;quot; = box_cox_5(y)), mapping = aes(x = new_y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &amp;quot;black&amp;quot;, fill = &amp;quot;orange&amp;quot;
  ) +
  labs(title = &amp;quot;Post-transformation&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       This is by no means the &lt;em&gt;optimal&lt;/em&gt; transformation, but it allows us to see the significant change to the distribution of our data. An added benefit of using the function factory is that we can visually explore how different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; transform our data. This can be carried out with the help of &lt;code&gt;ggplot2::stat_function&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;box_cox_plot_function &amp;lt;- function(lambda) {
  # Change line color based on lambda value
  ggplot2::stat_function(
    mapping = aes(color = lambda),
    # Use our function factory, which will return different manufactured functions based on &amp;quot;lambda&amp;quot;
    fun = box_cox_factory(lambda = lambda),
    size = 1
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see how different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\{0, 1, 2, 3, 4, 5\}\)&lt;/span&gt; change our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  purrr::map(.x = c(0, 1, 2, 3, 4, 5), .f = box_cox_plot_function) +
  scale_color_viridis_c(limits = c(0, 5)) +
  labs(
    y = &amp;quot;Transformed Y&amp;quot;,
    x = &amp;quot;Original Y&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The transformation can be applied in linear regression analysis as a remedial measure when model conditions— linearity, constancy of error variance— are not met. I will also cover this topic in my future posts as well.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-generator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrap Generator&lt;/h2&gt;
&lt;p&gt;       The bootstrap is a powerful statistical technique that allows us to quantify the uncertainty associated with a given estimator or statistic. Ideally, to quantify the uncertainty of an estimator or statistic, we would obtain new samples of data from the population, obtaining estimators or statistics for each individual sample. In reality, however, obtaining repeated samples from a given population can be costly. The bootstrap method emulates the process of obtaining new samples, allowing us to estimate the variability of estimators or statistics without actually generating additional samples. &lt;em&gt;Rather than repeatedly obtaining independent samples from the population, the method instead obtains distinct samples by repeatedly sampling observations from the original sample.&lt;/em&gt; The following diagram illustrates the essence of bootstrapping:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;bootstrap.png&#34; alt=&#34;Diagram from [An Introduction to Statistical Learning](https://www.statlearning.com/)&#34; width=&#34;489&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Diagram from &lt;a href=&#34;https://www.statlearning.com/&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;       Let us examine an application. Suppose we have a data set with two variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;; we wish to fit the simple linear regression model (&lt;span class=&#34;math inline&#34;&gt;\(E\{Y\}=\beta_{0}+\beta_{1}X\)&lt;/span&gt;) to the data. The point estimator we are interested is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt; and we wish to quantify the variability of this estimator. We can solve analytically for the point estimator of the variance of the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat{\sigma}^{2}\{\hat{\hat{\beta}_{1}}\}=\frac{MSE}{\sum(X_{1}-\bar{X})^{2}}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, we are not satisfied with just one point estimator. We would like to generate bootstrap samples of the data, fit the simple regression model to each sample, and obtain a point estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt; for each of the models. Then, we will empirically obtain the variance of the sampling distribution of all of these estimators, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       To solve this problem in R, we can combine function factories and functionals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate a random data frame
data &amp;lt;- tibble::tibble(
  y = sample(x = 1:200, size = 100, replace = FALSE),
  x = sample(x = 1:200, size = 100, replace = FALSE)
)
# Find the point estimator of the variance of the sampling distribution of beta hat
lm(y ~ x, data = data) %&amp;gt;%
  summary() %&amp;gt;%
  purrr::pluck(.x = ., &amp;quot;coefficients&amp;quot;) %&amp;gt;%
  `[`(4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1024499&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create a function factory, and it takes as input a given data frame and a variable from which the bootstrap sample is to be generated. Ultimately, the function factory will return a manufactured function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bootstrap_factory &amp;lt;- function(df, var) {

  # Initialize n
  n &amp;lt;- nrow(df)
  # Force evaluation of var
  base::force(var)

  # Manufactured function
  function() {
    # Select the variables and modify them
    # Use &amp;quot;drop&amp;quot; to prevent data frame from being simplified to a matrix
    df[var] &amp;lt;- df[var][sample(x = 1:n, replace = TRUE), , drop = FALSE]
    df
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       The benefit of creating a function factory is that this bootstrap generator is data frame and variable-agnostic. If we wish to create a bootstrapping function for another data frame or other variables, we could simply create another manufactured function for that task. In the simple linear regression case with one independent variable, it may not be obvious why a function factory is beneficial. However, imagine now we have a 20-variable data frame that we need to bootstrap. This function factory will save us a lot of time in terms of copying-and-pasting, minimizing code length and the amount of typing. Next, let us generate 1000 bootstrapped data frames based on the original data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a list of 1000 data frames
# Each list element is a bootstrapped data frame
list_of_data_frames &amp;lt;- purrr::map(
  .x = 1:1000,
  # Create a manufactured function
  .f = ~ bootstrap_factory(df = data, var = &amp;quot;y&amp;quot;)()
)
# Next fit the model to each of the bootstrapped data frames and extract the coefficient
vector_of_betas &amp;lt;- list_of_data_frames %&amp;gt;%
  # Fit the model to each of the 1000 data frames
  # This is a list of &amp;quot;lm&amp;quot; model objects
  purrr::map(.x = ., .f = ~ lm(y ~ x, data = .x)) %&amp;gt;%
  # Now extract the estimator of the coefficient on x from each of the model summary output
  purrr::map_dbl(.x = ., .f = ~ stats::coef(.x)[[2]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       As can be seen, the whole process only takes about 6 lines of code. Here’s the sampling distribution of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble::tibble(x = vector_of_betas), mapping = aes(x = x)) +
  geom_histogram(
    binwidth = function(x) (max(x) - min(x)) / nclass.FD(x),
    color = &amp;quot;black&amp;quot;,
    fill = &amp;quot;orange&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the standard deviation of this sampling distribution is 0.102395. Does this confirm our standard error calculation from earlier or refute it?&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;       Another application of function factories is the estimation of the parameters of the normal error regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Y_{i}=\beta_{0}+\beta_{1}X_{i} + \varepsilon_{i}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this model, each &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; observation is normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The method of maximum likelihood uses the density of the probability distribution at &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; as a measure of consistency for the observation &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt;. In general, the density of an observation &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; for the normal error model is given as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
f_{i}=\frac{1}{\sqrt{2\sigma}}\text{exp}\bigg[-\frac{1}{2}\bigg(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma}\bigg)^2\bigg]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;       In R, this density can be computed using the &lt;code&gt;dnorm&lt;/code&gt; function. We will specify the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y=Y_{i}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{expected_y}=E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{sigma}=\sigma\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dnorm(x = y, mean = expected_y, sd = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The maximum likelihood function uses the product of the densities of all the &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; observations as the measure of consistency of the parameter values with the sample data.&lt;/strong&gt; In other words, the likelihood function for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations &lt;span class=&#34;math inline&#34;&gt;\(Y_{1},Y_{2},...Y_{n}\)&lt;/span&gt; is the product of the individual densities &lt;span class=&#34;math inline&#34;&gt;\(f_{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    L(\beta_{o},\beta_{1},\sigma)=\prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may work with the natural log of &lt;span class=&#34;math inline&#34;&gt;\(L(\beta_{o},\beta_{1},\sigma)\)&lt;/span&gt; since both &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log_{e}L\)&lt;/span&gt; are maximized for the same values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{o},\beta_{1},\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Plus, we can use the product rule such that the natural log of a product of is the sum of the natural logs, &lt;span class=&#34;math inline&#34;&gt;\(\ln(xyz)=\ln(x)+\ln(y)+\ln(z)\)&lt;/span&gt;. Therefore, taking the natural log of the likelihood function means that the right hand side of the equation becomes the sum of the log densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \log_{e}L(\beta_{o},\beta_{1},\sigma)=\sum_{i = 1}^{n}\log_{e}\Bigg[ \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]\Bigg]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will simplify our implementation in R.&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       For R implementation, we first need to generate some random data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate random data
data &amp;lt;- tibble(
  x = rnorm(n = 150, mean = 2, sd = 24),
  y = rnorm(n = 150, mean = 45, sd = 7)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For comparison, let us also obtain the estimators of the parameters using the least squares method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model
model &amp;lt;- lm(y ~ x, data = data) %&amp;gt;%
  summary()
# Least squares estimators
purrr::pluck(.x = model, &amp;quot;coefficients&amp;quot;) %&amp;gt;%
  `[`(c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 44.572055917  0.001569285&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sigma
purrr::pluck(.x = model, &amp;quot;sigma&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7.168166&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, create the function factory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;likelihood_factory &amp;lt;- function(x, y) {

  # Initialize y and x
  y &amp;lt;- force(y)
  x &amp;lt;- force(x)

  # Manufactured function
  function(beta0, beta1, sigma) {

    # Linear regression model
    # The value of &amp;quot;x&amp;quot; is scoped from the enclosing environment of this manufactured function
    expected_y &amp;lt;- beta0 + beta1 * x
    # Negative log-likelihood function
    # The value of &amp;quot;y&amp;quot; is scoped from the enclosing environment of this manufactured function
    # Negatively scale the log-likelihood values since we want to maximize
    -sum(dnorm(x = y, mean = expected_y, sd = sigma, log = TRUE))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       The advantage of creating a function factory is that we initialize “x” and “y” in the execution environment of &lt;code&gt;likelihood_factory&lt;/code&gt;, which is the enclosing environment of the manufactured function and where it scopes for the values of “x” and “y”. Without the captured and encapsulated environment of a factory function, “x” and “y” will have to be stored in the global environment. Here they can be overwritten or deleted as well as interfere with other bindings. So, in a sense, the function factory here provides an extra layer of safeguarding. To find the maximum likelihood estimators, we supply a manufactured function to the &lt;code&gt;bbmle::mle2&lt;/code&gt; function from the &lt;code&gt;bbmle&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), 
            start = list(beta0 = 0, beta1 = 0, sigma = 50))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), start = list(beta0 = 0, 
    beta1 = 0, sigma = 50))

Coefficients:
       beta0        beta1        sigma 
44.572057288  0.001569382  7.120216813 

Log-likelihood: -507.28 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do the maximum likelihood estimators compare to those of the least squares method? (Hint: For normal data, they should be consistent with each other.)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;       And that is all for this post. We have covered three interesting applications of function factories in statistics. Among all of R’s functional programming tool-kits, function factories are perhaps the least utilized features as far as data science is concerned. This is likely because functional factories tend to have more of a mathematical or statistical flavor to them. However, as we have seen in this post and &lt;a href=&#34;https://www.kenwuyang.com/post/function-factory-estimating-probabilities-of-returns/&#34;&gt;others&lt;/a&gt;, there is a class of problems, particular in mathematical statistics, to which function factories could offer elegant solutions in R. Therefore, having them in our R toolkit may certainly reap some long term benefits, allowing us to solve a variety of problems beyond those that are typically encountered in data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating Probabilities of Asset Returns in R</title>
      <link>https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-problem&#34;&gt;The problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#not-theoretical-but-empirical&#34;&gt;Not theoretical, but empirical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-empirical-cumulative-distribution-function&#34;&gt;The Empirical Cumulative Distribution Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-probabilities-in-r&#34;&gt;Estimating probabilities in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-words-of-caution&#34;&gt;Some words of caution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In a previous &lt;a href=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/&#34;&gt;post&lt;/a&gt;, we covered some techniques for visualizing financial data— specifically, asset returns— using various types of visualization and R graphics libraries. We explored line charts, histograms, and density plots, which are all very useful ways to visualize asset returns. Nevertheless, we were left with a question— &lt;em&gt;how could one estimate the probabilities of these asset returns?&lt;/em&gt; We are interested in this question since stakeholders may need more information than what visualizations could provide. Simply put, stakeholders may need us to provide some ways to ascertain uncertainty, as difficult as this may seem, and we must answer the call.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;       The last time we examined financial data, we focused on a class of assets called the Exchange Traded Funds. We took a sample of daily prices from 2012-12-31 to 2021-7-31 and converted them to monthly log returns. This ultimately left us with a sample of 103 monthly log returns. The last section of my &lt;a href=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/&#34;&gt;post&lt;/a&gt; on asset visualization ended with this following visualization:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;densities.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       We also established that &lt;strong&gt;the heights of the curve (the y-axis) do not represent probabilities.&lt;/strong&gt; To convert to an actual probability, we need to find the area under the curve for a specific interval of returns. One way of accomplishing this task is to fit some type of probability distribution to our data, and the options are plentiful. The difficulty, however, is that data from the world of finance are often messy and none of these options are ever consistently adequate. The problem, as &lt;a href=&#34;https://www.investopedia.com/contributors/46/&#34;&gt;David Harper&lt;/a&gt; once described in a blog post, is precisely this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finance, a social science, is not as clean as physical sciences. Gravity, for example, has an elegant formula that we can depend on, time and again. Financial asset returns, on the other hand, cannot be replicated so consistently. A staggering amount of money has been lost over the years by clever people who confused the accurate distributions (i.e., as if derived from physical sciences) with the messy, unreliable approximations that try to depict financial returns. In finance, probability distributions are little more than crude pictorial representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;       I quote David’s blog post here simply to emphasize that it is generally difficult to capture uncertainty accurately and consistently in finance. But this is not a reason to give up, and, in my humble opinion, we certainly should not disregard all the mathematical and statistical models in &lt;em&gt;toto&lt;/em&gt;. Why? Perhaps, it is because having some information is usually more helpful than having no information. As the aphorism goes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All models are wrong, but some are useful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In light of this spirit, we will try to tackle the problem set forth in my last post to the best of our abilities.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;not-theoretical-but-empirical&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Not theoretical, but empirical&lt;/h2&gt;
&lt;p&gt;       A common practice is to use the normal distribution to approximate returns data, but empirical evidence often suggest sub-optimal goodness-of-fit due to skewness and excess kurtosis. Examine the skewness measures and excess kurtosis of our sample of returns data:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skewness&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Skewness
asset_returns_tq %&amp;gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::skewness,
    method = &amp;quot;moment&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       SPY        EFA        DIA        QQQ        AGG 
-0.7011322 -0.5213137 -0.7557169 -0.2351347 -0.0665614 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Excess Kurtosis&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Excess Kurtosis
asset_returns_tq %&amp;gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::kurtosis,
    method = &amp;quot;excess&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      SPY       EFA       DIA       QQQ       AGG 
2.1505474 2.1227652 2.3110488 0.3138191 0.3451339 &lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;       In addition, take a look at the results for a test of normality:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-6&#34;&gt;Table 1: &lt;/span&gt;Anderson-Darling Test of Normality
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
SPY
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
EFA
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
DIA
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
QQQ
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
AGG
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3e-04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0886
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7e-04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.129
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.6993
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Controlling the Type I error rate at &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1\)&lt;/span&gt;, only the returns from the Invesco QQQ and iShares Core U.S. Aggregate Bond ETF’s could be considered normally distributed for this particular sample. All of the empirical evidence above suggest that, in practice, the normal distribution may not be a good fit for asset returns. Further, violation of the assumptions that the monthly returns are independently and identically distributed are frequent. Examine the shapes of the densities above, are there any good reasons to believe that these shapes are constant across time and that each random draw of monthly return will come from these &lt;em&gt;exact&lt;/em&gt; distributions? Plus, we usually cannot argue &lt;strong&gt;a priori&lt;/strong&gt; that monthly asset returns are independent.&lt;/p&gt;
&lt;p&gt;       Various other theoretical distributions such as the log-normal distribution, beta distribution, t-distribution have been used as alternatives. To the extent that these theoretical distributions work well with specific random samples of returns data, it may be helpful to employ them in our estimations of probabilities. Nevertheless, we may choose a normal or a log-normal distribution and discover later on that it misled us on the likelihood of left-tail losses. Sometimes we employ a skewed distribution that fits the sample very well only to have the data in the next period prove us wrong. Consider the returns series for the SPDR Dow Jones Industrial Average ETF. Let us plot the skewness-kurtosis graph:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;summary statistics
------
min:  -0.1464204   max:  0.1147059 
median:  0.01368791 
mean:  0.0113744 
estimated sd:  0.0398583 
estimated skewness:  -0.7669313 
estimated kurtosis:  5.487882 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examine the blue dot (labeled &lt;strong&gt;observation&lt;/strong&gt;) in the plot above, does it look like our sample of returns is anywhere close to any of the theoretical distributions? The orange dots, which represent bootstrapped samples of the DIA returns series, are scattered all over the place; we simply cannot be certain if any of the theoretical distributions would be a good fit for our data. And we only have one sample in this case!&lt;/p&gt;
&lt;p&gt;       So for all of the reasons above, I opt to approach the task of estimating returns probabilities empirically. This approach entails the use of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Empirical_distribution_function&#34;&gt;empirical distribution function&lt;/a&gt;, commonly referred to as the eCDF. One advantage of this nonparametric approach is that we are depending on our data for estimation. Simply put, an empirical distribution is determined by the sample, whereas a theoretical distribution can only determine samples drawn from it. When the parametric conditions of validity are far from being met, we have to use the data itself to create a cumulative distribution in order to estimate probabilities.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;the-empirical-cumulative-distribution-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Empirical Cumulative Distribution Function&lt;/h2&gt;
&lt;p&gt;       Before we discuss the R implementations, however, a bit of theory on the eCDF and its properties certainly would not hurt. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(x_{1}, \ldots, x_{n}\)&lt;/span&gt; is a batch of observations (the word batch implies no commitment to an i.i.d stochastic model). The empirical cumulative distribution function is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
F_{n}(x)=\frac{1}{n}\left(\text{number of} x_{i} \leq x\right)
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next, we order the batch of observations by &lt;span class=&#34;math inline&#34;&gt;\(x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}\)&lt;/span&gt;, so&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x&amp;lt;x_{(1)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x_{(1)} \leq x&amp;lt;x_{(2)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=\frac{1}{n}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x_{(k)} \leq x&amp;lt;x_{(k+1)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=\frac{k}{n}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there is a single observation with value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; has a jump of height &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;; if there are &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; observations with the same value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; has a jump of height &lt;span class=&#34;math inline&#34;&gt;\(\frac{t}{n}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The eCDF is analogue to the cumulative distribution function of a random variable in a sense— &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; gives the probability that &lt;span class=&#34;math inline&#34;&gt;\(X \leq x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)\)&lt;/span&gt; gives the proportion of observations less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;       In the case where &lt;span class=&#34;math inline&#34;&gt;\(X_{1}, \ldots, X_{n}\)&lt;/span&gt; is a random sample from a continuous distribution function, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, we can express &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; as follows:
&lt;span class=&#34;math display&#34;&gt;\[
F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I_{(-\infty, x]}\left(X_{i}\right)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp;amp; \text { if } X_{i} \leq x \\
0, &amp;amp; \text { if } X_{i}&amp;gt;x
\end{array}\right.
\]&lt;/span&gt;
By the definition of CDF, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}\leq x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; and the probability of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}&amp;gt;x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1-F(x)\)&lt;/span&gt;. &lt;strong&gt;Note&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; is the true unknown cdf we intend to estimate. The random variables &lt;span class=&#34;math inline&#34;&gt;\(I_{(-\infty, x]}\left(X_{i}\right)\)&lt;/span&gt; are independent Bernoulli random variables:
&lt;span class=&#34;math display&#34;&gt;\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp;amp; \text { with probability } F(x) \\
0, &amp;amp; \text { with probability } 1-F(x)
\end{array}\right.
\]&lt;/span&gt;
Thus, &lt;span class=&#34;math inline&#34;&gt;\(n F_{n}(x)\)&lt;/span&gt; is a binomial random variable with
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E\left[F_{n}(x)\right] &amp;amp;=F(x) \\
\operatorname{Var}\left[F_{n}(x)\right] &amp;amp;=\frac{1}{n} F(x)[1-F(x)]
\end{aligned}
\]&lt;/span&gt;
In other words, &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)\)&lt;/span&gt; is an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; and it has a maximum variance at the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(F(x)=.5\)&lt;/span&gt;, which is the empirical median. As &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; becomes very large or very small, the variance tends toward zero.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-probabilities-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating probabilities in R&lt;/h2&gt;
&lt;p&gt;       The &lt;code&gt;stats&lt;/code&gt; package provides a &lt;a href=&#34;https://www.kenwuyang.com/post/function-factories-part-i/&#34;&gt;function factory&lt;/a&gt;, &lt;code&gt;ecdf&lt;/code&gt;, that returns an empirical cumulative distribution function given a vector of observations. Let us create a manufactured function using the returns series of the SPDR Dow Jones Industrial Average ETF. As can be seen from earlier sections, the returns series for this ETF is far from normal, and, for that matter, from any other theoretical distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a function using the function factory
ecdf_DIA &amp;lt;- stats::ecdf(x = asset_returns_tq[[&amp;quot;DIA&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we may plot the eCDF using &lt;code&gt;ggplot2&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
tibble::tibble(
  x = asset_returns_tq[[&amp;quot;DIA&amp;quot;]],
  y = ecdf_DIA(v = asset_returns_tq[[&amp;quot;DIA&amp;quot;]])
) %&amp;gt;% 
  # Plot using geom_step
  ggplot(data = ., mapping = aes(x = x, y = y)) +
  geom_step(color = &amp;quot;orange&amp;quot;) +
  labs(title = &amp;quot;Empirical Cumulative Distribution&amp;quot;,
       x = &amp;quot;Monthly Log Returns&amp;quot;,
       y = latex2exp::TeX(string = &amp;quot;$\\F_{n}(X)$&amp;quot;)) + 
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = &amp;quot;white&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The advantage of using a function factory is that we may now compute probabilities based on the eCDF. For instance, we may wish to know the probability that a monthly log return would fall below &lt;span class=&#34;math inline&#34;&gt;\(0\%\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Probability P(x &amp;lt; 0.0)
ecdf_DIA(v = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2815534&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, there is about a &lt;span class=&#34;math inline&#34;&gt;\(28\%\)&lt;/span&gt; chance that a monthly log return would fall below &lt;span class=&#34;math inline&#34;&gt;\(0\%\)&lt;/span&gt;. The probabilities that monthly log returns would fall within other intervals of values could be computed using the same method. &lt;strong&gt;Note:&lt;/strong&gt; We could never estimate the probability that a monthly log return would take on a &lt;em&gt;specific&lt;/em&gt; value. Because asset (or portfolio) returns are continuous, the probability that a monthly log return would take on any one specific value is zero.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;some-words-of-caution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some words of caution&lt;/h2&gt;
&lt;p&gt;       Viola! We now have a method, though an imperfect one, for estimating probabilities of asset returns. The empirical approach is not without its limitations. For instance, it is usually the case that a large amount of data is needed to accurately estimate a distribution nonparametrically, especially a continuous one. In this post, we employ of sample of 103 monthly returns from 2012 to 2021. For better accuracy, we could certainly switch to weekly or even daily frequency. The availability of historical data also ensures that we can expand our sample easily.&lt;/p&gt;
&lt;p&gt;       Still, some questions remain as we often need to make assumptions in order to interpolate between observed values of returns (What if we have yearly, weekly, or even daily frequencies?) and extrapolate outside the observed data range (What about the probabilities of returns intervals that are beyond the minimum and maximum values of our sample?). In addition, the reliability and convergence rate of the empirical approach in multivariate analyses decrease as data become more scattered in higher dimensions. Fortunately, in the context of asset returns, we often find ourselves in the case of univariate analysis.&lt;/p&gt;
&lt;p&gt;       In short, there is no perfect approach to estimating probabilities of asset returns. And, in reality, the focus is often placed on analyzing portfolio returns. To this end, there are are many alternative ways to quantify uncertainty outside of simple probabilities— value-at-risk, expected shortfalls, downside deviation, etc. In this post, we have demonstrated that perhaps no approach is ever completely &lt;em&gt;correct&lt;/em&gt; but we must always try our best to do what we can to bring value. Fortunately, there is a variety of topics to cover in those avenues and we will certainly tackle them in future posts.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Function Factories in R (Part I)</title>
      <link>https://www.kenwuyang.com/en/post/function-factories-part-i/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/function-factories-part-i/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/function-factories-part-i/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-makes-a-function-factory-special&#34;&gt;What makes a function factory special?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#diagrams&#34;&gt;Diagrams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#execution-environment-of-a-manufactured-function&#34;&gt;Execution environment of a manufactured function&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#diagram&#34;&gt;Diagram&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;         Recently, I came across a concept called &lt;strong&gt;function factory&lt;/strong&gt; in &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;, which allows one to create functions that return other functions as outputs. Why? I guess it is partly because R is at its heart a &lt;a href=&#34;https://www.oreilly.com/library/view/r-in-a/9781449358204/ch10.html&#34;&gt;functional programming language&lt;/a&gt;. And, the function factory actually has many useful applications, particularly when it is used in tandem with the &lt;code&gt;tidyverse&lt;/code&gt;’s graphics library &lt;code&gt;ggplot2&lt;/code&gt; and when it is used to implement statistical theories.&lt;/p&gt;
&lt;p&gt;         In this post, I want to document some of what I have learned in reading about the function factory and its cool applications. To accomplish that, a bit of theory on the R environments is required. The content of this post can be found in chapter 10 of &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;. The function factory is kind of a quirky concept that I believe is part of what makes R unique, and so I recommend reading more about it in Hadley’s book. With that being said, let us get started.&lt;/p&gt;
&lt;div id=&#34;what-makes-a-function-factory-special&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What makes a function factory special?&lt;/h1&gt;
&lt;p&gt;         The underlying key idea behind function factories can be succinctly expressed as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The enclosing environment of the manufactured function is an execution environment of the function factory — Hadley Wickham&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;         We can understand this better using an example. Suppose we have a function factory called &lt;code&gt;power&lt;/code&gt;, which returns manufactured functions that raise their input arguments to different powers, contingent on the &lt;code&gt;exp&lt;/code&gt; argument supplied to the factory function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power &amp;lt;- function(exp) {
  
  # Force the evaluation of &amp;quot;exp&amp;quot;
  base::force(exp)
  
  # This is the last evaluated expression and so the function returns another function
  # It takes an input &amp;quot;x&amp;quot; and raises it to the power of &amp;quot;exp&amp;quot;
  # The argument &amp;quot;exp&amp;quot; is supplied as an argument to &amp;quot;power&amp;quot;
  function(x) {
    x ^ exp
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By varying on our inputs to &lt;code&gt;exp&lt;/code&gt; via &lt;code&gt;power&lt;/code&gt;, we can create a class of functions that behave differently:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;square &amp;lt;- power(exp = 2)
cube &amp;lt;- power(exp = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;square(x = 32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1024&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cube(x = 9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 729&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, these functions are exactly how we would expect them to behave. But why do they behave differently? From the looks of it, their function bodies are exactly the same:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;function(x) {
    x ^ exp
  }
&amp;lt;environment: 0x7f87b76e7148&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cube&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;function(x) {
    x ^ exp
  }
&amp;lt;bytecode: 0x7f87bb1e03e0&amp;gt;
&amp;lt;environment: 0x7f87b7e6d450&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;         It turns out that the most important difference between these functions is that they have different enclosing or function environments (see the different environment addresses in the output above). The enclosing environments control how these functions scope for values of &lt;code&gt;exp&lt;/code&gt;. Each time a function is executed in R, a new &lt;strong&gt;execution&lt;/strong&gt; environment is created to host its execution. The first time we executed &lt;code&gt;power&lt;/code&gt; to create &lt;code&gt;square&lt;/code&gt;, an environment is created. Then, another execution environment of &lt;code&gt;power&lt;/code&gt; is created when we generated &lt;code&gt;cube&lt;/code&gt;. These two execution environments are the &lt;strong&gt;enclosing environments&lt;/strong&gt; of &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt;. The &lt;code&gt;rlang&lt;/code&gt; package contains functions that provide more insights into these relationships:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the environments associated with the function square
rlang::env_print(env = square)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;environment: 0x7f87b76e7148&amp;gt;
parent: &amp;lt;environment: global&amp;gt;
bindings:
 * exp: &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the environments associated with the function cube
rlang::env_print(env = cube)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;environment: 0x7f87b7e6d450&amp;gt;
parent: &amp;lt;environment: global&amp;gt;
bindings:
 * exp: &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;         As can be seen, there are two different environment addresses associated with these two functions, each of which was an execution environment of &lt;code&gt;power&lt;/code&gt;. These environments have the same parent— the enclosing or function environment of &lt;code&gt;power&lt;/code&gt;, which is also the global environment in which &lt;code&gt;power&lt;/code&gt; was created. Both the environments for &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt; contain a binding to &lt;code&gt;exp&lt;/code&gt;; we can access the value of &lt;code&gt;exp&lt;/code&gt; to see ultimately why these two functions behave differently:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the value of &amp;quot;exp&amp;quot; in the enclosing environment of square
rlang::fn_env(fn = square)[[&amp;quot;exp&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the value of &amp;quot;exp&amp;quot; in the enclosing environment of cube
rlang::fn_env(fn = cube)[[&amp;quot;exp&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;         So, in short, &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt; behave differently since the names &lt;code&gt;exp&lt;/code&gt; in their enclosing environments are bound to different values.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;diagrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagrams&lt;/h2&gt;
&lt;p&gt;         These relationships between a function factory and its manufactured functions can be analyzed diagrammatically. The components of the diagram below are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The boat-like structures are the functions, i.e. &lt;code&gt;power&lt;/code&gt;, &lt;code&gt;square&lt;/code&gt;, and &lt;code&gt;cube&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The global environment is represented by the rectangular shape labeled “R_GlobalEnv.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The execution environments of &lt;code&gt;power&lt;/code&gt; or, equivalently, the enclosing environments of &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt; are symbolized by the grey boxes each of which has a binding from the name &lt;code&gt;exp&lt;/code&gt; to a double vector object.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The black arrows indicate &lt;strong&gt;bindings&lt;/strong&gt; either from names to objects— be it a function object like &lt;code&gt;power&lt;/code&gt; or a vector object like 2— or from a function object to its enclosing environment— be it when &lt;code&gt;power&lt;/code&gt; binds its enclosing global environment or when &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt; bind their respective enclosing environments.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The blue arrows indicate relationships between environments and their parent environments. The blue arrow always goes in a one-way direction from the child environment to the parent environment.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;function_fact1.png&#34; alt=&#34;Diagram from [Advance R](https://adv-r.hadley.nz/function-factories.html)&#34; width=&#34;442&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Diagram from &lt;a href=&#34;https://adv-r.hadley.nz/function-factories.html&#34;&gt;Advance R&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the diagram, the relationships can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The function &lt;code&gt;power&lt;/code&gt; (bottom-right) binds the global environment, which is where it was created. The global environment has a binding from the name “power1” to the function object (boat-like structure). These two bindings are made clear in the diagram by the black arrows &lt;em&gt;pointing&lt;/em&gt; from &lt;code&gt;power&lt;/code&gt; to global and from global to &lt;code&gt;power&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The global environment has in its bag two other bindings from the names “square” and “cube” to the function objects located in the top left of the diagram.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The parent of the execution environments of &lt;code&gt;power&lt;/code&gt; is the function or enclosing environment of &lt;code&gt;power&lt;/code&gt;. The blue arrows going from the grey boxes to the bloat-like structure representing &lt;code&gt;power&lt;/code&gt; reveal these relationships.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The two execution environments are bound by the function objects &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt;, which is why there are black arrows pointing from their structures to the grey boxes in the diagram above.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The two execution environments each has its own binding from the name &lt;code&gt;exp&lt;/code&gt; to the double vector objects; this is indicated by the black arrows pointing from the grey boxes to the values 2 and 3.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;execution-environment-of-a-manufactured-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Execution environment of a manufactured function&lt;/h1&gt;
&lt;p&gt;         What happens when we execute &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt;? The answer is— same as before. Each time a function is called, an environment is created to host its execution. The parent of this execution environment is the enclosing or function environment of the function, which is determined by where it is created. Therefore, calling &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt; individually generates execution environments of their own. And the parents of these execution environments are the enclosing or function environments of &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt;, respectively. For instance, let us look at &lt;code&gt;square&lt;/code&gt;. We can explicitly return the execution environment of &lt;code&gt;square&lt;/code&gt; by using the &lt;code&gt;current_env()&lt;/code&gt; function from the &lt;code&gt;rlang&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# New power function factory
power &amp;lt;- function(exp) {
  
  # Force the evaluation of &amp;quot;exp&amp;quot;
  base::force(exp)
  
  function(x) {
    x ^ exp
    # We explicitly force the return of the execution environment of any manufactured function
    rlang::current_env()
  }
}
# New square function
square &amp;lt;- power(exp = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, whenever we execute &lt;code&gt;square&lt;/code&gt;, it will return the execution environment of &lt;code&gt;square&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Raise 10 to the power of 2
square(x = 10) %&amp;gt;% 
  # Execution and its parent environment
  rlang::env_print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;environment: 0x7f87bb130f30&amp;gt;
parent: &amp;lt;environment: 0x7f87ba13e038&amp;gt;
bindings:
 * x: &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;         In the output above, the &lt;strong&gt;first line&lt;/strong&gt; represents the address of the execution environment of &lt;code&gt;square&lt;/code&gt;. The &lt;strong&gt;last line&lt;/strong&gt; shows that there is a binding from the name “x” to a double vector object (“dbl” is short for double), which is the argument that we supplied to the function &lt;code&gt;square&lt;/code&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x=10\)&lt;/span&gt;. The &lt;strong&gt;second line&lt;/strong&gt; of the output above is of the utmost importance and it should make sense— it is the enclosing environment of &lt;code&gt;square&lt;/code&gt;. How can we be sure? Well, we can manually check the enclosing environment of &lt;code&gt;square&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rlang::env_print(env = square)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;environment: 0x7f87ba13e038&amp;gt;
parent: &amp;lt;environment: global&amp;gt;
bindings:
 * exp: &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;         As can be seen, the first line of the output above indeed matches with the second line of the output from earlier. We now have a proof of the relationship between the execution environment and the enclosing environment of a manufactured function. And, it is no different than the relationships between environments of any other functions.&lt;/p&gt;
&lt;div id=&#34;diagram&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagram&lt;/h2&gt;
&lt;p&gt;         But can we expound on the execution environment of a manufactured function more? Perhaps we can see the relationships more clearly via a diagram:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;square.png&#34; alt=&#34;Diagram from [Advance R](https://adv-r.hadley.nz/function-factories.html)&#34; width=&#34;413&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Diagram from &lt;a href=&#34;https://adv-r.hadley.nz/function-factories.html&#34;&gt;Advance R&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The relationships in the diagram can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The boat-like structure represents &lt;code&gt;square&lt;/code&gt; (clearly, the name &lt;code&gt;square&lt;/code&gt; is bound to this function object via the black arrow). The function also binds its enclosing environment, which is the grey box containing a binding &lt;code&gt;exp&lt;/code&gt;. Recall that this environment was also one of the execution environments of &lt;code&gt;power&lt;/code&gt;. However, in this diagram, we are zooming in to get a more focused view on the manufactured function &lt;code&gt;square&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What we have proved in the section above is precisely the relationship between the grey box on top of &lt;code&gt;square&lt;/code&gt; and the grey box to the right of &lt;code&gt;square&lt;/code&gt;. The grey box on top is indeed the execution environment of &lt;code&gt;square&lt;/code&gt; (with the blue arrow pointing to the function object), and the grey box to the right is its enclosing environment. When &lt;code&gt;square&lt;/code&gt; is executed, it scopes for the value of “x” in the execution environment, &lt;span class=&#34;math inline&#34;&gt;\(x=10\)&lt;/span&gt;, and the value of &lt;code&gt;exp&lt;/code&gt; in its enclosing environment, which is 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;         And, this is what makes a function factory special. The behavior of any manufactured function is powered by the execution environment of the factory function, which is different each time the factory function is used to create a manufactured function. With this core idea under our belt, we will be able to produce some elegant solutions when it comes to creating formatter functions for graphs or statistical functions used to transform, sample, or estimate data. We will tackle all of these in the next few posts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Portfolio Optimization Dashboard in R</title>
      <link>https://www.kenwuyang.com/en/post/portfolio-optimization-dashboard/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/portfolio-optimization-dashboard/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-dashboard/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;       In a previous &lt;a href=&#34;https://www.kenwuyang.com/en/post/customizable-portfolio-dashboard/&#34;&gt;post&lt;/a&gt;, I shared a customizable portfolio dashboard via R shiny. In that application, the weights of the portfolio are taken as given, and we focus more on the various types of visualizations and performance metrics. In this post, we create a portfolio optimization dashboard, which seeks to find the optimal set of weights, subject to some constraints, that yields a set of optimal portfolios. We will introduce some portfolio theory and implement the theoretical framework in R using packages such as &lt;code&gt;PortfolioAnalytics&lt;/code&gt;, &lt;code&gt;PerformanceAnalytics&lt;/code&gt;, and &lt;code&gt;tidyquant&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;       For more readings on the theory, I recommend &lt;a href=&#34;https://www.amazon.com/ISE-Essentials-Investments-Zvi-Bodie/dp/1260288390/&#34;&gt;Essentials of Investments&lt;/a&gt; and &lt;a href=&#34;https://www.amazon.com/Practical-Portfolio-Performance-Measurement-Attribution/dp/0470059281/&#34;&gt;Practical Portfolio Performance Measurement and Attribution&lt;/a&gt;. For implementing these theories in R, I suggest reading the documentations for the aforementioned R packages; in addition, Matt Dancho from &lt;a href=&#34;https://university.business-science.io/&#34;&gt;Business Science University&lt;/a&gt; (he is also the co-author of the &lt;code&gt;tidyquant&lt;/code&gt; package) provides some very useful &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;vignettes&lt;/a&gt; on the &lt;code&gt;tidyquant&lt;/code&gt; package. Many of my posts are inspired by his incredibly practical learning labs and vignettes. Finally, the dashboard can be found &lt;a href=&#34;https://rpubs.com/yangwu1227/personal_project&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating A Customizable Portfolio Dashboard Using R Shiny</title>
      <link>https://www.kenwuyang.com/en/post/customizable-portfolio-dashboard/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/customizable-portfolio-dashboard/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/customizable-portfolio-dashboard/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;       Shiny is a framework for creating web applications using R code. It’s server uses reactive programming, which allows for an interactive approach to communicating data analysis results. Shiny allows &lt;em&gt;you&lt;/em&gt;, the users of the web applications, to interact with the data, analysis, and results, which the creator has put together in the back-end.&lt;/p&gt;
&lt;p&gt;       In this application, I will create a customizable portfolio dashboard that empowers you to accomplish the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Construct a portfolio by selecting assets and weights&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specify the investment horizon by selecting a start and an end date&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specify portfolio strategies— rebalancing frequency, risk-free rate, benchmark, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute measures of risk and return and other portfolio performance metrics&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visualize measures of risk and return and other portfolio performance metrics&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;       The idea of creating such a user-friendly portfolio dashboard is taken from &lt;a href=&#34;http://www.reproduciblefinance.com/&#34;&gt;Reproducible Finance with R&lt;/a&gt;. I strongly recommend this book to those of you who may be interested in finance and are passionate about R like I am.&lt;/p&gt;
&lt;p&gt;       If you prefer, you could also see this &lt;a href=&#34;https://kenyangwu1227.shinyapps.io/Yang-Wu-Customizable-Portfolio-Dashboard/&#34;&gt;application&lt;/a&gt; hosted on the shiny server, which has a better layout in my opinion. Please press the control panel on the top left corner to see the other tabs.&lt;/p&gt;
&lt;iframe height=&#34;1500&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34; https://kenyangwu1227.shinyapps.io/Yang-Wu-Customizable-Portfolio-Dashboard/&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Asset Returns in R Using ggplot2 and highcharter</title>
      <link>https://www.kenwuyang.com/en/post/visualizing-asset-returns/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/visualizing-asset-returns/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-asset-returns-in-xts&#34;&gt;Visualization Asset returns in xts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-log-returns-highcharts&#34;&gt;Monthly log returns highcharts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-log-returns-histograms&#34;&gt;Monthly log returns Histograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualizations-in-the-tidyvserse&#34;&gt;Visualizations in the tidyvserse&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histogram&#34;&gt;Histogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#densty&#34;&gt;Densty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;visualization-asset-returns-in-xts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization Asset returns in xts&lt;/h2&gt;
&lt;p&gt;       In this post, we will explore some visualizations of asset returns. Similar to the previous &lt;a href=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/&#34;&gt;post&lt;/a&gt;, we import our data from Yahoo Finance. The five assets under examination are Exchange Traded Funds, which are funds that can be traded on an exchange like a stock. Exchange-traded funds are a type of investment fund that offers the best attributes of two popular assets: they have the diversification benefits of mutual funds and the ease with which stocks are traded. We will take a sample of daily prices from 2012-12-31 to 2021-7-31, converting them to monthly returns. This leaves us with a sample of 103 monthly returns. If you are interested in exploring a different sample, you could expand or shorten the time horizon upon importing the data. The visualization methods in this post are agnostic to the underlying data; the interpretation of these plots, however, will be different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a vector of ticker symbols
symbols &amp;lt;- c(&amp;quot;SPY&amp;quot;, &amp;quot;EFA&amp;quot;, &amp;quot;DIA&amp;quot;, &amp;quot;QQQ&amp;quot;, &amp;quot;AGG&amp;quot;)
# Load data from 2012 to today
prices &amp;lt;- quantmod::getSymbols(
  Symbols = symbols,
  src = &amp;quot;yahoo&amp;quot;,
  from = &amp;quot;2012-12-31&amp;quot;,
  to = &amp;quot;2021-7-31&amp;quot;,
  auto.assign = TRUE,
  warnings = FALSE
) %&amp;gt;%
  # The map function takes an anonymous function and will return a list of five
  # The Ad() function extracts the adjusted price series for each ETF
  purrr::map(.f = ~ quantmod::Ad(get(x = .x))) %&amp;gt;%
  # Use reduce() to merge the elements of .x consecutively
  purrr::reduce(.f = merge) %&amp;gt;%
  # Use a replacement function to set column names as ticker symbols
  # This function is in prefix form
  # It is equivalent to colnames(x = prices) &amp;lt;- value
  `colnames&amp;lt;-`(value = symbols)
# Remove all objects but price series and ticker symbol vector
rm(list = setdiff(x = ls(), y = c(&amp;quot;prices&amp;quot;, &amp;quot;symbols&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;       Since we will not be aggregating asset returns to compute portfolio returns, we choose log returns, i.e., the continuously compounded rate of returns, over the simple returns. Continuously compounded rate of returns should be used in statistical analysis (and visualizations) because unlike simple returns they are not positively biased. In addition, we opt to convert daily prices to monthly returns by finding the relative change of prices between the last day of each month. We could have easily chosen to use the first day of each month, and the values of the monthly returns will be different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Keep only the last price reading of each month
asset_returns_xts &amp;lt;- xts::to.monthly(
  x = prices,
  drop.time = TRUE,
  indexAt = &amp;quot;lastof&amp;quot;,
  OHLC = FALSE
) %&amp;gt;%
  # Compute log returns
  PerformanceAnalytics::Return.calculate(method = &amp;quot;log&amp;quot;) %&amp;gt;%
  # Drop the first row since we lose one observation in 12/31/2012
  stats::na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monthly-log-returns-highcharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly log returns highcharts&lt;/h3&gt;
&lt;p&gt;       The &lt;code&gt;highcharter&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/highcharter/highcharter.pdf&#34;&gt;package&lt;/a&gt; is a wrapper for the “Highcharts” Library, which has an amazing visualization infrastructure for time series and financial data. The &lt;code&gt;highcharter&lt;/code&gt; package houses functions that accept xts objects (R’s time series object class) as arguments, making it seamless to move from time series data to visualizations. The plot below displays the line chart for a subset of the ETF’s. We could have easily plotted all five ETF’s on the same line chart, but it would be harder for our eyes to compare, contrast, and identify patterns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Crate Highstock widget
highchart(type = &amp;quot;stock&amp;quot;) %&amp;gt;%
  # Add chart main title
  hc_title(text = &amp;quot;Monthly Log Returns for SPY, QQQ, DIA&amp;quot;) %&amp;gt;%
  # Add returns series to highchart objects
  # We use &amp;quot;symbols&amp;quot; to reference series since we may need to add/remove ETF&amp;#39;s in the future
  # Use matrix sub-setting and character indexing to select returns by column
  hc_add_series(
    data = asset_returns_xts[, symbols[[1]]],
    name = symbols[[1]]
  ) %&amp;gt;%
  hc_add_series(
    data = asset_returns_xts[, symbols[[4]]],
    name = symbols[[4]]
  ) %&amp;gt;%
  hc_add_series(
    data = asset_returns_xts[, symbols[[3]]],
    name = symbols[[3]]
  ) %&amp;gt;%
  # Add theme to highchart object
  # More themes to be found in the vignette
  hc_add_theme(hc_thm = hc_theme_flat()) %&amp;gt;%
  # Navigator
  hc_navigator(enabled = TRUE) %&amp;gt;%
  # Scrollbar
  hc_scrollbar(enabled = TRUE) %&amp;gt;%
  # Exporting
  hc_exporting(enabled = TRUE) %&amp;gt;%
  # Add legend
  hc_legend(enabled = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Line_chart.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The navigator is a small series below the main series, displaying a view of the entire data set. It provides tools to zoom in and out on parts of the data as well as panning across the data-set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The scroll-bar is a means of panning over the X axis of a stock chart. Scroll-bars can also be applied to other types of axes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;Highcharter&lt;/code&gt; plots are highly interactive; that is, an user can hover over the line chart to interact with it. However, due to internal conflicts with &lt;code&gt;blogdown&lt;/code&gt; themes, this &lt;code&gt;Highcharter&lt;/code&gt; plot cannot be rendered dynamically. Please navigate to this &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;link&lt;/a&gt; to view the interactive version of this line chart.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-log-returns-histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly log returns Histograms&lt;/h3&gt;
&lt;p&gt;       Create a function that returns a uni-variate histogram given a series of returns. The function will also take several other arguments— an xts object of returns, a vector of ticker symbols, a symbol index, and a color for plotting. Internally, the function creates a list of histogram components: counts, density, bin breaks, etc. Then, the function &lt;code&gt;hchart()&lt;/code&gt; is called on the histogram list object to plot the uni-variate histogram; this is the final output of the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc_hist_fun &amp;lt;- function(xts_obj, tickers, symbol_index, color) {

  # Check for invalid input
  if (!is.xts(xts_obj) || !rlang::is_character(color) || !rlang::is_character(tickers)) {
    rlang::abort(
      message = &amp;quot;Invalid input type for xts_object, tickers, and/or color arguments&amp;quot;
    )
  }

  # Create histogram list object with 6 elements
  hc_hist &amp;lt;- graphics::hist(xts_obj[, tickers[[symbol_index]]],
    breaks = &amp;quot;Freedman-Diaconis&amp;quot;,
    plot = FALSE
  )

  # Call hchart on the histogram list object
  hchart(object = hc_hist, color = color) %&amp;gt;%
    hc_title(
      text =
        paste(tickers[[symbol_index]], &amp;quot;Log Returns Distribution&amp;quot;, sep = &amp;quot; &amp;quot;)
    ) %&amp;gt;%
    hc_add_theme(hc_thm = hc_theme_flat()) %&amp;gt;%
    hc_exporting(enabled = TRUE) %&amp;gt;%
    hc_legend(enabled = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we utilize the functional programming tool from &lt;code&gt;purrr&lt;/code&gt; to apply the function above to each of the five uni-variate returns series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map the histogram function to each of the returns series
list_of_histogram &amp;lt;- purrr::map(
  .x = 1:5,
  .f = ~ hc_hist_fun(
    xts_obj = asset_returns_xts,
    tickers = symbols,
    symbol_index = .x,
    color = &amp;quot;cornflowerblue&amp;quot;
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;SPY.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;EFA.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DIA.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;QQQ.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;AGG.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please navigate to this &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;link&lt;/a&gt; to view the interactive version of these histograms.&lt;/p&gt;
&lt;p&gt;       As can be seen, most of our assets are negatively skewed, indicating that there were a few &lt;em&gt;really bad months&lt;/em&gt;. For the iShares Core US Aggregate Bond ETF (AGG), most months have returns that may be statistically indistinguishable from zero. From a sheer numbers perspective, investors who wish to maximize gains may consider such an asset undesirable. However, other performance factors such as risk, diversifier effect, and time horizon are important. The iShares Core US Aggregate Bond ETF seeks to track the investment results of an index composed of the total U.S. investment-grade bond market. And we expect bonds to produce lower returns for investors because they are also considered less volatile than stocks.&lt;/p&gt;
&lt;p&gt;The Invesco QQQ Trust Series 1 ETF (QQQ) stands out as a strong performing asset. Out of 103 months, about &lt;span class=&#34;math inline&#34;&gt;\(22%\)&lt;/span&gt; of its monthly returns fall between &lt;span class=&#34;math inline&#34;&gt;\(2\%\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(4\%\)&lt;/span&gt;. Hover over the histograms &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;here&lt;/a&gt; to see the counts and break points of the returns distribution.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations-in-the-tidyvserse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations in the tidyvserse&lt;/h2&gt;
&lt;p&gt;       Similarly, we could plot our asset returns using &lt;code&gt;ggplot2&lt;/code&gt;, which implements the layered grammar of graphics approach. For efficiency, we will convert the xts object into the &lt;em&gt;long tidy&lt;/em&gt; format that the tidyverse functions are designed to work well with. For another method of data importation that automatically converts the data into a &lt;em&gt;tidy&lt;/em&gt; format, please see this &lt;a href=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asset_returns_dplyr &amp;lt;- xts::to.monthly(
  x = prices,
  drop.time = TRUE,
  indexAt = &amp;quot;lastof&amp;quot;,
  OHLC = FALSE
) %&amp;gt;%
  # Create a new &amp;quot;date&amp;quot; variable by extracting the date indices from the xts object
  base::data.frame(&amp;quot;date&amp;quot; = zoo::index(x = .)) %&amp;gt;%
  # Coerce to tibble
  dplyr::as_tibble() %&amp;gt;%
  # Create a key column &amp;quot;asset&amp;quot; that contains the column names, i.e. ticker symbols
  # Create a value column that contains all the cells associated with each column
  # We convert to long format since it is easier to compute the returns using lag()
  tidyr::pivot_longer(
    cols = 1:5,
    names_to = &amp;quot;asset&amp;quot;,
    values_to = &amp;quot;returns&amp;quot;
  ) %&amp;gt;%
  # Group by ticker symbol
  dplyr::group_by(asset) %&amp;gt;%
  # Compute log returns manually
  dplyr::mutate(
    &amp;quot;returns&amp;quot; = (
      log(x = returns, base = exp(1)) - log(x = dplyr::lag(x = returns), base = exp(1))
    )
  ) %&amp;gt;%
  # Remove NA_double_ readings for 12/31/2021
  na.omit()
# See the results
head(asset_returns_dplyr, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 3
# Groups:   asset [5]
  date       asset  returns
  &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
1 2013-01-31 SPY    0.0499 
2 2013-01-31 EFA    0.0366 
3 2013-01-31 DIA    0.0593 
4 2013-01-31 QQQ    0.0264 
5 2013-01-31 AGG   -0.00623&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;histogram&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histogram&lt;/h3&gt;
&lt;p&gt;       Here are the histograms. Notice that we can either overlay the histograms on top of each other or show them in separate panels. I recommend using the panel approach for studying the shapes (spread, central tendency, skewness, tailed-ness, etc.) of the uni-variate distributions, and employ the overlaying histograms for making comparisons between these distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute Freedman-Diaconis bin numbers
bins_fd &amp;lt;- function(vec) {
  diff(range(vec)) / (2 * IQR(vec) / length(vec)^(1 / 3))
}
# Histogram
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histogram with panels
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  facet_wrap(~asset) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;densty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Densty&lt;/h3&gt;
&lt;p&gt;       We could also plot the probability density functions of these historical returns. Take a look at the y-axis of these plots and compare them to those of the histograms. This is an important distinction between these otherwise similar visualizations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Density plot
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_density(mapping = aes(color = asset)) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Density plot with panels
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_density(mapping = aes(color = asset)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  facet_wrap(~asset) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The smoothed densities can be useful for estimating the probabilities of returns, or an infinitesimal interval of returns, to be mathematically precise. Take caution when interpreting these kernel densities as it is important to understand that &lt;strong&gt;the heights of the curve (the y-axis) do not represent probabilities.&lt;/strong&gt; The y-axis in a density plot is the probability density function for the kernel density estimation. To convert to an actual probability, we need to find the area under the curve for a specific interval of returns. It is generally difficult to estimate probabilities from densities and we will have to tackle this problem in another post.&lt;/p&gt;
&lt;p&gt;       For now, we have equipped ourselves with some nice visualization techniques in R. These are not the only ways to visualize financial data by any means. In future posts, I will explore other aspects of financial analytics and portfolio analytics.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Portfolio Optimization and Returns in R</title>
      <link>https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#optimal-weights-for-a-five-asset-portfolio-minimum-variance&#34;&gt;Optimal Weights for a five-asset portfolio (Minimum Variance)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-in-r&#34;&gt;Implementation in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimal-weights-for-the-five-asset-portfolio-maximum-expected-return&#34;&gt;Optimal Weights for the five-asset portfolio (Maximum Expected Return)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-in-r-1&#34;&gt;Implementation in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-a-portfolio&#34;&gt;Building a portfolio&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-portfolio-returns-by-hand&#34;&gt;Monthly portfolio returns by hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-portfolio-returns-in-xts&#34;&gt;Monthly portfolio returns in xts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-portfolio-returns-in-the-tidyverse&#34;&gt;Monthly portfolio returns in the tidyverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-portfolio-returns-in-tidyquant&#34;&gt;Monthly portfolio returns in tidyquant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-and-contrast-the-four-methods&#34;&gt;Compare and Contrast the four methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In this post, we will explore some finance topics— portfolio optimization and computing portfolio returns. My goal is to apply what I’ve learned in portfolio theory using R as the main tool of analysis. There are many advantages to using a GUI like MS Excel, but R has an amazing data analysis work flow— a sort of one-stop solution from initial data importation to data wrangling and transformation to computations and analysis and then finally to visualizing and reporting results. We will be using functions from several R packages— &lt;code&gt;xts&lt;/code&gt;, &lt;code&gt;PerformanceAnalytics&lt;/code&gt;, &lt;code&gt;PortfolioAnalytics&lt;/code&gt;, &lt;code&gt;tidyquant&lt;/code&gt;, &lt;code&gt;tidyverse&lt;/code&gt;. In particular, the &lt;code&gt;tidyquant&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;package&lt;/a&gt; is a recent development that has markedly enriched R’s financial analytics infrastructure, enhancing its usability in finance. While I cover some theory in this post, my main focus will be on the implementation of these topics in R.&lt;/p&gt;
&lt;p&gt;       For more readings on the theory, I recommend &lt;a href=&#34;https://www.amazon.com/ISE-Essentials-Investments-Zvi-Bodie/dp/1260288390/&#34;&gt;Essentials of Investments&lt;/a&gt; and &lt;a href=&#34;https://www.amazon.com/Practical-Portfolio-Performance-Measurement-Attribution/dp/0470059281/&#34;&gt;Practical Portfolio Performance Measurement and Attribution&lt;/a&gt;. To learn more about analyzing financial data in R, there is &lt;a href=&#34;http://www.reproduciblefinance.com/&#34;&gt;Reproducible Finance with R&lt;/a&gt;, which is a very practical book with a strong emphasis on application.&lt;/p&gt;
&lt;p&gt;       With that being said, let us get financial!&lt;/p&gt;
&lt;div id=&#34;optimal-weights-for-a-five-asset-portfolio-minimum-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal Weights for a five-asset portfolio (Minimum Variance)&lt;/h2&gt;
&lt;p&gt;       We will employ Markowitz’s Mean-Variance model as the framework for computing optimal weights, essentially treating the task as an “unconstrained” optimization problem. The objective of this optimization problem is one of minimization:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{Minimize}\hspace{2mm}(\sigma^{2}=\vec{W}^{T}\sum\vec{W}))
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;subject to the sum of weights constraint and the box constraint:
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^{N} W_{i}=1 \quad \text { and } \hspace{3mm} \varepsilon_{i} \leq W_{i} \leq \delta_{i}
\]&lt;/span&gt;
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}=0.05 \hspace{3mm} \delta_{i}=0.6\)&lt;/span&gt; are the lower and upper bounds for the weights &lt;span class=&#34;math inline&#34;&gt;\(W_{i}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;       The minimization problem is a quadratic problem with linear constraints, since the variance formula is a quadratic function and the constraints are linear functions; this type of problem is well suited to be solved using a quadratic programming solver. The &lt;code&gt;PortfolioAnalytics&lt;/code&gt; package uses &lt;code&gt;ROI.plugin.quadprog&lt;/code&gt;, a plug-in for the “R” Optimization Infrastructure, to solve the problem. The solver can be specified with the optimize_method argument in &lt;code&gt;optimize.portfolio()&lt;/code&gt;. If optimize_method = “ROI” is specified, a default solver will be selected based on the optimization problem. The &lt;code&gt;glpk&lt;/code&gt; solver is the default solver for LP and MILP optimization problems. The &lt;code&gt;quadprog&lt;/code&gt; solver is the default solver for QP optimization problems.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;implementation-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation in R&lt;/h3&gt;
&lt;p&gt;       For this task, we will import our data from Yahoo Finance. The five assets under examination are Exchange Traded Funds, which are funds that can be traded on an exchange like a stock. Exchange-traded funds are a type of investment fund that offers the best attributes of two popular assets: they have the diversification benefits of mutual funds and the ease with which stocks are traded. However, before we import any data, we must answer the following question: In what form do we want our data to be? Since we are in the world of finance, times series is the most common type of data. R has the &lt;code&gt;xts&lt;/code&gt; package to handle data that are indexed by date. Our task, therefore, reduces to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Import daily prices from Yahoo Finance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convert daily prices to monthly return&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because we will be aggregating monthly returns to form a portfolio, we will need to compute the simple returns&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a vector of ticker symbols
symbols &amp;lt;- c(&amp;quot;SPY&amp;quot;, &amp;quot;EFA&amp;quot;, &amp;quot;IJS&amp;quot;, &amp;quot;EEM&amp;quot;, &amp;quot;AGG&amp;quot;)
# Load data from 2012 to today
# Specify the &amp;quot;to = &amp;quot; argument to specify an end date
prices &amp;lt;- quantmod::getSymbols(
  Symbols = symbols,
  src = &amp;quot;yahoo&amp;quot;,
  from = &amp;quot;2012-12-31&amp;quot;,
  auto.assign = TRUE,
  warnings = FALSE
) %&amp;gt;%
  # The map function takes an anonymous function and will return a list of five
  # The function Ad() extracts the daily adjusted price series
  purrr::map(.f = ~ quantmod::Ad(get(x = .x))) %&amp;gt;%
  # Use reduce() to merge the elements of .x interactively
  purrr::reduce(.f = merge) %&amp;gt;%
  # Use a replacement function to set column names to ticker symbols
  # This function is in prefix form
  # It is equivalent to colnames(x = prices) &amp;lt;- value
  `colnames&amp;lt;-`(value = symbols)
# Keep only the last reading of each month
# We could have chosen to keep only the first reading of each month
asset_returns_xts &amp;lt;- xts::to.monthly(
  x = prices,
  drop.time = TRUE,
  indexAt = &amp;quot;lastof&amp;quot;,
  OHLC = FALSE
) %&amp;gt;%
  # Compute simple returns
  # Log returns are time-additive but not portfolio additive
  PerformanceAnalytics::Return.calculate(method = &amp;quot;discrete&amp;quot;) %&amp;gt;%
  # Drop the first row since we lose 12/31/2012
  stats::na.omit()
# Keep only the xts returns, ticker symbols, and the prices series
rm(list = setdiff(x = ls(), y = c(&amp;quot;symbols&amp;quot;, &amp;quot;prices&amp;quot;, &amp;quot;asset_returns_xts&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Create Portfolio object&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Examine the monthly simple returns for our five ETF&amp;#39;s
head(x = asset_returns_xts, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  SPY         EFA         IJS          EEM           AGG
2013-01-31 0.05119052  0.03728466 0.053516334 -0.002930946 -0.0062115144
2013-02-28 0.01275821 -0.01288577 0.016306731 -0.022840526  0.0059086990
2013-03-31 0.03797176  0.01305398 0.041079862 -0.010182972  0.0009852088
2013-04-30 0.01921213  0.05018653 0.001223168  0.012158139  0.0096857020
2013-05-31 0.02360972 -0.03019051 0.042869516 -0.048279085 -0.0200111498&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create Portfolio object which is essentially a list object
min_var_portfolio &amp;lt;- PortfolioAnalytics::portfolio.spec(assets = symbols)
typeof(min_var_portfolio)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add constraints to the portfolio object&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add the full investment constraint that specifies that the weights must sum to 1
min_var_portfolio &amp;lt;- PortfolioAnalytics::add.constraint(
  portfolio = min_var_portfolio,
  type = &amp;quot;full_investment&amp;quot;
)
# Examine the constraint element by extracting min_var_portfolio[[&amp;quot;constraints&amp;quot;]][[1]]
str(pluck(.x = min_var_portfolio, &amp;quot;constraints&amp;quot;, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 6
 $ type   : chr &amp;quot;full_investment&amp;quot;
 $ enabled: logi TRUE
 $ message: logi FALSE
 $ min_sum: num 1
 $ max_sum: num 1
 $ call   : language PortfolioAnalytics::add.constraint(portfolio = min_var_portfolio, type = &amp;quot;full_investment&amp;quot;)
 - attr(*, &amp;quot;class&amp;quot;)= chr [1:2] &amp;quot;weight_sum_constraint&amp;quot; &amp;quot;constraint&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add the box constraint that ensure the weights are between 0.1 and 0.6
min_var_portfolio &amp;lt;- PortfolioAnalytics::add.constraint(
  portfolio = min_var_portfolio,
  type = &amp;quot;box&amp;quot;, min = 0.05, max = 0.6
)
# Examine the constraint element by extracting min_var_portfolio[[&amp;quot;constraints&amp;quot;]][[2]]
str(pluck(.x = min_var_portfolio, &amp;quot;constraints&amp;quot;, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 5
 $ type   : chr &amp;quot;box&amp;quot;
 $ enabled: logi TRUE
 $ min    : Named num [1:5] 0.05 0.05 0.05 0.05 0.05
  ..- attr(*, &amp;quot;names&amp;quot;)= chr [1:5] &amp;quot;SPY&amp;quot; &amp;quot;EFA&amp;quot; &amp;quot;IJS&amp;quot; &amp;quot;EEM&amp;quot; ...
 $ max    : Named num [1:5] 0.6 0.6 0.6 0.6 0.6
  ..- attr(*, &amp;quot;names&amp;quot;)= chr [1:5] &amp;quot;SPY&amp;quot; &amp;quot;EFA&amp;quot; &amp;quot;IJS&amp;quot; &amp;quot;EEM&amp;quot; ...
 $ call   : language PortfolioAnalytics::add.constraint(portfolio = min_var_portfolio, type = &amp;quot;box&amp;quot;,      min = 0.05, max = 0.6)
 - attr(*, &amp;quot;class&amp;quot;)= chr [1:2] &amp;quot;box_constraint&amp;quot; &amp;quot;constraint&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add objective function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add objective to minimize variance
min_var_portfolio &amp;lt;- PortfolioAnalytics::add.objective(
  portfolio = min_var_portfolio,
  # Minimize risk
  type = &amp;quot;risk&amp;quot;,
  # A character corresponding to a function name, var()
  name = &amp;quot;var&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Run the optimization
global_min_portfolio &amp;lt;- PortfolioAnalytics::optimize.portfolio(
  R = asset_returns_xts,
  portfolio = min_var_portfolio,
  # This defaults to the &amp;quot;quadprog&amp;quot; solver
  optimize_method = &amp;quot;quadprog&amp;quot;,
  # Return additional information on the path or portfolios searched
  trace = TRUE
)
# Examine returned portfolio list object
global_min_portfolio&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;***********************************
PortfolioAnalytics Optimization
***********************************

Call:
PortfolioAnalytics::optimize.portfolio(R = asset_returns_xts, 
    portfolio = min_var_portfolio, optimize_method = &amp;quot;quadprog&amp;quot;, 
    trace = TRUE)

Optimal Weights:
  SPY   EFA   IJS   EEM   AGG 
0.213 0.087 0.050 0.050 0.600 

Objective Measure:
 StdDev 
0.01644 &lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;optimal-weights-for-the-five-asset-portfolio-maximum-expected-return&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal Weights for the five-asset portfolio (Maximum Expected Return)&lt;/h2&gt;
&lt;p&gt;       Now that we found the global minimum variance portfolio, we may be interested in finding the maximal expected return portfolio. The objective is one of maximization:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\text{Maximize}\hspace{2mm}(\vec{\mu}^{T}\vec{W})
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;subject to the sum of weights constraint and the box constraint:
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^{N} W_{i}=1 \quad \text { and } \hspace{3mm} \varepsilon_{i} \leq W_{i} \leq \delta_{i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;       The optimization problem in this case is a linear programming problem, since the portfolio expected return formula is a linear function. This is best tackled using a linear programming solver. The package &lt;code&gt;PortfolioAnalytics&lt;/code&gt; uses the ROI package with the &lt;code&gt;glpk plugin&lt;/code&gt;, the GNU Linear Programming toolkit of R’s Optimization Infrastructure.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;implementation-in-r-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation in R&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create portfolio object&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create Portfolio object
max_exp_return_portfolio &amp;lt;- PortfolioAnalytics::portfolio.spec(assets = symbols)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add constraints to the object&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add the full investment constraint that specifies the weights must sum to 1
max_exp_return_portfolio &amp;lt;- PortfolioAnalytics::add.constraint(
  portfolio = max_exp_return_portfolio,
  type = &amp;quot;full_investment&amp;quot;
)
# Add the box constraint that ensure the weights are between 0.1 and 0.6
max_exp_return_portfolio &amp;lt;- PortfolioAnalytics::add.constraint(
  portfolio = max_exp_return_portfolio,
  type = &amp;quot;box&amp;quot;, min = 0.05, max = 0.6
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add objective function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add objective to maximize mean returns
max_exp_return_portfolio &amp;lt;- PortfolioAnalytics::add.objective(
  portfolio = max_exp_return_portfolio,
  # Maximize expected returns
  type = &amp;quot;return&amp;quot;,
  # A character corresponding to a function name, mean()
  name = &amp;quot;mean&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Run the optimization
global_max_portfolio &amp;lt;- PortfolioAnalytics::optimize.portfolio(
  R = asset_returns_xts,
  portfolio = max_exp_return_portfolio,
  # This defaults to the &amp;quot;glpk&amp;quot; solver
  optimize_method = &amp;quot;glpk&amp;quot;,
  # Return additional information on the path or portfolios searched
  trace = TRUE
)
# Examine returned portfolio list object
global_max_portfolio&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;***********************************
PortfolioAnalytics Optimization
***********************************

Call:
PortfolioAnalytics::optimize.portfolio(R = asset_returns_xts, 
    portfolio = max_exp_return_portfolio, optimize_method = &amp;quot;glpk&amp;quot;, 
    trace = TRUE)

Optimal Weights:
 SPY  EFA  IJS  EEM  AGG 
0.60 0.05 0.25 0.05 0.05 

Objective Measure:
  mean 
0.0114 &lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a portfolio&lt;/h2&gt;
&lt;p&gt;       We have found two sets of optimal weights that yield portfolios that offer the lowest possible risk and the high possible expected return given two basic constraints. Our next task is to aggregate the monthly returns of the individual ETF’s to find the monthly returns of our five-asset portfolio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set optimal weights
weights &amp;lt;- pluck(.x = global_max_portfolio, &amp;quot;weights&amp;quot;)
# Check if the weights and symbols align
tibble(weights, symbols)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 2
  weights symbols
    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  
1   0.6   SPY    
2   0.05  EFA    
3   0.250 IJS    
4   0.05  EEM    
5   0.05  AGG    &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ensure that the weights vector sums up to 1
tibble(weights, symbols) %&amp;gt;%
  dplyr::summarize(total_weight = sum(weights))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 × 1
  total_weight
         &amp;lt;dbl&amp;gt;
1            1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The portfolio return in month, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
r_{\text{portfolio,t}}=\sum_{i=1}^{5}W_{i}r_{i,t}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;monthly-portfolio-returns-by-hand&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly portfolio returns by hand&lt;/h3&gt;
&lt;p&gt;       We can compute portfolio monthly returns using a brute force method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute by hand
portfolio_returns_by_hand &amp;lt;-
  (weights[[1]] * asset_returns_xts[, 1]) +
  (weights[[2]] * asset_returns_xts[, 2]) +
  (weights[[3]] * asset_returns_xts[, 3]) +
  (weights[[4]] * asset_returns_xts[, 4]) +
  (weights[[5]] * asset_returns_xts[, 5])
# Name the series
portfolio_returns_by_hand &amp;lt;- `names&amp;lt;-`(portfolio_returns_by_hand, &amp;quot;Monthly portfolio returns&amp;quot;)
# Examine
head(portfolio_returns_by_hand, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Monthly portfolio returns
2013-01-31                0.04550050
2013-02-28                0.01024073
2013-03-31                0.03324583
2013-04-30                0.01543459
2013-05-31                0.01995917&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-portfolio-returns-in-xts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly portfolio returns in xts&lt;/h3&gt;
&lt;p&gt;       Another way to compute portfolio monthly returns is to use functions from the &lt;code&gt;PerformanceAnalytics&lt;/code&gt; package, which works well with &lt;code&gt;xts&lt;/code&gt; objects. We also adopt monthly re-balancing as a strategy. The re-balancing of investments is the action of keeping the portfolio weights consistent with the optimal weights. Note that re-balancing the weights every month may be unrealistic; in reality, the convention is often to re-balance quarterly or annually. For this example, however, we will re-balance monthly to be consistent with our brute force computation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute monthly portfolio returns
portfolio_returns_xts_rebalanced_monthly &amp;lt;-
  PerformanceAnalytics::Return.portfolio(
    R = asset_returns_xts,
    weights = weights,
    # Monthly re-balancing
    reblance_on = &amp;quot;months&amp;quot;,
    # Use simple/arithmetic chaining to aggregate returns
    geometric = FALSE
  ) %&amp;gt;%
  `colnames&amp;lt;-`(&amp;quot;Monthly_portfolio_returns&amp;quot;)
# Examine
head(portfolio_returns_xts_rebalanced_monthly, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Monthly_portfolio_returns
2013-01-31                0.04550050
2013-02-28                0.01024073
2013-03-31                0.03324583
2013-04-30                0.01543459
2013-05-31                0.01995917&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;Return.portfolio(R, weights = NULL, wealth.index = FALSE, contribution = FALSE, geometric = TRUE, rebalance_on = c(NA, &#34;years&#34;, &#34;quarters&#34;, &#34;months&#34;, &#34;weeks&#34;, &#34;days&#34;), value = 1, verbose = FALSE, ...)&lt;/code&gt; calculates the returns of a portfolio with the same periodicity of the returns data.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-portfolio-returns-in-the-tidyverse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly portfolio returns in the tidyverse&lt;/h3&gt;
&lt;p&gt;       The &lt;code&gt;tidyverse&lt;/code&gt; is a collection of R packages designed with the same underlying philosophy, grammar, and data structures. Simply put, the “tidy” data structure that works well with tidyverse functions is one where every row is an observation and every column is a variable. If we re-examine the &lt;code&gt;xts&lt;/code&gt; object called “asset_returns_xts,” we see that every column is a returns series for a particular asset. This is inconsistent with the tidyverse data structure, and so we must convert the xts object to a tidy format for our computations. Ideally, we would like to have one column called “asset” that specifies the names of the ETF instead of having five individual columns of returns data. This idea will become clearer once we convert our xts object to a tibble.&lt;/p&gt;
&lt;p&gt;Now, examine the “tidy” data structure and compare it to the xts object created earlier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FSA::headtail(asset_returns_long, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          date asset       returns
1   2013-01-31   SPY  0.0511905160
2   2013-01-31   EFA  0.0372846569
3   2013-01-31   IJS  0.0535163338
4   2013-01-31   EEM -0.0029309465
5   2013-01-31   AGG -0.0062115144
521 2021-09-30   SPY -0.0224997875
522 2021-09-30   EFA -0.0017360987
523 2021-09-30   IJS -0.0316548170
524 2021-09-30   EEM -0.0198435604
525 2021-09-30   AGG -0.0001984158&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our returns data is in a “tidy” format, computing portfolio monthly returns is very a straight forward. For those who are familiar with SQL, we are essentially using the CASE statement here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use vectorized if else statements to assign weights according to the asset column
potfolio_returns_dplyr_byhand &amp;lt;- asset_returns_long %&amp;gt;%
  group_by(asset) %&amp;gt;%
  mutate(
    weights = dplyr::case_when(
      asset == symbols[[1]] ~ weights[[1]],
      asset == symbols[[2]] ~ weights[[2]],
      asset == symbols[[3]] ~ weights[[3]],
      asset == symbols[[4]] ~ weights[[4]],
      asset == symbols[[5]] ~ weights[[5]]
    ),
    weighted_returns = weights * returns
  ) %&amp;gt;%
  # Group by date
  # We need to group by date so that the aggregate sum() function is carried out by month
  # For each date, the original series has 5 weighted returns, one for each ETF
  # The results should be 1 portfolio return (the sum of the 5 weighted returns) for each month
  dplyr::group_by(date) %&amp;gt;%
  # Compute monthly portfolio returns
  dplyr::summarize(Monthly_portfolio_returns = sum(weighted_returns))
# Examine the data
head(potfolio_returns_dplyr_byhand, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 2
  date       Monthly_portfolio_returns
  &amp;lt;date&amp;gt;                         &amp;lt;dbl&amp;gt;
1 2013-01-31                    0.0455
2 2013-02-28                    0.0102
3 2013-03-31                    0.0332
4 2013-04-30                    0.0154
5 2013-05-31                    0.0200&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;summarize(.data, ..., .groups = NULL)&lt;/code&gt; creates a new data frame. It will have one (or more) rows for each combination of grouping variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-portfolio-returns-in-tidyquant&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly portfolio returns in tidyquant&lt;/h3&gt;
&lt;p&gt;       The &lt;code&gt;tidyquant&lt;/code&gt; package gives us two ways to compute the portfolio monthly returns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use function from the tidyquant
portfolio_returns_tq_rebalanced_monthly_method_1 &amp;lt;- asset_returns_long %&amp;gt;%
  tidyquant::tq_portfolio(
    assets_col = asset,
    returns_col = returns,
    weights = weights,
    col_rename = &amp;quot;Monthly_portfolio_returns&amp;quot;,
    rebalance_on = &amp;quot;months&amp;quot;
  )
# Examine
head(portfolio_returns_tq_rebalanced_monthly_method_1, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 2
  date       Monthly_portfolio_returns
  &amp;lt;date&amp;gt;                         &amp;lt;dbl&amp;gt;
1 2013-01-31                    0.0455
2 2013-02-28                    0.0102
3 2013-03-31                    0.0332
4 2013-04-30                    0.0154
5 2013-05-31                    0.0200&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;tq_portfolio(data, assets_col, returns_col, weights = NULL, col_rename = NULL, ...)&lt;/code&gt; aggregates a group of returns by asset into portfolio returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;       An alternative way to load data is to use the &lt;code&gt;tidyquant&lt;/code&gt; wrapper function, which automatically coerces the returns into a long tidy format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data
asset_returns_tq &amp;lt;- tidyquant::tq_get(
  x = symbols,
  get = &amp;quot;stock.prices&amp;quot;,
  from = &amp;quot;2012-12-31&amp;quot;
) %&amp;gt;%
  # The asset column is named symbol by default (see body(tidyquant::tq_get))
  dplyr::group_by(symbol) %&amp;gt;%
  # Select adjusted daily prices
  tidyquant::tq_transmute(
    select = adjusted,
    col_rename = &amp;quot;returns&amp;quot;,
    # This function is from quantmod
    mutate_fun = periodReturn,
    # These arguments are passed along to the mutate function quantmod::periodReturn
    period = &amp;quot;monthly&amp;quot;,
    # Simple returns
    type = &amp;quot;arithmetic&amp;quot;,
    # Do not return leading period data
    leading = FALSE,
    # This argument is passed along to xts::to.period, which is wrapped by quantmod::periodReturn
    # We use the last reading of each month to find percentage changes
    indexAt = &amp;quot;lastof&amp;quot;
  ) %&amp;gt;%
  dplyr::rename(asset = symbol) %&amp;gt;%
  na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;tq_get(x, get = &#34;stock.prices&#34;, complete_cases = TRUE, ...)&lt;/code&gt; gets data in tibble format. The most important argument is perhaps the dot-dot-dot, since these are the arguments passed to the underlying functions from the other packages that &lt;code&gt;tq_get()&lt;/code&gt; uses. There could be multiple layers of wrapper functions, and so it is best practice to read the documentations carefully.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;       A possibly more useful method of aggregating asset returns to portfolio returns is then to &lt;em&gt;map a tibble of ticker symbols and weights to the tibble of ticker symbols and monthly returns&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a tibble of weights
weights_tibble &amp;lt;- tibble(
  asset = symbols,
  weights = weights
)
head(weights_tibble, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 2
  asset weights
  &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
1 SPY     0.6  
2 EFA     0.05 
3 IJS     0.250
4 EEM     0.05 
5 AGG     0.05 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map the weights to the returns column using the asset column as the identifier
portfolio_returns_tq_rebalanced_monthly_method_2 &amp;lt;- asset_returns_tq %&amp;gt;%
  tidyquant::tq_portfolio(
    assets_col = asset,
    returns_col = returns,
    weights = weights_tibble,
    col_rename = &amp;quot;Monthly_portfolio_returns&amp;quot;,
    rebalance_on = &amp;quot;months&amp;quot;
  )
head(portfolio_returns_tq_rebalanced_monthly_method_2, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 2
  date       Monthly_portfolio_returns
  &amp;lt;date&amp;gt;                         &amp;lt;dbl&amp;gt;
1 2013-01-31                    0.0455
2 2013-02-28                    0.0102
3 2013-03-31                    0.0332
4 2013-04-30                    0.0154
5 2013-05-31                    0.0200&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-and-contrast-the-four-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compare and Contrast the four methods&lt;/h3&gt;
&lt;p&gt;       We have covered a variety of different methods for aggregating asset monthly returns to portfolio monthly returns. As a sanity check, we want to ensure that these methods yield consistent results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;potfolio_returns_dplyr_byhand %&amp;gt;%
  # Rename the column
  rename(tidyverse_method = Monthly_portfolio_returns) %&amp;gt;%
  # Create three new columns that contain results for other methods
  dplyr::mutate(
    &amp;quot;equation_byhand&amp;quot; = zoo::coredata(x = portfolio_returns_by_hand),
    &amp;quot;tq_method_1&amp;quot; = portfolio_returns_tq_rebalanced_monthly_method_1[[&amp;quot;Monthly_portfolio_returns&amp;quot;]],
    &amp;quot;tq_method_2&amp;quot; = portfolio_returns_tq_rebalanced_monthly_method_2[[&amp;quot;Monthly_portfolio_returns&amp;quot;]],
    &amp;quot;xts_method&amp;quot; = zoo::coredata(portfolio_returns_xts_rebalanced_monthly)
  ) %&amp;gt;%
  purrr::modify_if(.p = is.numeric, .f = ~ round(x = .x, digits = 7)) %&amp;gt;%
  dplyr::select(-date) %&amp;gt;%
  # Examine
  head(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 × 5
   tidyverse_method equation_byhand[,… tq_method_1 tq_method_2 xts_method[,&amp;quot;Mon…
              &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
 1           0.0455             0.0455      0.0455      0.0455            0.0455
 2           0.0102             0.0102      0.0102      0.0102            0.0102
 3           0.0332             0.0332      0.0332      0.0332            0.0332
 4           0.0154             0.0154      0.0154      0.0154            0.0154
 5           0.0200             0.0200      0.0200      0.0200            0.0200
 6          -0.0131            -0.0131     -0.0131     -0.0131           -0.0131
 7           0.0509             0.0509      0.0509      0.0509            0.0509
 8          -0.0292            -0.0292     -0.0292     -0.0292           -0.0292
 9           0.0436             0.0436      0.0436      0.0436            0.0436
10           0.0406             0.0406      0.0406      0.0406            0.0406&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;coredata(x, ...)&lt;/code&gt; is a generic function for extracting the core data contained in, a (more complex) object and replacing it. The replacement function is &lt;code&gt;coredata&amp;lt;-&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;       As can be seen, the results are all consistent. &lt;em&gt;Sigh of relief&lt;/em&gt;. For the upcoming posts, I also plan on tackling other topics in portfolio theory and to build off of what I’ve covered in this post. In particular, Python also has many great libraries— NumPy, SciPy, Quandl— all of which contain great tools for financial analysis. Utilizing R’s tidyverse ecosystem and Python’s fast, memory-efficient methods, there’s great deal of content to cover.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Central Limit Theorem and Power Simulation in R</title>
      <link>https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multivariate-normal-distribution&#34;&gt;Multivariate Normal Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simulating-power&#34;&gt;Simulating Power&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;central-limit-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;       The central limit theorem, crudely speaking, states that— if there is a distribution with expected value given by the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and finite variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and we take &lt;em&gt;sufficiently&lt;/em&gt; large random samples from this distribution with replacement, then the distribution of the sample means will be &lt;em&gt;approximately&lt;/em&gt; normally distributed. This will hold true regardless of whether the source or “parent” distribution is normal, conditional on the fact that the sample size is sufficiently large (say, n &amp;gt; 30). In this post, we demonstrate two characteristics of the CLT:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The mean of the distribution of sample means, &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\bar{x}}\)&lt;/span&gt; should converge towards the mean of the “parent population,” the population from which the random samples are drawn, as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The standard deviation of the distribution of sample means, also known as the standard error of the mean, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\bar{x}}= \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt;, should become smaller as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;. In other words, the spread of the distribution of sample means should decrease as sample size, n, increases.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we examine a poisson distribution &lt;span class=&#34;math inline&#34;&gt;\(X\sim pois(220/24)\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Lambda is the number of geese that arrive per hour
lambda &amp;lt;- 220 / 24
# Number of random samples to be drawn
numsim &amp;lt;- 10000
# Initialize variables
mean5 &amp;lt;- rep(0, numsim)
mean15 &amp;lt;- rep(0, numsim)
mean30 &amp;lt;- rep(0, numsim)
mean100 &amp;lt;- rep(0, numsim)
mean200 &amp;lt;- rep(0, numsim)
# Loop for simulating
for (i in 1:numsim) {

  # sample means
  mean5[i] &amp;lt;- mean(rpois(5, lambda))
  mean15[i] &amp;lt;- mean(rpois(15, lambda))
  mean30[i] &amp;lt;- mean(rpois(30, lambda))
  mean100[i] &amp;lt;- mean(rpois(100, lambda))
  mean200[i] &amp;lt;- mean(rpois(200, lambda))
}
# Create five data frames for these sampling distributions with varying sample sizes
# Create a variable &amp;quot;sample_size&amp;quot; used as an identifier
n_5 &amp;lt;- data.frame(sample_size = as.factor(rep(5, 10000)), sample_means = mean5)
n_15 &amp;lt;- data.frame(sample_size = as.factor(rep(15, 10000)), sample_means = mean15)
n_30 &amp;lt;- data.frame(sample_size = as.factor(rep(30, 10000)), sample_means = mean30)
n_100 &amp;lt;- data.frame(sample_size = as.factor(rep(100, 10000)), sample_means = mean100)
n_200 &amp;lt;- data.frame(sample_size = as.factor(rep(200, 10000)), sample_means = mean200)
# Combine into a single data frame
# The function rbind() combines data frames by rows
sampling_distributions &amp;lt;- rbind(n_5, n_15, n_30, n_100, n_200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first characteristic of the CLT states that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\bar{x}}\)&lt;/span&gt; should converge towards &lt;span class=&#34;math inline&#34;&gt;\(\lambda= 9.1667\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;. Let’s check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using tapply
round(tapply(
  sampling_distributions$sample_means,
  sampling_distributions$sample_size, mean
), digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      5      15      30     100     200 
9.15652 9.15523 9.16742 9.17297 9.16732 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\bar{x}}\)&lt;/span&gt; gets closer and closer to &lt;span class=&#34;math inline&#34;&gt;\(\lambda= 9.1667\)&lt;/span&gt; as sample size becomes larger. For the second characteristic, let’s examine the overlay density plots of these sampling distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot()
ggplot(
  data = sampling_distributions,
  mapping = aes(x = sample_means)
) +
  geom_density(aes(fill = sample_size, color = sample_size), alpha = 0.2) +
  geom_vline(xintercept = lambda, linetype = &amp;quot;dashed&amp;quot;) +
  ggtitle(&amp;quot;Density Plot of Sampling Distributions&amp;quot;) +
  xlab(&amp;quot;Distributions of Sample Means&amp;quot;) +
  ylab(&amp;quot;Density&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;azure2&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The dashed line in the figure above indicates &lt;span class=&#34;math inline&#34;&gt;\(\lambda= 9.1667\)&lt;/span&gt;. Evidently, as sample size gets larger, the spread of the distribution of sample means gets smaller. We have a visual confirmation of the CTL that standard error of the mean should become smaller as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;. Below, we compute the standard deviations of these distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using tapply
round(tapply(sampling_distributions$sample_means, sampling_distributions$sample_size, sd), digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      5      15      30     100     200 
1.33956 0.77560 0.55026 0.30485 0.21358 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       Now, we have a numeric proof of this characteristic as well. Additionally, we may also compare these simulated values to our analytical solutions. We know that the standard deviation of a poisson distribution is simply the square root of the mean, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Before we run any simulations, we may compute the standard errors of the mean, given a sample size, n, analytically, using the formula: &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\bar{x}}= \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sd of the distribution of a random sample n=5
sd_5 &amp;lt;- sqrt(lambda) / sqrt(5)
# sd of the distribution of a random sample n=15
sd_15 &amp;lt;- sqrt(lambda) / sqrt(15)
# sd of the distribution of a random sample n=30
sd_30 &amp;lt;- sqrt(lambda) / sqrt(30)
# sd of the distribution of a random sample n=100
sd_100 &amp;lt;- sqrt(lambda) / sqrt(100)
# sd of the distribution of a random sample n=200
sd_200 &amp;lt;- sqrt(lambda) / sqrt(200)
# store in one vector
se &amp;lt;- c(sd_5, sd_15, sd_30, sd_100, sd_200)
names(se) &amp;lt;- c(&amp;quot;5&amp;quot;, &amp;quot;15&amp;quot;, &amp;quot;30&amp;quot;, &amp;quot;100&amp;quot;, &amp;quot;200&amp;quot;)
se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        5        15        30       100       200 
1.3540064 0.7817360 0.5527708 0.3027650 0.2140872 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we see that our simulated values match pretty well with our analytical solutions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate Normal Distribution&lt;/h2&gt;
&lt;p&gt;       When two random variables &lt;span class=&#34;math inline&#34;&gt;\(X_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{2}\)&lt;/span&gt; have a bivariate normal distribution, we can express them using matrix notations.&lt;/p&gt;
&lt;p&gt;A random &lt;span class=&#34;math inline&#34;&gt;\(2 \times 1\)&lt;/span&gt; column vector X and its mean:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
X &amp;amp;= \begin{pmatrix} X_{1}\\ X_{2} \end{pmatrix} \\
\mu &amp;amp;= \begin{pmatrix} \mu_{X_{1}}\\ \mu_{X_{2}} \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; covariance matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sum &amp;amp;= \begin{pmatrix} Var_{X_{1}} &amp;amp; Cov_{X_{2}X_{1}}\\ Cov_{X_{1}X_{2}} &amp;amp; Var_{X_{2}} \end{pmatrix} \\
     &amp;amp;= \begin{pmatrix} \sigma^2_{X_{1}} &amp;amp; \rho\cdot\sigma_{X_{2}}\cdot\sigma_{X_{1}} \\ \rho\cdot \sigma_{X_{1}}\cdot\sigma_{X_{2}} &amp;amp; \sigma^2_{X_{2}} \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, we say &lt;span class=&#34;math inline&#34;&gt;\(X \sim N(\mu,\sum)\)&lt;/span&gt;. To sample from a multivariate normal distribution in R, we use the following &lt;a href=&#34;http://www.math.smith.edu/~nhorton/R/multiv.R&#34;&gt;user-defined&lt;/a&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define a function for generating random multivariate normals
rmultnorm &amp;lt;- function(n, mu, vmat, tol = 1e-07) {
  p &amp;lt;- ncol(vmat)
  if (length(mu) != p) {
    stop(&amp;quot;alright, alright, alright, mu vector is the wrong length&amp;quot;)
  }
  if (max(abs(vmat - t(vmat))) &amp;gt; tol) {
    stop(&amp;quot;vmat not symmetric&amp;quot;)
  }
  vs &amp;lt;- svd(vmat)
  vsqrt &amp;lt;- t(vs$v %*% (t(vs$u) * sqrt(vs$d)))
  ans &amp;lt;- matrix(rnorm(n * p), nrow = n) %*% vsqrt
  ans &amp;lt;- sweep(ans, 2, mu, &amp;quot;+&amp;quot;)
  dimnames(ans) &amp;lt;- list(NULL, dimnames(vmat)[[2]])
  return(ans)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       We now have a function for genearting random multivariate normals. If we would like simulate observations from a bivariate normal distribution with mean 100, variance 14. And if we wish to specify a &lt;em&gt;moderately&lt;/em&gt; negative association based on &lt;a href=&#34;https://journals.sagepub.com/doi/10.1177/014662168801200410&#34;&gt;Cohen (1988)&lt;/a&gt; and its convention for interpreting the strength of the correlation coefficient, we need to find &lt;span class=&#34;math inline&#34;&gt;\(X_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{2}\)&lt;/span&gt; such that their correlation coefficient, &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\approx\)&lt;/span&gt; -0.3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\frac{Cov_{X_{1}X_{2}}}{\sigma_{X_{1}}\sigma_{X_{2}}}&amp;amp;=-0.3\\
Cov_{X_{1}X_{2}}&amp;amp;=\sigma_{X_{1}}\sigma_{X_{2}}\cdot(-0.3)\\
Cov_{X_{1}X_{2}}&amp;amp;=-0.3\sqrt{14}\sqrt{14}\\
Cov_{X_{1}X_{2}}&amp;amp;\approx-4.2
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We modify the inputs of the function defined above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sample size
n &amp;lt;- 100
# Mean Vector
mean &amp;lt;- c(100, 100)
# Variance
var_1 &amp;lt;- 14
var_2 &amp;lt;- var_1
# Covariance
cov &amp;lt;- -0.3 * sqrt(14) * sqrt(14)
# Generate two normal random variables with rho = -0.3
set.seed(12)
moderate_negative &amp;lt;- rmultnorm(n, mean, matrix(c(var_1, cov, cov, var_2), nrow = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a scatter plot on the xy-plane:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot()
ggplot(
  data = data.frame(moderate_negative),
  mapping = aes(
    x = moderate_negative[, 1],
    y = moderate_negative[, 2]
  )
) +
  geom_point(color = &amp;quot;orange&amp;quot;) +
  xlim(c(86, 115)) +
  ylim(c(88, 112)) +
  ggtitle(
    paste(
      &amp;quot;Scatter Plot of Bivariate Normal Random Variables
                 with Moderate Negative Association (rho = &amp;quot;,
      round(cor(moderate_negative[, 1], moderate_negative[, 2]), digits = 2),
      &amp;quot;)&amp;quot;
    )
  ) +
  xlab(&amp;quot;X1&amp;quot;) +
  ylab(&amp;quot;X2&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;azure2&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s a 3D plot of the joint probability density function of the two random variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bandwidth
# Using a rule-of-thumb for choosing the bandwidth of a Gaussian kernel density estimator
bw_vector &amp;lt;- c(bandwidth.nrd(moderate_negative[, 1]), bandwidth.nrd(moderate_negative[, 2]))
bw_moderate_negative &amp;lt;- mean(bw_vector)
# Using the kde2d() function from the MASS package
# kde2d() is used for 2d kernel density estimation
joint_pdf &amp;lt;- kde2d(x = moderate_negative[, 1], y = moderate_negative[, 2], h = bw_moderate_negative)
# Using plot_ly() from the plotly graphics library
# the pipe operator %&amp;gt;% will forward the result of an expression into the next function call
joint_pdf2d &amp;lt;- plot_ly(x = joint_pdf$x, y = joint_pdf$y, z = joint_pdf$z)
joint_pdf2d &amp;lt;- joint_pdf2d %&amp;gt;% layout(title = &amp;quot;Bivariate Density&amp;quot;)
joint_pdf2d &amp;lt;- joint_pdf2d %&amp;gt;% add_surface()
joint_pdf2d&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:576px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;17ca3451f1f6&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;17ca3451f1f6&#34;,&#34;attrs&#34;:{&#34;17ca3451f1f6&#34;:{&#34;x&#34;:[91.1225845792483,91.8159946067567,92.5094046342652,93.2028146617736,93.8962246892821,94.5896347167905,95.283044744299,95.9764547718074,96.6698647993159,97.3632748268243,98.0566848543328,98.7500948818412,99.4435049093497,100.136914936858,100.830324964367,101.523734991875,102.217145019383,102.910555046892,103.6039650744,104.297375101909,104.990785129417,105.684195156926,106.377605184434,107.071015211943,107.764425239451],&#34;y&#34;:[92.9673324212696,93.5922905492131,94.2172486771566,94.8422068051002,95.4671649330437,96.0921230609872,96.7170811889308,97.3420393168743,97.9669974448178,98.5919555727614,99.2169137007049,99.8418718286484,100.466829956592,101.091788084535,101.716746212479,102.341704340423,102.966662468366,103.59162059631,104.216578724253,104.841536852197,105.46649498014,106.091453108084,106.716411236027,107.341369363971,107.966327491914],&#34;z&#34;:[[1.87011334411545e-06,2.39103948308038e-06,3.09355817870029e-06,4.20287894859379e-06,5.97683293869588e-06,8.75797484293117e-06,1.33022742052132e-05,2.17830463865876e-05,3.97229393077791e-05,7.73939186044429e-05,0.000146521071133156,0.000249180019212858,0.000364911760714294,0.000451891196940173,0.000471017404224172,0.000417182420569302,0.0003290814515624,0.000268948106008751,0.000291686994746042,0.000423078810626483,0.000647681680409775,0.000903714632905289,0.00109660701196013,0.00114135973147392,0.00101337490903684],[9.02250985979411e-06,1.13793404169283e-05,1.44116941284174e-05,1.90523733681906e-05,2.62015441498829e-05,3.65381851853952e-05,5.06424700211677e-05,7.00429027098206e-05,0.00010019266074852,0.000153773400769353,0.000247602013931228,0.000387129095410334,0.000546558336914926,0.000668445012157964,0.000697261636159005,0.000624881549104817,0.000505427729983345,0.000425038906435183,0.000454821847700905,0.000619663122231587,0.000888727643628531,0.00118086071428972,0.00138815536767879,0.00142145110503088,0.00125851326471792],[3.50901121985418e-05,4.37544716762594e-05,5.43694941879922e-05,7.01259937507076e-05,9.38054829614379e-05,0.000126649857919798,0.000167506742583469,0.00021295636017687,0.000261530662023761,0.000321704101928853,0.000413401858558368,0.00055110894476914,0.000716089463627292,0.000849201023852298,0.000886827539107952,0.00081625791405419,0.00069718860987325,0.00062690344609418,0.000681564571863745,0.000878224244934674,0.00117042600494385,0.00146568985394287,0.0016562526567515,0.00166037272615788,0.00146141154447764],[0.000110386704337733,0.000136432342588468,0.00016673892279024,0.00021022482218276,0.000274203650571324,0.000360732092527133,0.000463530004018578,0.000565734820590972,0.000646171745781653,0.000697717820169826,0.000741526365296987,0.000813014061306095,0.000922546736463707,0.00103116703174935,0.0010776229124048,0.00103872445134119,0.000962065971745845,0.000935908611753339,0.00102612235072696,0.0012357379271868,0.0015082316523355,0.00175509406718307,0.00188550447124127,0.00183497518446083,0.00159344797680931],[0.000282271349787745,0.000346881157040786,0.000418113647142235,0.000516229281405287,0.000657391044436295,0.000844797191004771,0.00106199192427942,0.001266918244565,0.00140213506129521,0.00143204726715937,0.001380288795644,0.00132122955655319,0.00132017933664458,0.00137764648301089,0.0014377216682186,0.00145429080417838,0.00144386608098416,0.00146875464546744,0.00157356498659352,0.00174489925452883,0.00192530219719534,0.00204861934735179,0.00205883308828067,0.00191756463615695,0.0016214584977227],[0.000591016634510215,0.000725075662729436,0.000864981802476532,0.00104790819092238,0.00130390196005485,0.00163854193745999,0.00202131261583164,0.0023750140599892,0.00259221830740394,0.00259983313477515,0.0024285414992727,0.00220897230300517,0.00207946677974861,0.00208615583248479,0.00217001040426957,0.00224673965297074,0.00228936437232589,0.00232834205159719,0.00238627426521963,0.00243739748398448,0.00243399341652674,0.00234809227630777,0.00217279284563039,0.00190276919726711,0.00154085614433158],[0.00102458459236045,0.00126203261220834,0.00149728291956262,0.00178457366825613,0.00217109356755748,0.00266913831634063,0.00323724700198982,0.00376278071861784,0.00408509392948947,0.00409179785661598,0.00382381558880089,0.00347302851821725,0.00324977087492117,0.00323554358286649,0.00335470317947528,0.00347647005038331,0.00352954961240491,0.00351623768733637,0.00344352204880993,0.0032829317681945,0.00300851590228608,0.00264164992945169,0.00223199503547275,0.00180980875227289,0.00138136690118761],[0.00149687584204009,0.00186685798750663,0.00221852575710951,0.00261185452724667,0.00310936015102939,0.00373963962913605,0.00446580789868289,0.00515407821725739,0.00559969753956605,0.00565370213673833,0.00536223942377539,0.00496410924115754,0.00472008093501347,0.00472710854292219,0.00488534801830522,0.00502389794218815,0.00503797924425597,0.00490893640889358,0.00463429999185263,0.0041931327855555,0.00359270545144297,0.00290987015604872,0.00225074077678257,0.00167784435360889,0.00119523686857418],[0.00189603434362354,0.00242351568365005,0.00291449694471216,0.00340999413929136,0.00397825236597954,0.004677670316874,0.00550337307112638,0.00632704740438619,0.00691574866743407,0.00708412805562546,0.00686638796635438,0.00651832647065865,0.00632068751666932,0.00636949220288652,0.00654556681988226,0.00665697039859458,0.00658347140971427,0.0062946652492409,0.00578410109479652,0.00504554148062816,0.00412410263843703,0.00314811661721372,0.00226976775565565,0.00157376504745076,0.00105513155334387],[0.00217262918386838,0.00288664419437683,0.00355379067202752,0.00416359519381571,0.00477220982811543,0.00547872488005511,0.00634132248994623,0.00726936493299941,0.00801966789569586,0.00836578469901739,0.0083029820456058,0.00806743064883594,0.00793332710069597,0.00799213759537864,0.0081218851773215,0.00813329215791915,0.0079109305810746,0.00742776333571041,0.00668500778587716,0.00569577931275299,0.00453406293725154,0.00335756845265364,0.00233961944332525,0.00157045234296848,0.00103175012989031],[0.00236999283549946,0.0032994004457189,0.00418412983626856,0.00493489823431805,0.00557455573745505,0.00624337970630743,0.00708450881364446,0.00807546518359225,0.00898475009854803,0.00954337481791338,0.00968007508682349,0.00957391801984507,0.00947033846621183,0.00946124460940762,0.00944456141780364,0.00925844200162158,0.00881433269938848,0.00811065195201446,0.00717375372612907,0.00603563270095524,0.00477396993514846,0.00353627255004935,0.00248263040721494,0.0016932567864427,0.00114074738461662],[0.00255648281745146,0.00369583034117764,0.0047977157806274,0.00568980205448956,0.00634984373869069,0.0069476643885302,0.00771193544075069,0.00870677452534699,0.00973850517366604,0.010507834221654,0.0108614658188879,0.0108878859862303,0.0107784662121675,0.010626209836945,0.0103709589443817,0.00990646321778126,0.00919712773796003,0.00828843010124306,0.00724091568721884,0.00609071858363926,0.00488151395662591,0.00370790816817359,0.00269083947451908,0.00190057668867476,0.001319512531313],[0.00274296397193584,0.00401164464287966,0.00523869212665418,0.0062008573210402,0.0068432294911037,0.00734770882718321,0.00800112315029332,0.00894625829014939,0.0100462598714747,0.0109970167522618,0.0115696002277998,0.0117434550704982,0.0116302631334982,0.011314342785821,0.0107912866457027,0.0100373750782781,0.00909383304176792,0.00806249491221437,0.00702759316288337,0.00600082855068426,0.00495595654997507,0.00390807821184538,0.00293483284263437,0.00211551302672175,0.00147141959193188],[0.00288134250412659,0.00413234560400425,0.00531584059227615,0.00621902958627432,0.00679044519474177,0.00720684247376342,0.0077613602212396,0.00863458029761061,0.00974523058669457,0.0108147482322802,0.0115714985926756,0.0118983386825477,0.0118179464789621,0.0113913760488151,0.0106657590861049,0.00970567928985203,0.00863194199804296,0.00759256029212062,0.00667528267658922,0.00584851105690073,0.00500702100315701,0.00408811625229436,0.00313640235703505,0.00225707541322147,0.00152894235948206],[0.00292487957596522,0.00402066451321897,0.00501210033827424,0.00575545358454852,0.00623764842581557,0.00661109138936076,0.00711358681021209,0.0079037403549372,0.00893798074151837,0.00999574604503774,0.0108222984001817,0.0112551531791753,0.0112465802183456,0.0108197780638033,0.0100415984042084,0.00903064415550463,0.00795597253978282,0.0069869839387191,0.00620634629246545,0.00555530104291846,0.00488328525035029,0.0040787754853971,0.00316091151854623,0.00225071297174376,0.0014720772198086],[0.00286499415006566,0.00375569145935151,0.00452227834572062,0.00511240139336093,0.00555684851328508,0.00595920860544255,0.00645198542167769,0.00712119766552948,0.00794285019223999,0.00878796932431601,0.00949246486325,0.00992430627681423,0.0100032542452852,0.00969873231636636,0.00904211921970225,0.00814111297143075,0.00716266004032553,0.00627440890738531,0.00556451125355157,0.00499059152472352,0.00441528335173782,0.0037190835805533,0.00289574153606871,0.00204839476877073,0.00130687859593374],[0.00271633129615889,0.00344117506376342,0.00405950070522879,0.00459156686876367,0.00508757249956234,0.00558214558585674,0.00608412526755575,0.00658634474809407,0.00707297210030935,0.00752538835450611,0.00792419411144824,0.00823298335858312,0.00837740848021732,0.00826233626587149,0.00783351349925603,0.00713026971936235,0.00628062456206634,0.00544251675763149,0.00472598866711319,0.00413962549214836,0.00360160937275582,0.00301530612052606,0.00235037940778531,0.00166369258487644,0.00105256576397318],[0.00248421953084787,0.00310136398170855,0.00365874294598314,0.00421379923216339,0.00480894063869362,0.00541205562400623,0.0059297890192653,0.00627340962976466,0.00642334700567878,0.00645135145795684,0.0064792461057872,0.00658642018392851,0.00673207723961198,0.0067694729549068,0.00655056968735544,0.0060306552552093,0.00529162301402639,0.00448439941673381,0.00374431454385972,0.00313091775661677,0.00262007041320964,0.00214651355594671,0.00166443403885472,0.00118228153010148,0.000750233936825724],[0.00215286698383106,0.00267871631491649,0.00319267109857742,0.00376205700339877,0.00441364032294727,0.0050776873304962,0.00560967325064784,0.00587287081448873,0.00582939597131291,0.00558295724245398,0.00533050219057751,0.00523336075002997,0.00529744336279478,0.00536791749663989,0.00524812482451166,0.0048385543456765,0.00418676941314704,0.00343423032477944,0.00272503106758463,0.00214193779117687,0.00169238303706245,0.001334061325391,0.00101791623282433,0.000722796695800565,0.000461305153063027],[0.00171566362953023,0.00213101356457451,0.00256311838862125,0.003074579615084,0.0036833263592779,0.00431929304556425,0.00484232528646507,0.00510983252953663,0.00506102192201451,0.00477076029503512,0.00442228481627392,0.004192726698998,0.0041280691506385,0.00411477834195754,0.00397645995589488,0.00360886322264827,0.00304105032540076,0.00239348277883805,0.00179281872166766,0.0013132990500131,0.000966205178973438,0.000720426962068635,0.000532990548595361,0.000374762099918213,0.000239571033384792],[0.00122357228127691,0.00152196621095253,0.00184857906824881,0.00225131801363966,0.00274228530301803,0.00327139006309774,0.00373789359293946,0.00403047895570707,0.00408220500844907,0.00391634973111974,0.00364427242973667,0.00339690788784358,0.00323179358917424,0.00309531513432146,0.00287809884159909,0.00251294510777295,0.00202672467691848,0.00151169795352123,0.00105943364182158,0.000717226964129065,0.000485327511058247,0.000336430496807846,0.000237132970714683,0.000162924677049895,0.000103472858738945],[0.000782414578502858,0.000998010894368245,0.00125044838385874,0.00156364752946556,0.0019352694894436,0.00233004976852847,0.00269163446249268,0.00296051301423946,0.00309409104363937,0.00308634145414701,0.00297315932183384,0.00280921159086737,0.00262599636920634,0.00240788013345982,0.00211495956334445,0.00173413810628595,0.00130603877027568,0.000901910152660608,0.000578709371518447,0.000354723088334626,0.00021610925572305,0.00013603036634207,8.930651305618e-05,5.89414543566549e-05,3.67919457213681e-05],[0.000480205655323605,0.000668344461051084,0.00090773562744362,0.00119350148807351,0.00149733536986268,0.0017808841111405,0.00201637832615091,0.00219566469855013,0.00232327891456157,0.00240278560979424,0.00242817966461155,0.00238478557100195,0.00225466608142535,0.00202349332780952,0.00169346183675693,0.00129814517960626,0.000899972691563504,0.000562681417728649,0.000320665652161062,0.000171228655855429,8.97634442190927e-05,4.89367935548397e-05,2.87371915335802e-05,1.77310924829687e-05,1.07149975641974e-05],[0.000322747301802389,0.00051938421565177,0.00078320589792828,0.00108459739349008,0.00136778686203278,0.00157986585707457,0.00170279271041875,0.00176353797990375,0.00181322504571895,0.00188696726216186,0.00197304338331036,0.00201690417511176,0.00195613692352463,0.00175803960059682,0.00143807478228361,0.00105513355471763,0.000686626131028592,0.000394053090961356,0.000200140689499709,9.16377290281987e-05,3.94765328724076e-05,1.72472848505869e-05,8.31678588055811e-06,4.51511177385327e-06,2.56264794419386e-06],[0.000247986986089118,0.000448816909610532,0.000723611494149579,0.00103014555769676,0.00129781071400225,0.00146330860651205,0.00150854700214513,0.00147412508719009,0.0014359446668045,0.00145428505576341,0.00152890797743422,0.00159739335236103,0.0015792680838984,0.0014288875392946,0.00115992779488708,0.000833615484424724,0.000524839434264461,0.000287187602884249,0.000136092390796634,5.60959276611541e-05,2.05210374480404e-05,7.02566546930307e-06,2.50530699978789e-06,1.04987897193029e-06,5.16034402361683e-07]],&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;surface&#34;,&#34;inherit&#34;:true}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;title&#34;:&#34;Bivariate Density&#34;,&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:[]},&#34;yaxis&#34;:{&#34;title&#34;:[]},&#34;zaxis&#34;:{&#34;title&#34;:[]}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;yanchor&#34;:&#34;top&#34;,&#34;y&#34;:0.5}},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;colorbar&#34;:{&#34;title&#34;:&#34;&#34;,&#34;ticklen&#34;:2,&#34;len&#34;:0.5,&#34;lenmode&#34;:&#34;fraction&#34;,&#34;y&#34;:1,&#34;yanchor&#34;:&#34;top&#34;},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:true,&#34;x&#34;:[91.1225845792483,91.8159946067567,92.5094046342652,93.2028146617736,93.8962246892821,94.5896347167905,95.283044744299,95.9764547718074,96.6698647993159,97.3632748268243,98.0566848543328,98.7500948818412,99.4435049093497,100.136914936858,100.830324964367,101.523734991875,102.217145019383,102.910555046892,103.6039650744,104.297375101909,104.990785129417,105.684195156926,106.377605184434,107.071015211943,107.764425239451],&#34;y&#34;:[92.9673324212696,93.5922905492131,94.2172486771566,94.8422068051002,95.4671649330437,96.0921230609872,96.7170811889308,97.3420393168743,97.9669974448178,98.5919555727614,99.2169137007049,99.8418718286484,100.466829956592,101.091788084535,101.716746212479,102.341704340423,102.966662468366,103.59162059631,104.216578724253,104.841536852197,105.46649498014,106.091453108084,106.716411236027,107.341369363971,107.966327491914],&#34;z&#34;:[[1.87011334411545e-06,2.39103948308038e-06,3.09355817870029e-06,4.20287894859379e-06,5.97683293869588e-06,8.75797484293117e-06,1.33022742052132e-05,2.17830463865876e-05,3.97229393077791e-05,7.73939186044429e-05,0.000146521071133156,0.000249180019212858,0.000364911760714294,0.000451891196940173,0.000471017404224172,0.000417182420569302,0.0003290814515624,0.000268948106008751,0.000291686994746042,0.000423078810626483,0.000647681680409775,0.000903714632905289,0.00109660701196013,0.00114135973147392,0.00101337490903684],[9.02250985979411e-06,1.13793404169283e-05,1.44116941284174e-05,1.90523733681906e-05,2.62015441498829e-05,3.65381851853952e-05,5.06424700211677e-05,7.00429027098206e-05,0.00010019266074852,0.000153773400769353,0.000247602013931228,0.000387129095410334,0.000546558336914926,0.000668445012157964,0.000697261636159005,0.000624881549104817,0.000505427729983345,0.000425038906435183,0.000454821847700905,0.000619663122231587,0.000888727643628531,0.00118086071428972,0.00138815536767879,0.00142145110503088,0.00125851326471792],[3.50901121985418e-05,4.37544716762594e-05,5.43694941879922e-05,7.01259937507076e-05,9.38054829614379e-05,0.000126649857919798,0.000167506742583469,0.00021295636017687,0.000261530662023761,0.000321704101928853,0.000413401858558368,0.00055110894476914,0.000716089463627292,0.000849201023852298,0.000886827539107952,0.00081625791405419,0.00069718860987325,0.00062690344609418,0.000681564571863745,0.000878224244934674,0.00117042600494385,0.00146568985394287,0.0016562526567515,0.00166037272615788,0.00146141154447764],[0.000110386704337733,0.000136432342588468,0.00016673892279024,0.00021022482218276,0.000274203650571324,0.000360732092527133,0.000463530004018578,0.000565734820590972,0.000646171745781653,0.000697717820169826,0.000741526365296987,0.000813014061306095,0.000922546736463707,0.00103116703174935,0.0010776229124048,0.00103872445134119,0.000962065971745845,0.000935908611753339,0.00102612235072696,0.0012357379271868,0.0015082316523355,0.00175509406718307,0.00188550447124127,0.00183497518446083,0.00159344797680931],[0.000282271349787745,0.000346881157040786,0.000418113647142235,0.000516229281405287,0.000657391044436295,0.000844797191004771,0.00106199192427942,0.001266918244565,0.00140213506129521,0.00143204726715937,0.001380288795644,0.00132122955655319,0.00132017933664458,0.00137764648301089,0.0014377216682186,0.00145429080417838,0.00144386608098416,0.00146875464546744,0.00157356498659352,0.00174489925452883,0.00192530219719534,0.00204861934735179,0.00205883308828067,0.00191756463615695,0.0016214584977227],[0.000591016634510215,0.000725075662729436,0.000864981802476532,0.00104790819092238,0.00130390196005485,0.00163854193745999,0.00202131261583164,0.0023750140599892,0.00259221830740394,0.00259983313477515,0.0024285414992727,0.00220897230300517,0.00207946677974861,0.00208615583248479,0.00217001040426957,0.00224673965297074,0.00228936437232589,0.00232834205159719,0.00238627426521963,0.00243739748398448,0.00243399341652674,0.00234809227630777,0.00217279284563039,0.00190276919726711,0.00154085614433158],[0.00102458459236045,0.00126203261220834,0.00149728291956262,0.00178457366825613,0.00217109356755748,0.00266913831634063,0.00323724700198982,0.00376278071861784,0.00408509392948947,0.00409179785661598,0.00382381558880089,0.00347302851821725,0.00324977087492117,0.00323554358286649,0.00335470317947528,0.00347647005038331,0.00352954961240491,0.00351623768733637,0.00344352204880993,0.0032829317681945,0.00300851590228608,0.00264164992945169,0.00223199503547275,0.00180980875227289,0.00138136690118761],[0.00149687584204009,0.00186685798750663,0.00221852575710951,0.00261185452724667,0.00310936015102939,0.00373963962913605,0.00446580789868289,0.00515407821725739,0.00559969753956605,0.00565370213673833,0.00536223942377539,0.00496410924115754,0.00472008093501347,0.00472710854292219,0.00488534801830522,0.00502389794218815,0.00503797924425597,0.00490893640889358,0.00463429999185263,0.0041931327855555,0.00359270545144297,0.00290987015604872,0.00225074077678257,0.00167784435360889,0.00119523686857418],[0.00189603434362354,0.00242351568365005,0.00291449694471216,0.00340999413929136,0.00397825236597954,0.004677670316874,0.00550337307112638,0.00632704740438619,0.00691574866743407,0.00708412805562546,0.00686638796635438,0.00651832647065865,0.00632068751666932,0.00636949220288652,0.00654556681988226,0.00665697039859458,0.00658347140971427,0.0062946652492409,0.00578410109479652,0.00504554148062816,0.00412410263843703,0.00314811661721372,0.00226976775565565,0.00157376504745076,0.00105513155334387],[0.00217262918386838,0.00288664419437683,0.00355379067202752,0.00416359519381571,0.00477220982811543,0.00547872488005511,0.00634132248994623,0.00726936493299941,0.00801966789569586,0.00836578469901739,0.0083029820456058,0.00806743064883594,0.00793332710069597,0.00799213759537864,0.0081218851773215,0.00813329215791915,0.0079109305810746,0.00742776333571041,0.00668500778587716,0.00569577931275299,0.00453406293725154,0.00335756845265364,0.00233961944332525,0.00157045234296848,0.00103175012989031],[0.00236999283549946,0.0032994004457189,0.00418412983626856,0.00493489823431805,0.00557455573745505,0.00624337970630743,0.00708450881364446,0.00807546518359225,0.00898475009854803,0.00954337481791338,0.00968007508682349,0.00957391801984507,0.00947033846621183,0.00946124460940762,0.00944456141780364,0.00925844200162158,0.00881433269938848,0.00811065195201446,0.00717375372612907,0.00603563270095524,0.00477396993514846,0.00353627255004935,0.00248263040721494,0.0016932567864427,0.00114074738461662],[0.00255648281745146,0.00369583034117764,0.0047977157806274,0.00568980205448956,0.00634984373869069,0.0069476643885302,0.00771193544075069,0.00870677452534699,0.00973850517366604,0.010507834221654,0.0108614658188879,0.0108878859862303,0.0107784662121675,0.010626209836945,0.0103709589443817,0.00990646321778126,0.00919712773796003,0.00828843010124306,0.00724091568721884,0.00609071858363926,0.00488151395662591,0.00370790816817359,0.00269083947451908,0.00190057668867476,0.001319512531313],[0.00274296397193584,0.00401164464287966,0.00523869212665418,0.0062008573210402,0.0068432294911037,0.00734770882718321,0.00800112315029332,0.00894625829014939,0.0100462598714747,0.0109970167522618,0.0115696002277998,0.0117434550704982,0.0116302631334982,0.011314342785821,0.0107912866457027,0.0100373750782781,0.00909383304176792,0.00806249491221437,0.00702759316288337,0.00600082855068426,0.00495595654997507,0.00390807821184538,0.00293483284263437,0.00211551302672175,0.00147141959193188],[0.00288134250412659,0.00413234560400425,0.00531584059227615,0.00621902958627432,0.00679044519474177,0.00720684247376342,0.0077613602212396,0.00863458029761061,0.00974523058669457,0.0108147482322802,0.0115714985926756,0.0118983386825477,0.0118179464789621,0.0113913760488151,0.0106657590861049,0.00970567928985203,0.00863194199804296,0.00759256029212062,0.00667528267658922,0.00584851105690073,0.00500702100315701,0.00408811625229436,0.00313640235703505,0.00225707541322147,0.00152894235948206],[0.00292487957596522,0.00402066451321897,0.00501210033827424,0.00575545358454852,0.00623764842581557,0.00661109138936076,0.00711358681021209,0.0079037403549372,0.00893798074151837,0.00999574604503774,0.0108222984001817,0.0112551531791753,0.0112465802183456,0.0108197780638033,0.0100415984042084,0.00903064415550463,0.00795597253978282,0.0069869839387191,0.00620634629246545,0.00555530104291846,0.00488328525035029,0.0040787754853971,0.00316091151854623,0.00225071297174376,0.0014720772198086],[0.00286499415006566,0.00375569145935151,0.00452227834572062,0.00511240139336093,0.00555684851328508,0.00595920860544255,0.00645198542167769,0.00712119766552948,0.00794285019223999,0.00878796932431601,0.00949246486325,0.00992430627681423,0.0100032542452852,0.00969873231636636,0.00904211921970225,0.00814111297143075,0.00716266004032553,0.00627440890738531,0.00556451125355157,0.00499059152472352,0.00441528335173782,0.0037190835805533,0.00289574153606871,0.00204839476877073,0.00130687859593374],[0.00271633129615889,0.00344117506376342,0.00405950070522879,0.00459156686876367,0.00508757249956234,0.00558214558585674,0.00608412526755575,0.00658634474809407,0.00707297210030935,0.00752538835450611,0.00792419411144824,0.00823298335858312,0.00837740848021732,0.00826233626587149,0.00783351349925603,0.00713026971936235,0.00628062456206634,0.00544251675763149,0.00472598866711319,0.00413962549214836,0.00360160937275582,0.00301530612052606,0.00235037940778531,0.00166369258487644,0.00105256576397318],[0.00248421953084787,0.00310136398170855,0.00365874294598314,0.00421379923216339,0.00480894063869362,0.00541205562400623,0.0059297890192653,0.00627340962976466,0.00642334700567878,0.00645135145795684,0.0064792461057872,0.00658642018392851,0.00673207723961198,0.0067694729549068,0.00655056968735544,0.0060306552552093,0.00529162301402639,0.00448439941673381,0.00374431454385972,0.00313091775661677,0.00262007041320964,0.00214651355594671,0.00166443403885472,0.00118228153010148,0.000750233936825724],[0.00215286698383106,0.00267871631491649,0.00319267109857742,0.00376205700339877,0.00441364032294727,0.0050776873304962,0.00560967325064784,0.00587287081448873,0.00582939597131291,0.00558295724245398,0.00533050219057751,0.00523336075002997,0.00529744336279478,0.00536791749663989,0.00524812482451166,0.0048385543456765,0.00418676941314704,0.00343423032477944,0.00272503106758463,0.00214193779117687,0.00169238303706245,0.001334061325391,0.00101791623282433,0.000722796695800565,0.000461305153063027],[0.00171566362953023,0.00213101356457451,0.00256311838862125,0.003074579615084,0.0036833263592779,0.00431929304556425,0.00484232528646507,0.00510983252953663,0.00506102192201451,0.00477076029503512,0.00442228481627392,0.004192726698998,0.0041280691506385,0.00411477834195754,0.00397645995589488,0.00360886322264827,0.00304105032540076,0.00239348277883805,0.00179281872166766,0.0013132990500131,0.000966205178973438,0.000720426962068635,0.000532990548595361,0.000374762099918213,0.000239571033384792],[0.00122357228127691,0.00152196621095253,0.00184857906824881,0.00225131801363966,0.00274228530301803,0.00327139006309774,0.00373789359293946,0.00403047895570707,0.00408220500844907,0.00391634973111974,0.00364427242973667,0.00339690788784358,0.00323179358917424,0.00309531513432146,0.00287809884159909,0.00251294510777295,0.00202672467691848,0.00151169795352123,0.00105943364182158,0.000717226964129065,0.000485327511058247,0.000336430496807846,0.000237132970714683,0.000162924677049895,0.000103472858738945],[0.000782414578502858,0.000998010894368245,0.00125044838385874,0.00156364752946556,0.0019352694894436,0.00233004976852847,0.00269163446249268,0.00296051301423946,0.00309409104363937,0.00308634145414701,0.00297315932183384,0.00280921159086737,0.00262599636920634,0.00240788013345982,0.00211495956334445,0.00173413810628595,0.00130603877027568,0.000901910152660608,0.000578709371518447,0.000354723088334626,0.00021610925572305,0.00013603036634207,8.930651305618e-05,5.89414543566549e-05,3.67919457213681e-05],[0.000480205655323605,0.000668344461051084,0.00090773562744362,0.00119350148807351,0.00149733536986268,0.0017808841111405,0.00201637832615091,0.00219566469855013,0.00232327891456157,0.00240278560979424,0.00242817966461155,0.00238478557100195,0.00225466608142535,0.00202349332780952,0.00169346183675693,0.00129814517960626,0.000899972691563504,0.000562681417728649,0.000320665652161062,0.000171228655855429,8.97634442190927e-05,4.89367935548397e-05,2.87371915335802e-05,1.77310924829687e-05,1.07149975641974e-05],[0.000322747301802389,0.00051938421565177,0.00078320589792828,0.00108459739349008,0.00136778686203278,0.00157986585707457,0.00170279271041875,0.00176353797990375,0.00181322504571895,0.00188696726216186,0.00197304338331036,0.00201690417511176,0.00195613692352463,0.00175803960059682,0.00143807478228361,0.00105513355471763,0.000686626131028592,0.000394053090961356,0.000200140689499709,9.16377290281987e-05,3.94765328724076e-05,1.72472848505869e-05,8.31678588055811e-06,4.51511177385327e-06,2.56264794419386e-06],[0.000247986986089118,0.000448816909610532,0.000723611494149579,0.00103014555769676,0.00129781071400225,0.00146330860651205,0.00150854700214513,0.00147412508719009,0.0014359446668045,0.00145428505576341,0.00152890797743422,0.00159739335236103,0.0015792680838984,0.0014288875392946,0.00115992779488708,0.000833615484424724,0.000524839434264461,0.000287187602884249,0.000136092390796634,5.60959276611541e-05,2.05210374480404e-05,7.02566546930307e-06,2.50530699978789e-06,1.04987897193029e-06,5.16034402361683e-07]],&#34;type&#34;:&#34;surface&#34;,&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;       As can be seen, when &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is negative, the bell-shaped surface becomes flattened on a negative sloping line extending out towards the top left and bottom right corners. So for &lt;span class=&#34;math inline&#34;&gt;\(\rho &amp;lt; 0\)&lt;/span&gt;, X1 varies negatively with X2 (X2 is denoted as Y in the figure above).&lt;/p&gt;
&lt;p&gt;       To simulate observations from a bivariate normal distribution with a &lt;em&gt;strong&lt;/em&gt; negative association, say, &lt;span class=&#34;math inline&#34;&gt;\(\rho = -0.8\)&lt;/span&gt;, we again modify the inputs to the function we defined in the beginning. Here are the scatter plot and the plot of their joint probability function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot()
ggplot(
  data = data.frame(strong_negative),
  mapping = aes(
    x = strong_negative[, 1],
    y = strong_negative[, 2]
  )
) +
  geom_point(color = &amp;quot;orange&amp;quot;) +
  xlim(c(89, 112)) +
  ylim(c(86, 111)) +
  ggtitle(
    paste(
      &amp;quot;Scatter Plot of Bivariate Normal Random Variables
                 with Strong Negative Association (rho = &amp;quot;,
      round(cor(strong_negative[, 1], strong_negative[, 2]), digits = 2),
      &amp;quot;)&amp;quot;
    )
  ) +
  xlab(&amp;quot;X1&amp;quot;) +
  ylab(&amp;quot;X2&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;azure2&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bandwidth
bw_vector &amp;lt;- c(bandwidth.nrd(strong_negative[, 1]), bandwidth.nrd(strong_negative[, 2]))
bw_strong_negative &amp;lt;- mean(bw_vector)
# 2d kernel density estimation
joint_pdf &amp;lt;- kde2d(x = strong_negative[, 1], y = strong_negative[, 2], h = bw_strong_negative)
# Using plot_ly()
joint_pdf2d &amp;lt;- plot_ly(x = joint_pdf$x, y = joint_pdf$y, z = joint_pdf$z)
joint_pdf2d &amp;lt;- joint_pdf2d %&amp;gt;% layout(title = &amp;quot;Bivariate Density&amp;quot;)
joint_pdf2d &amp;lt;- joint_pdf2d %&amp;gt;% add_surface()
joint_pdf2d&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:576px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;17ca53b65cd8&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;17ca53b65cd8&#34;,&#34;attrs&#34;:{&#34;17ca53b65cd8&#34;:{&#34;x&#34;:[88.9398907298665,89.8067295301701,90.6735683304737,91.5404071307773,92.4072459310809,93.2740847313845,94.1409235316881,95.0077623319917,95.8746011322953,96.7414399325989,97.6082787329024,98.475117533206,99.3419563335096,100.208795133813,101.075633934117,101.94247273442,102.809311534724,103.676150335028,104.542989135331,105.409827935635,106.276666735938,107.143505536242,108.010344336546,108.877183136849,109.744021937153],&#34;y&#34;:[92.3004145455501,92.9920082957279,93.6836020459057,94.3751957960835,95.0667895462613,95.758383296439,96.4499770466168,97.1415707967946,97.8331645469724,98.5247582971502,99.2163520473279,99.9079457975057,100.599539547683,101.291133297861,101.982727048039,102.674320798217,103.365914548395,104.057508298572,104.74910204875,105.440695798928,106.132289549106,106.823883299284,107.515477049461,108.207070799639,108.898664549817],&#34;z&#34;:[[6.85666889382839e-11,2.56461062778276e-10,9.14548163093976e-10,3.26626647514284e-09,1.12750183663048e-08,3.4785687172498e-08,9.07838153113766e-08,1.99001293966871e-07,3.97141310428745e-07,9.22358341739491e-07,3.02026975362924e-06,1.13066974739126e-05,3.8170877923777e-05,0.000106719835351139,0.000241828993363136,0.00044269764711603,0.000659596627729255,0.000821128669645684,0.000910306523017004,0.000998058016700087,0.00116204483952298,0.00137182421139454,0.00149162068465402,0.00140776035786185,0.00113026510110035],[1.11615835674272e-09,3.84835427750987e-09,1.19025888503256e-08,3.57462738075808e-08,1.07444931103708e-07,3.06461164075519e-07,7.68178653776298e-07,1.61731450130647e-06,2.86382881071087e-06,4.61741514587336e-06,8.34415807496569e-06,1.9876453215551e-05,5.36950913343337e-05,0.000134123762947215,0.000284001240966961,0.000497275803454737,0.000719821719970888,0.000881793499810989,0.000975332234608128,0.0010850071155523,0.00129943523477154,0.00158618751397982,0.00178751203904542,0.00175422575404816,0.00146879928618221],[1.31235745879944e-08,4.32261402658838e-08,1.21441417745742e-07,3.12211604633419e-07,7.91313402231582e-07,1.99502482158448e-06,4.68361957852786e-06,9.53946591847351e-06,1.63204364686323e-05,2.38857454313454e-05,3.29766089112879e-05,5.13729957487455e-05,9.71083187883779e-05,0.000193506430965398,0.000352718867993018,0.000554654243080187,0.000743530318951299,0.000864784960156907,0.000925570327380137,0.00100962423772967,0.00119937119482059,0.00147023418632963,0.00168166704076597,0.00168712279177908,0.00144898843411981],[1.09911343641979e-07,3.53563894903477e-07,9.39362367460101e-07,2.15980928884618e-06,4.6378730433022e-06,9.92783386768111e-06,2.09242589069714e-05,4.03802128790854e-05,6.71980432684405e-05,9.51053195858386e-05,0.000119947213858853,0.00015262569540425,0.000223046812735061,0.000363539911042556,0.000578957845443621,0.000827456482736782,0.00103394331761079,0.00113621692616736,0.00113556478222011,0.00110559076341983,0.00113213674484974,0.00122745909378559,0.0013096947547856,0.00127486819825425,0.00108474871733017],[6.5168763122664e-07,2.07253371905638e-06,5.34748710361426e-06,1.14929223889604e-05,2.172675273907e-05,3.91357716858365e-05,7.07663013794747e-05,0.000124573022267512,0.00019870514029092,0.000275248237941528,0.000335841521218775,0.00039184651856227,0.000494216542819814,0.00070237907227133,0.00103341003020348,0.00143150763374217,0.00178090129597042,0.00195990830390536,0.00191368292349384,0.00169662837575476,0.00143182272642004,0.00121082032443105,0.00103736693992751,0.000865552029230912,0.000667567909650526],[2.73464694867009e-06,8.65457543271709e-06,2.20211302176026e-05,4.56632587600797e-05,7.9662368601395e-05,0.000124449183345366,0.000188898368924797,0.000289417558050988,0.000429120264312591,0.000579483381917023,0.000702393622055255,0.00080228003400572,0.000946800193929489,0.00122574680288443,0.00168433651808754,0.00227445151669815,0.0028417149090851,0.00317240617257218,0.00311438759132685,0.00269448064291072,0.00209944463137004,0.00152463821067587,0.00105908347737357,0.000703246018178874,0.000437586623696983],[8.16989145215803e-06,2.58163915535273e-05,6.53337797583114e-05,0.000133267930326929,0.000222774367596013,0.000317081936966473,0.000412668922636484,0.000533958577489767,0.000706627554088166,0.000917066749559779,0.00112423798950584,0.00132153171526058,0.00156780323523225,0.00194920417151718,0.00251567573961144,0.00322696141195755,0.00391347487964094,0.00430484673322298,0.00418422803684917,0.00356790028782686,0.00270136612373187,0.00185967326028506,0.00118429053089977,0.000694045751741409,0.000366144417563987],[1.7705973401565e-05,5.58090498598399e-05,0.000141032375099624,0.000286211619331567,0.000470164799662055,0.000638624428175901,0.000753001902392312,0.000840711276000456,0.000974778279506381,0.00120128148570241,0.0015126142903653,0.00189326875149723,0.00235896848447161,0.00293798529019944,0.00362683365966636,0.00435112114887996,0.00493639950698799,0.00513717081088634,0.0047800730693443,0.00392830045885396,0.00286569552423162,0.0018892289160653,0.0011426007840506,0.000630569723667282,0.000309376215249178],[2.95549692713721e-05,9.11520574802817e-05,0.00022875383913956,0.000463964388847512,0.000761684129362018,0.00102533199181644,0.0011725858998501,0.00123001217495882,0.00132816027750187,0.00159405346304718,0.00206790163984292,0.00271730733745129,0.00349063722457331,0.00433357295279647,0.00516238920038595,0.00583393121027146,0.00615365433200052,0.0059444070885943,0.00516365892538808,0.0039847636763741,0.00272919051261671,0.0016770817049814,0.000937868756291197,0.000478461538355227,0.000218756974259418],[4.58799437780387e-05,0.000127371340415318,0.0003045329283728,0.000609282536610866,0.00100834278938046,0.00139126254703779,0.00165689693859952,0.00183289494648892,0.00208077882353108,0.00256415920766287,0.00332448813195212,0.00427760931685683,0.00529567801835271,0.00626933854407633,0.00708869730661971,0.00758777145001986,0.00756028692402217,0.0068782179032787,0.00561752498474101,0.00406859761261279,0.00259883381657567,0.00146730433718884,0.000738326116317373,0.000333715062783288,0.000135313571850087],[9.04307834004146e-05,0.000199978786021514,0.00041145208825048,0.000767864979242401,0.00126465397178254,0.00182903009763921,0.00238655427492837,0.00296396715964237,0.00368781267018262,0.00465306525085559,0.00580588669340734,0.00696198209263478,0.00792989261668283,0.00861357924861013,0.00899030709756365,0.00899779672166434,0.00849489561486629,0.0073848303868999,0.00577759409090739,0.00400026338986493,0.00242657035979037,0.00128377137177582,0.000592009591046872,0.000238414229292211,8.40530495290131e-05],[0.000219621096545493,0.000406718653022114,0.0007025663678406,0.00116343902194707,0.00183840071775762,0.00273060454348387,0.00381160031183083,0.00507601805508973,0.00653082969589267,0.00809523279440875,0.00953879609998321,0.0105625620978281,0.0109706572990055,0.010783563789338,0.0101810262644235,0.00931721668817131,0.0081998514382664,0.0067707951358342,0.00509037860939661,0.00339934903303321,0.00198454453574152,0.00100392578287881,0.000437806583383735,0.000163931123032876,5.251405063661e-05],[0.000487552616338708,0.000844350089539425,0.00134098609947208,0.00205350262742157,0.00309322793609741,0.00452712408236722,0.00632363115232282,0.00837318279733729,0.0105009855544983,0.0124208933370359,0.0137389391360079,0.0141082725662149,0.0134428704183162,0.0119910167609991,0.0101847910450437,0.00838366547146284,0.00672064653854754,0.0051598387391375,0.00367479409475276,0.00235003648272868,0.00131803531822095,0.000639545334411646,0.000266510244287618,9.4870630575931e-05,2.8678232923752e-05],[0.000862791917691593,0.00146032814244806,0.00225714633918541,0.00335600997585559,0.00491558180871327,0.00700804719155804,0.00951027303222967,0.0121426069631282,0.0145512662211511,0.0163178987394588,0.0169999962088472,0.0163304611938991,0.0144392788921922,0.0118377237857227,0.00914189225668659,0.00677754340068518,0.00488056942327614,0.0033939443415516,0.00222420239316539,0.00132937839875018,0.000703929464053635,0.000323849152419299,0.000128052662765595,4.3254659267416e-05,1.24157601106926e-05],[0.00120526998400815,0.00201431562788824,0.00307458016821542,0.00450621434948806,0.00648115136626639,0.00902715835194616,0.0118996491241045,0.0146714162868047,0.0168876808541337,0.0181125888921989,0.0179794919036685,0.0163899305866742,0.0136892440535825,0.0105483922761543,0.00760918988123397,0.00521725137860985,0.00342293261060348,0.00213570751851605,0.0012469600071104,0.0006671134103092,0.000319999729821679,0.000135033811544371,4.94297685407141e-05,1.55555447784646e-05,4.18302001774013e-06],[0.00139968689759105,0.00231044264006865,0.00347196125687355,0.00498466374662915,0.00699208093503516,0.00947707531869666,0.0121452294836972,0.0145428695763928,0.0162337099982018,0.0168558548572954,0.0161735944997398,0.0142403742712271,0.011492337753115,0.00856959371511669,0.00598840180332856,0.00396184923892789,0.00247106746855414,0.0014263456221857,0.000745526748746561,0.000347548702604191,0.000143482097262019,5.22842229144714e-05,1.67510580676872e-05,4.69106023748056e-06,1.1403433570134e-06],[0.00151709393267833,0.00245227226940204,0.00357487077721427,0.00492143338973997,0.00657609752632379,0.00850378476945546,0.0104575028025069,0.0120844254448121,0.0130727687045445,0.0131954965717884,0.0123490469272068,0.0106569102176895,0.00848896805852828,0.00629557095248751,0.00439467366885379,0.00289601426887205,0.00177747823854604,0.000988763394936515,0.000484064894229241,0.000204190396317799,7.3535185478648e-05,2.26488325716307e-05,6.02072713594894e-06,1.39719899526003e-06,2.85430831382039e-07],[0.00175243781679329,0.00271836303202744,0.00375981850055126,0.00483727081300309,0.00597951837672207,0.00715756825707258,0.00821283743797088,0.0089405941941086,0.00919726466523418,0.00891758610017268,0.00811431147256506,0.00691075890599943,0.00552172310932043,0.00415763779188178,0.00294997142629689,0.00195361851530932,0.00118430377600547,0.000640432714312719,0.000300827084113726,0.000120038258751629,4.00821377509778e-05,1.11223662844322e-05,2.56764985657158e-06,4.97518400751407e-07,8.21960993192181e-08],[0.00214214366471261,0.00313808682193879,0.00406745696653187,0.00485222131900009,0.00551250441933312,0.00604710669963634,0.00636978986103137,0.00638413056064758,0.00607443541312872,0.00550600568032104,0.00477952126628566,0.00399710178228299,0.00323009198618831,0.00250147688177082,0.0018161432234738,0.00120299556611057,0.000710162840905731,0.000366934913040787,0.000163393098002776,6.17768250224477e-05,1.95585587970514e-05,5.12692117215287e-06,1.10452392022524e-06,1.9505157799594e-07,2.83158615593228e-08],[0.00247038137276678,0.00339675746458354,0.00411703768025453,0.00458188714213629,0.00486118573165939,0.0049970535712664,0.00493627141581355,0.00461087647274645,0.00404184435710605,0.00335012354951096,0.00268645574434491,0.00214768638737768,0.00173372154420607,0.00137639937593574,0.00101964183992951,0.00067200307859708,0.000382908681932882,0.000186282149090324,7.70274148744226e-05,2.6986047019004e-05,7.96669193870036e-06,1.96565376966442e-06,4.01604408184758e-07,6.73822240027374e-08,9.23048965291072e-09],[0.00257641800108467,0.00330489255779684,0.00372685077589713,0.00386952913459893,0.00387554889621464,0.00382610745785675,0.00367140424274825,0.00331921597902061,0.00276022889072584,0.00210616755199578,0.00151896254288375,0.00110143829958839,0.000844032459599759,0.00066506062519795,0.000495200410754434,0.000324008925164832,0.000179275856474936,8.27138562006869e-05,3.17778464428013e-05,1.02053487631905e-05,2.75104771062703e-06,6.23231990309589e-07,1.1827767384227e-07,1.86769912018289e-08,2.43352004078936e-09],[0.00255143375431312,0.00305017012969949,0.00319217748546576,0.00307688112075409,0.00289215014672367,0.00273848438274123,0.00257296000518896,0.00229221630381501,0.00185464728489241,0.00133435333868465,0.00086643634148012,0.000543492964712101,0.000364142399470306,0.000266548356480978,0.000193399102449354,0.000124931054144076,6.78111280138756e-05,3.02587347460922e-05,1.10471029770642e-05,3.30985074146116e-06,8.1925011012351e-07,1.68787674629709e-07,2.9113605069629e-08,4.21199889576943e-09,5.09638755752928e-10],[0.00247575212820804,0.00279370853349698,0.0027515285389952,0.00248471635445634,0.00217768402609266,0.00192624925240054,0.00171256387552738,0.00146611150255725,0.00114618614210518,0.000787351517632539,0.000470926949531804,0.000255403468635934,0.000140225140419774,8.68649532956368e-05,5.80418194248358e-05,3.63567873000886e-05,1.94163761030336e-05,8.50575509150092e-06,3.02073135391069e-06,8.68765991137761e-07,2.03108370536072e-07,3.88554121285071e-08,6.1323631113589e-09,8.05013832034892e-10,8.84262601460625e-11],[0.00222810048712189,0.00240711185713688,0.00226941150735108,0.00195506450726412,0.00161254025361887,0.00131348321080274,0.00106251975415001,0.000835393660243612,0.000611752053621267,0.000398357803133751,0.000223809784230041,0.000108994672772302,4.92973571293224e-05,2.38917487607935e-05,1.3417782642461e-05,7.80200476378976e-06,4.057190761583e-06,1.75327303389372e-06,6.13916404141525e-07,1.73027122650087e-07,3.92562319465047e-08,7.19284568457656e-09,1.07017132518722e-09,1.30235989966091e-10,1.30742002882528e-11],[0.00171210400212498,0.00178009849590719,0.00161203992304887,0.00133113459963996,0.00104270884592682,0.000788709142088073,0.000576684584539615,0.000405489660289202,0.000269327363345885,0.000162980486943903,8.63105040688027e-05,3.9109359104307e-05,1.54601714150422e-05,5.88214910043934e-06,2.52452312610639e-06,1.25128176054242e-06,6.1235678227539e-07,2.59202738911165e-07,8.98423371593413e-08,2.50694366444913e-08,5.60857113931821e-09,1.0063524985112e-09,1.45175373046061e-10,1.69064506991318e-11,1.598603403865e-12]],&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;surface&#34;,&#34;inherit&#34;:true}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;title&#34;:&#34;Bivariate Density&#34;,&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:[]},&#34;yaxis&#34;:{&#34;title&#34;:[]},&#34;zaxis&#34;:{&#34;title&#34;:[]}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;yanchor&#34;:&#34;top&#34;,&#34;y&#34;:0.5}},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;colorbar&#34;:{&#34;title&#34;:&#34;&#34;,&#34;ticklen&#34;:2,&#34;len&#34;:0.5,&#34;lenmode&#34;:&#34;fraction&#34;,&#34;y&#34;:1,&#34;yanchor&#34;:&#34;top&#34;},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:true,&#34;x&#34;:[88.9398907298665,89.8067295301701,90.6735683304737,91.5404071307773,92.4072459310809,93.2740847313845,94.1409235316881,95.0077623319917,95.8746011322953,96.7414399325989,97.6082787329024,98.475117533206,99.3419563335096,100.208795133813,101.075633934117,101.94247273442,102.809311534724,103.676150335028,104.542989135331,105.409827935635,106.276666735938,107.143505536242,108.010344336546,108.877183136849,109.744021937153],&#34;y&#34;:[92.3004145455501,92.9920082957279,93.6836020459057,94.3751957960835,95.0667895462613,95.758383296439,96.4499770466168,97.1415707967946,97.8331645469724,98.5247582971502,99.2163520473279,99.9079457975057,100.599539547683,101.291133297861,101.982727048039,102.674320798217,103.365914548395,104.057508298572,104.74910204875,105.440695798928,106.132289549106,106.823883299284,107.515477049461,108.207070799639,108.898664549817],&#34;z&#34;:[[6.85666889382839e-11,2.56461062778276e-10,9.14548163093976e-10,3.26626647514284e-09,1.12750183663048e-08,3.4785687172498e-08,9.07838153113766e-08,1.99001293966871e-07,3.97141310428745e-07,9.22358341739491e-07,3.02026975362924e-06,1.13066974739126e-05,3.8170877923777e-05,0.000106719835351139,0.000241828993363136,0.00044269764711603,0.000659596627729255,0.000821128669645684,0.000910306523017004,0.000998058016700087,0.00116204483952298,0.00137182421139454,0.00149162068465402,0.00140776035786185,0.00113026510110035],[1.11615835674272e-09,3.84835427750987e-09,1.19025888503256e-08,3.57462738075808e-08,1.07444931103708e-07,3.06461164075519e-07,7.68178653776298e-07,1.61731450130647e-06,2.86382881071087e-06,4.61741514587336e-06,8.34415807496569e-06,1.9876453215551e-05,5.36950913343337e-05,0.000134123762947215,0.000284001240966961,0.000497275803454737,0.000719821719970888,0.000881793499810989,0.000975332234608128,0.0010850071155523,0.00129943523477154,0.00158618751397982,0.00178751203904542,0.00175422575404816,0.00146879928618221],[1.31235745879944e-08,4.32261402658838e-08,1.21441417745742e-07,3.12211604633419e-07,7.91313402231582e-07,1.99502482158448e-06,4.68361957852786e-06,9.53946591847351e-06,1.63204364686323e-05,2.38857454313454e-05,3.29766089112879e-05,5.13729957487455e-05,9.71083187883779e-05,0.000193506430965398,0.000352718867993018,0.000554654243080187,0.000743530318951299,0.000864784960156907,0.000925570327380137,0.00100962423772967,0.00119937119482059,0.00147023418632963,0.00168166704076597,0.00168712279177908,0.00144898843411981],[1.09911343641979e-07,3.53563894903477e-07,9.39362367460101e-07,2.15980928884618e-06,4.6378730433022e-06,9.92783386768111e-06,2.09242589069714e-05,4.03802128790854e-05,6.71980432684405e-05,9.51053195858386e-05,0.000119947213858853,0.00015262569540425,0.000223046812735061,0.000363539911042556,0.000578957845443621,0.000827456482736782,0.00103394331761079,0.00113621692616736,0.00113556478222011,0.00110559076341983,0.00113213674484974,0.00122745909378559,0.0013096947547856,0.00127486819825425,0.00108474871733017],[6.5168763122664e-07,2.07253371905638e-06,5.34748710361426e-06,1.14929223889604e-05,2.172675273907e-05,3.91357716858365e-05,7.07663013794747e-05,0.000124573022267512,0.00019870514029092,0.000275248237941528,0.000335841521218775,0.00039184651856227,0.000494216542819814,0.00070237907227133,0.00103341003020348,0.00143150763374217,0.00178090129597042,0.00195990830390536,0.00191368292349384,0.00169662837575476,0.00143182272642004,0.00121082032443105,0.00103736693992751,0.000865552029230912,0.000667567909650526],[2.73464694867009e-06,8.65457543271709e-06,2.20211302176026e-05,4.56632587600797e-05,7.9662368601395e-05,0.000124449183345366,0.000188898368924797,0.000289417558050988,0.000429120264312591,0.000579483381917023,0.000702393622055255,0.00080228003400572,0.000946800193929489,0.00122574680288443,0.00168433651808754,0.00227445151669815,0.0028417149090851,0.00317240617257218,0.00311438759132685,0.00269448064291072,0.00209944463137004,0.00152463821067587,0.00105908347737357,0.000703246018178874,0.000437586623696983],[8.16989145215803e-06,2.58163915535273e-05,6.53337797583114e-05,0.000133267930326929,0.000222774367596013,0.000317081936966473,0.000412668922636484,0.000533958577489767,0.000706627554088166,0.000917066749559779,0.00112423798950584,0.00132153171526058,0.00156780323523225,0.00194920417151718,0.00251567573961144,0.00322696141195755,0.00391347487964094,0.00430484673322298,0.00418422803684917,0.00356790028782686,0.00270136612373187,0.00185967326028506,0.00118429053089977,0.000694045751741409,0.000366144417563987],[1.7705973401565e-05,5.58090498598399e-05,0.000141032375099624,0.000286211619331567,0.000470164799662055,0.000638624428175901,0.000753001902392312,0.000840711276000456,0.000974778279506381,0.00120128148570241,0.0015126142903653,0.00189326875149723,0.00235896848447161,0.00293798529019944,0.00362683365966636,0.00435112114887996,0.00493639950698799,0.00513717081088634,0.0047800730693443,0.00392830045885396,0.00286569552423162,0.0018892289160653,0.0011426007840506,0.000630569723667282,0.000309376215249178],[2.95549692713721e-05,9.11520574802817e-05,0.00022875383913956,0.000463964388847512,0.000761684129362018,0.00102533199181644,0.0011725858998501,0.00123001217495882,0.00132816027750187,0.00159405346304718,0.00206790163984292,0.00271730733745129,0.00349063722457331,0.00433357295279647,0.00516238920038595,0.00583393121027146,0.00615365433200052,0.0059444070885943,0.00516365892538808,0.0039847636763741,0.00272919051261671,0.0016770817049814,0.000937868756291197,0.000478461538355227,0.000218756974259418],[4.58799437780387e-05,0.000127371340415318,0.0003045329283728,0.000609282536610866,0.00100834278938046,0.00139126254703779,0.00165689693859952,0.00183289494648892,0.00208077882353108,0.00256415920766287,0.00332448813195212,0.00427760931685683,0.00529567801835271,0.00626933854407633,0.00708869730661971,0.00758777145001986,0.00756028692402217,0.0068782179032787,0.00561752498474101,0.00406859761261279,0.00259883381657567,0.00146730433718884,0.000738326116317373,0.000333715062783288,0.000135313571850087],[9.04307834004146e-05,0.000199978786021514,0.00041145208825048,0.000767864979242401,0.00126465397178254,0.00182903009763921,0.00238655427492837,0.00296396715964237,0.00368781267018262,0.00465306525085559,0.00580588669340734,0.00696198209263478,0.00792989261668283,0.00861357924861013,0.00899030709756365,0.00899779672166434,0.00849489561486629,0.0073848303868999,0.00577759409090739,0.00400026338986493,0.00242657035979037,0.00128377137177582,0.000592009591046872,0.000238414229292211,8.40530495290131e-05],[0.000219621096545493,0.000406718653022114,0.0007025663678406,0.00116343902194707,0.00183840071775762,0.00273060454348387,0.00381160031183083,0.00507601805508973,0.00653082969589267,0.00809523279440875,0.00953879609998321,0.0105625620978281,0.0109706572990055,0.010783563789338,0.0101810262644235,0.00931721668817131,0.0081998514382664,0.0067707951358342,0.00509037860939661,0.00339934903303321,0.00198454453574152,0.00100392578287881,0.000437806583383735,0.000163931123032876,5.251405063661e-05],[0.000487552616338708,0.000844350089539425,0.00134098609947208,0.00205350262742157,0.00309322793609741,0.00452712408236722,0.00632363115232282,0.00837318279733729,0.0105009855544983,0.0124208933370359,0.0137389391360079,0.0141082725662149,0.0134428704183162,0.0119910167609991,0.0101847910450437,0.00838366547146284,0.00672064653854754,0.0051598387391375,0.00367479409475276,0.00235003648272868,0.00131803531822095,0.000639545334411646,0.000266510244287618,9.4870630575931e-05,2.8678232923752e-05],[0.000862791917691593,0.00146032814244806,0.00225714633918541,0.00335600997585559,0.00491558180871327,0.00700804719155804,0.00951027303222967,0.0121426069631282,0.0145512662211511,0.0163178987394588,0.0169999962088472,0.0163304611938991,0.0144392788921922,0.0118377237857227,0.00914189225668659,0.00677754340068518,0.00488056942327614,0.0033939443415516,0.00222420239316539,0.00132937839875018,0.000703929464053635,0.000323849152419299,0.000128052662765595,4.3254659267416e-05,1.24157601106926e-05],[0.00120526998400815,0.00201431562788824,0.00307458016821542,0.00450621434948806,0.00648115136626639,0.00902715835194616,0.0118996491241045,0.0146714162868047,0.0168876808541337,0.0181125888921989,0.0179794919036685,0.0163899305866742,0.0136892440535825,0.0105483922761543,0.00760918988123397,0.00521725137860985,0.00342293261060348,0.00213570751851605,0.0012469600071104,0.0006671134103092,0.000319999729821679,0.000135033811544371,4.94297685407141e-05,1.55555447784646e-05,4.18302001774013e-06],[0.00139968689759105,0.00231044264006865,0.00347196125687355,0.00498466374662915,0.00699208093503516,0.00947707531869666,0.0121452294836972,0.0145428695763928,0.0162337099982018,0.0168558548572954,0.0161735944997398,0.0142403742712271,0.011492337753115,0.00856959371511669,0.00598840180332856,0.00396184923892789,0.00247106746855414,0.0014263456221857,0.000745526748746561,0.000347548702604191,0.000143482097262019,5.22842229144714e-05,1.67510580676872e-05,4.69106023748056e-06,1.1403433570134e-06],[0.00151709393267833,0.00245227226940204,0.00357487077721427,0.00492143338973997,0.00657609752632379,0.00850378476945546,0.0104575028025069,0.0120844254448121,0.0130727687045445,0.0131954965717884,0.0123490469272068,0.0106569102176895,0.00848896805852828,0.00629557095248751,0.00439467366885379,0.00289601426887205,0.00177747823854604,0.000988763394936515,0.000484064894229241,0.000204190396317799,7.3535185478648e-05,2.26488325716307e-05,6.02072713594894e-06,1.39719899526003e-06,2.85430831382039e-07],[0.00175243781679329,0.00271836303202744,0.00375981850055126,0.00483727081300309,0.00597951837672207,0.00715756825707258,0.00821283743797088,0.0089405941941086,0.00919726466523418,0.00891758610017268,0.00811431147256506,0.00691075890599943,0.00552172310932043,0.00415763779188178,0.00294997142629689,0.00195361851530932,0.00118430377600547,0.000640432714312719,0.000300827084113726,0.000120038258751629,4.00821377509778e-05,1.11223662844322e-05,2.56764985657158e-06,4.97518400751407e-07,8.21960993192181e-08],[0.00214214366471261,0.00313808682193879,0.00406745696653187,0.00485222131900009,0.00551250441933312,0.00604710669963634,0.00636978986103137,0.00638413056064758,0.00607443541312872,0.00550600568032104,0.00477952126628566,0.00399710178228299,0.00323009198618831,0.00250147688177082,0.0018161432234738,0.00120299556611057,0.000710162840905731,0.000366934913040787,0.000163393098002776,6.17768250224477e-05,1.95585587970514e-05,5.12692117215287e-06,1.10452392022524e-06,1.9505157799594e-07,2.83158615593228e-08],[0.00247038137276678,0.00339675746458354,0.00411703768025453,0.00458188714213629,0.00486118573165939,0.0049970535712664,0.00493627141581355,0.00461087647274645,0.00404184435710605,0.00335012354951096,0.00268645574434491,0.00214768638737768,0.00173372154420607,0.00137639937593574,0.00101964183992951,0.00067200307859708,0.000382908681932882,0.000186282149090324,7.70274148744226e-05,2.6986047019004e-05,7.96669193870036e-06,1.96565376966442e-06,4.01604408184758e-07,6.73822240027374e-08,9.23048965291072e-09],[0.00257641800108467,0.00330489255779684,0.00372685077589713,0.00386952913459893,0.00387554889621464,0.00382610745785675,0.00367140424274825,0.00331921597902061,0.00276022889072584,0.00210616755199578,0.00151896254288375,0.00110143829958839,0.000844032459599759,0.00066506062519795,0.000495200410754434,0.000324008925164832,0.000179275856474936,8.27138562006869e-05,3.17778464428013e-05,1.02053487631905e-05,2.75104771062703e-06,6.23231990309589e-07,1.1827767384227e-07,1.86769912018289e-08,2.43352004078936e-09],[0.00255143375431312,0.00305017012969949,0.00319217748546576,0.00307688112075409,0.00289215014672367,0.00273848438274123,0.00257296000518896,0.00229221630381501,0.00185464728489241,0.00133435333868465,0.00086643634148012,0.000543492964712101,0.000364142399470306,0.000266548356480978,0.000193399102449354,0.000124931054144076,6.78111280138756e-05,3.02587347460922e-05,1.10471029770642e-05,3.30985074146116e-06,8.1925011012351e-07,1.68787674629709e-07,2.9113605069629e-08,4.21199889576943e-09,5.09638755752928e-10],[0.00247575212820804,0.00279370853349698,0.0027515285389952,0.00248471635445634,0.00217768402609266,0.00192624925240054,0.00171256387552738,0.00146611150255725,0.00114618614210518,0.000787351517632539,0.000470926949531804,0.000255403468635934,0.000140225140419774,8.68649532956368e-05,5.80418194248358e-05,3.63567873000886e-05,1.94163761030336e-05,8.50575509150092e-06,3.02073135391069e-06,8.68765991137761e-07,2.03108370536072e-07,3.88554121285071e-08,6.1323631113589e-09,8.05013832034892e-10,8.84262601460625e-11],[0.00222810048712189,0.00240711185713688,0.00226941150735108,0.00195506450726412,0.00161254025361887,0.00131348321080274,0.00106251975415001,0.000835393660243612,0.000611752053621267,0.000398357803133751,0.000223809784230041,0.000108994672772302,4.92973571293224e-05,2.38917487607935e-05,1.3417782642461e-05,7.80200476378976e-06,4.057190761583e-06,1.75327303389372e-06,6.13916404141525e-07,1.73027122650087e-07,3.92562319465047e-08,7.19284568457656e-09,1.07017132518722e-09,1.30235989966091e-10,1.30742002882528e-11],[0.00171210400212498,0.00178009849590719,0.00161203992304887,0.00133113459963996,0.00104270884592682,0.000788709142088073,0.000576684584539615,0.000405489660289202,0.000269327363345885,0.000162980486943903,8.63105040688027e-05,3.9109359104307e-05,1.54601714150422e-05,5.88214910043934e-06,2.52452312610639e-06,1.25128176054242e-06,6.1235678227539e-07,2.59202738911165e-07,8.98423371593413e-08,2.50694366444913e-08,5.60857113931821e-09,1.0063524985112e-09,1.45175373046061e-10,1.69064506991318e-11,1.598603403865e-12]],&#34;type&#34;:&#34;surface&#34;,&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-power&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating Power&lt;/h2&gt;
&lt;p&gt;       There is a variety of perspectives on the definition of power. Simply put, power is the probability of avoiding a Type II error, according to Neil Weiss in Introductory Statistics. In the section below we explore the concept of power by examining a two-way ANOVA with interaction. Note that for simplicity, I used arbitrarily picked values for the model instead of real empirical data. In the context of a two-way ANOVA with interaction, we could interpret power as &lt;em&gt;the probability that a test of significance will pick up on an effect that is present&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define a function for a two-way anova model with interaction
two_way_anova_interaction_regression &amp;lt;- function(n, b0, b1, b2, b3,
                                                 x1_mean = 0, x1_sd = 1, err_mean = 0, err_sd = 1) {

  # x1 draws n values from a normal distribution with a mean of 0 &amp;amp; sd of 1
  # x2 draws integers btw 0 to 1, n times (i.e., n numbers of either 0 or 1)
  x1 &amp;lt;- rnorm(n, mean = x1_mean, sd = x1_sd)
  x2 &amp;lt;- sample(0:1, n, replace = TRUE)

  # y is a linear combination of x1 &amp;amp; x2 multiplied by coefficients/effect sizes
  # the last term is the error term-- i.e. the unexplained portion of y
  y &amp;lt;- b0 + (b1 * x1) + (b2 * x2) + (b3 * x1 * x2) + rnorm(n, mean = err_mean, sd = err_sd)

  # regression model
  anova_model &amp;lt;- lm(y ~ x1 * x2)
  summary(anova_model)

  # store model outputs
  output &amp;lt;- summary(anova_model)$coefficients
  coeffs &amp;lt;- output[, 1]
  p_values &amp;lt;- output[, 4]
  r_sqr &amp;lt;- summary(anova_model)$r.squared

  # output
  results &amp;lt;- c(coeffs, p_values, r_sqr)
  names(results) &amp;lt;- c(
    &amp;quot;$\\beta_{0}$&amp;quot;, &amp;quot;$\\beta_{1}$&amp;quot;, &amp;quot;$\\beta_{2}$&amp;quot;,
    &amp;quot;$\\beta_{3}$&amp;quot;, &amp;quot;$\\beta_{0}$_pvalue&amp;quot;,
    &amp;quot;$\\beta_{1}$_pvalue&amp;quot;, &amp;quot;$\\beta_{2}$_pvalue&amp;quot;,
    &amp;quot;$\\beta_{3}$_pvalue&amp;quot;, &amp;quot;$r^2$&amp;quot;
  )
  return(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try using this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using arbitrarily picked values
anova_model &amp;lt;- two_way_anova_interaction_regression(n = 100, b0 = 0, b1 = 0.2, b2 = 0.4, b3 = 0.5)
# Generate table using kable () function from the knitr package
kable(anova_model, caption = &amp;quot;Two-way ANOVA Regression Model with Interaction&amp;quot;) %&amp;gt;%
  kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-14&#34;&gt;Table 1: &lt;/span&gt;Two-way ANOVA Regression Model with Interaction
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0310109
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2611402
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5979107
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3640547
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;_pvalue
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8446025
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;_pvalue
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1368724
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt;_pvalue
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0042647
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt;_pvalue
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1080374
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2291234
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;       Now, we run 1000 simulations of the model, finding the coefficients and their associating p-values for each iteration. The output would be a data frame, with one row per simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of simulations
num_sims &amp;lt;- 1000
# Using a function called ldply() from the plyr package by Hadley Wickham
# ldply () applies function for each element of a list then combine results into a data frame.
power_simulations &amp;lt;- ldply(1:num_sims, two_way_anova_interaction_regression,
  n = 100, b0 = 0, b1 = 0.2, b2 = 0.4, b3 = 0.5
)
# First 5 rows
kable(power_simulations[1:5, ], caption = &amp;quot;Two-way ANOVA Regression Model with Interaction&amp;quot;) %&amp;gt;%
  kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-15&#34;&gt;Table 2: &lt;/span&gt;Two-way ANOVA Regression Model with Interaction
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;_pvalue
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;_pvalue
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt;_pvalue
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt;_pvalue
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1657905
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1852724
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7968441
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5459245
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4342815
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2469565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0103175
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0136621
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4588430
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1945950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2394697
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7110830
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3147623
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5081150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0582744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0991120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1033851
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3969188
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7726047
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5142571
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8882506
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2458852
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0712582
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000984
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1898985
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2411440
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5116927
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3787538
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1250316
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2926884
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8758351
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0884618
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5130762
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1906276
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0003018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6237839
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2492963
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1122680
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2806965
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7228013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7372446
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4271540
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7963389
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0010111
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7472213
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;       We can estimate power by finding the proportion of p-values that are significant. We are primarily interested in the p-value for the interaction effect, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt;. We use a logical expression to find out whether the p-values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt; is less than .05. This would return a logical vector of TRUEs and FALSEs. We then sum this vector (TRUE gets counted as 1 &amp;amp; FALSE as 0). Finally, we divide this value by the total number of simulations, 1000, to get the proportion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Estimating Power
power &amp;lt;- sum(power_simulations[[8]] &amp;lt; .05) / nrow(power_simulations)
cat(power)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.67&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       We could tweak the parameters to see how it affects our power estimate. For instance, we could investigate what happens when we increase sample sizes for x1 and x2:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a vector of sample sizes ranging from 50 to 500
sample_sizes &amp;lt;- c(50, 100, 200, 300, 500)
# Initualize variable
results &amp;lt;- list()
# For Loop
for (val in sample_sizes) {

  # cycle through each value in &amp;quot;sample_sizes&amp;quot; and sets val to be that value
  # then pass that value to the simulation function as the sample size n
  power_simulations &amp;lt;- ldply(1:1000, two_way_anova_interaction_regression,
    n = val, b0 = 0, b1 = 0.3, b2 = 0.2, b3 = 0.3
  )

  # create new variable called n in the output data frame &amp;quot;results&amp;quot;
  # this variable functions as an indentifier
  power_simulations$n &amp;lt;- as.factor(val)


  # rbind() combines data frames by rows
  # notice that the first argument &amp;quot;results&amp;quot; in rbind() is an empty list when val=50
  # after each cycle, this dataframe gets 1000 more rows added to it
  # in the end we should have a single data frame with 5000 rows as there are five values in &amp;quot;sample_sizes&amp;quot;
  results &amp;lt;- rbind(results, power_simulations)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could examine the results using a table and a plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Split results into five individual data frames using the variable &amp;quot;n&amp;quot; as the identifier
list_of_results &amp;lt;- split(results, results[[10]])
# Find the power estimates associated with each sample size
power_n_50 &amp;lt;- sum(list_of_results[[&amp;quot;50&amp;quot;]][[8]] &amp;lt; .05) / nrow(list_of_results[[&amp;quot;50&amp;quot;]])
power_n_100 &amp;lt;- sum(list_of_results[[&amp;quot;100&amp;quot;]][[8]] &amp;lt; .05) / nrow(list_of_results[[&amp;quot;100&amp;quot;]])
power_n_200 &amp;lt;- sum(list_of_results[[&amp;quot;200&amp;quot;]][[8]] &amp;lt; .05) / nrow(list_of_results[[&amp;quot;200&amp;quot;]])
power_n_300 &amp;lt;- sum(list_of_results[[&amp;quot;300&amp;quot;]][[8]] &amp;lt; .05) / nrow(list_of_results[[&amp;quot;300&amp;quot;]])
power_n_500 &amp;lt;- sum(list_of_results[[&amp;quot;500&amp;quot;]][[8]] &amp;lt; .05) / nrow(list_of_results[[&amp;quot;500&amp;quot;]])
# Store all power estimates in one single vector
power_estimates &amp;lt;- c(power_n_50, power_n_100, power_n_200, power_n_300, power_n_500)
# Create data frame
power_table &amp;lt;- data.frame(
  &amp;quot;Sample Size&amp;quot; = sample_sizes,
  &amp;quot;Power Estimate&amp;quot; = power_estimates
)
# Generate table using kable () function from the knitr package
kable(power_table, caption = &amp;quot;Power Estimates by Sample Size&amp;quot;) %&amp;gt;%
  kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-18&#34;&gt;Table 3: &lt;/span&gt;Power Estimates by Sample Size
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sample.Size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Power.Estimate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.175
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.284
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.542
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.728
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.910
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot()
ggplot(
  data = power_table,
  mapping = aes(x = Sample.Size, y = Power.Estimate)
) +
  geom_point(color = &amp;quot;orange&amp;quot;) +
  geom_line(color = &amp;quot;orange&amp;quot;) +
  geom_hline(yintercept = 0.8, linetype = &amp;quot;dashed&amp;quot;) +
  ylim(c(0, 1)) +
  ggtitle(&amp;quot;Power Estimates by Sample Size&amp;quot;) +
  xlab(&amp;quot;Sample Size&amp;quot;) +
  ylab(&amp;quot;Power Estimates&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;azure2&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/the-central-limit-theorem-and-power-simulation/index.en_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       As can be seen, our power estimates increase as sample size increases. If we take 0.8 as the rough rule of thumb of desired level of power, then a sample size of &lt;span class=&#34;math inline&#34;&gt;\(\approx350\)&lt;/span&gt; would yield us that level of power given the set of parameters I chose.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling with Economics Data in R</title>
      <link>https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-importation&#34;&gt;Data Importation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrangling&#34;&gt;Wrangling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#merging-data-sets&#34;&gt;Merging Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nested-data&#34;&gt;Nested Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#is-data-deletion-best-practice&#34;&gt;Is data deletion best practice?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In this post, I experiment with some data wrangling techniques. I would be using the &lt;a href=&#34;https://www.rug.nl/ggdc/productivity/pwt/?lang=en&#34;&gt;Penn World Table&lt;/a&gt; version 9.1 (PWT 9.1) and the &lt;a href=&#34;https://www.cntsdata.com/&#34;&gt;Cross National Time Series&lt;/a&gt; (CNTS) data set to practice these techniques.&lt;/p&gt;
&lt;div id=&#34;data-importation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Importation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- read_csv(&amp;quot;pwt91.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 12376 Columns: 52&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;── Column specification ────────────────────────────────────────────────────────
Delimiter: &amp;quot;,&amp;quot;
chr  (8): countrycode, country, currency_unit, i_cig, i_xm, i_xr, i_outlier,...
dbl (44): year, rgdpe, rgdpo, pop, emp, avh, hc, ccon, cda, cgdpe, cgdpo, cn...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrangling&lt;/h2&gt;
&lt;p&gt;       We first need to “mutate” some variables. The total employment numbers do not offer as much context as the employment–population ratios; we create a new variable by dividing total employment by population. Next, we “select” the variables that are of interest to us. Suppose we wish to keep the following variables: country-code, country name, year, employment–population ratios:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  mutate(emp_pop_ratio = emp / pop) %&amp;gt;%
  select(countrycode, country, year, emp_pop_ratio)
pwt91&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 12,376 × 4
   countrycode country  year emp_pop_ratio
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1 ABW         Aruba    1950            NA
 2 ABW         Aruba    1951            NA
 3 ABW         Aruba    1952            NA
 4 ABW         Aruba    1953            NA
 5 ABW         Aruba    1954            NA
 6 ABW         Aruba    1955            NA
 7 ABW         Aruba    1956            NA
 8 ABW         Aruba    1957            NA
 9 ABW         Aruba    1958            NA
10 ABW         Aruba    1959            NA
# … with 12,366 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s do some simple counting to get a good sense of our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(pwt91$country)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;182&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of years covered in this data set
cat(length(unique(pwt91$year)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;68&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make sure that country and country-code are consistent
cat(length(unique(pwt91$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;182&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First and last year
cat(min(pwt91$year))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1950&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(max(pwt91$year))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2017&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       Notice that there are lots of NA’s, i.e., for some combination of country and year, there are no data. More succinctly put, the further back we go in time, the more we observe missing values for countries. Also, I expect that some countries would have more missing values than others. Before we trim the sample down, we want to examine the number missing values by year and by country to see if this is indeed a widespread trend:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(year) %&amp;gt;%
  count(is.na(emp_pop_ratio))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 136 × 3
# Groups:   year [68]
    year `is.na(emp_pop_ratio)`     n
   &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;                  &amp;lt;int&amp;gt;
 1  1950 FALSE                     50
 2  1950 TRUE                     132
 3  1951 FALSE                     54
 4  1951 TRUE                     128
 5  1952 FALSE                     55
 6  1952 TRUE                     127
 7  1953 FALSE                     57
 8  1953 TRUE                     125
 9  1954 FALSE                     61
10  1954 TRUE                     121
# … with 126 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       After examining the table above, I find that the count confirms my thinking. &lt;code&gt;True&lt;/code&gt; indicates the count for missing values, and &lt;code&gt;False&lt;/code&gt; indicates the opposite. The first two decades over this period of 68 years (1950-2017) have many countries with missing values— more than 100 countries in our sample of 182 have missing values. As we go into the 1990’s, the number of countries with missing values drastically decreases— less than 10 countries have missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(country) %&amp;gt;%
  count(is.na(emp_pop_ratio))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 312 × 3
# Groups:   country [182]
   country             `is.na(emp_pop_ratio)`     n
   &amp;lt;chr&amp;gt;               &amp;lt;lgl&amp;gt;                  &amp;lt;int&amp;gt;
 1 Albania             FALSE                     48
 2 Albania             TRUE                      20
 3 Algeria             FALSE                     58
 4 Algeria             TRUE                      10
 5 Angola              FALSE                     48
 6 Angola              TRUE                      20
 7 Anguilla            FALSE                     29
 8 Anguilla            TRUE                      39
 9 Antigua and Barbuda FALSE                      9
10 Antigua and Barbuda TRUE                      59
# … with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       OECD members such as the US, UK, Australia, Canada, France, Finland, Germany, etc., have no missing values over this period. Sub-Saharan African countries, in particular, have relatively more missing values. Then, some Latin American countries have even more missing values. This is to be expected with socio-economic data; we confirm that the sampling quality introduces some region bias. Deleting missing observations can result in biased parameters and estimates and reduce the statistical power of the analyses. However, in this case, I would be using list-wise deletion, where all observations that have missing values are deleted. This means that, if I were to continue on with my analysis, the models would only be trained on data from countries that have a more complete set of data. While there may be better ways to handle biased samples, for this activity, I would simply use the year 1990 as a cut off since a reasonable number of countries would have values from 1990 to 2017:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  filter(year &amp;gt;= 1990) %&amp;gt;%
  na.exclude()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We count the number of rows for each country to see which countries do not have all 28 years (1990-2017) worth of data, and we drop those countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  filter(n() == 28)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a sanity check to make sure we are on track:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 169 × 2
# Groups:   countrycode [169]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 ARM            28
 6 AUS            28
 7 AUT            28
 8 AZE            28
 9 BDI            28
10 BEL            28
# … with 159 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(pwt91$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, all remaining countries have values for the 28-year period; we now have a sample of 169 countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4,732 × 4
# Groups:   countrycode [169]
   countrycode country  year emp_pop_ratio
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454
 2 AGO         Angola   1991         0.451
 3 AGO         Angola   1992         0.449
 4 AGO         Angola   1993         0.448
 5 AGO         Angola   1994         0.445
 6 AGO         Angola   1995         0.442
 7 AGO         Angola   1996         0.439
 8 AGO         Angola   1997         0.438
 9 AGO         Angola   1998         0.437
10 AGO         Angola   1999         0.437
# … with 4,722 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;merging-data-sets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Merging Data Sets&lt;/h2&gt;
&lt;p&gt;       Suppose after we completed our analysis, we found another data set containing some other relevant variables. We would like to “join” the two data sets by a common variable. In my case, that common variable is “country-code.” We start by loading in the second data set, which I’ve converted to a .csv file for convenience:&lt;/p&gt;
&lt;p&gt;       First we need to make sure that the “key” variables share the same names across these two data sets. Then, we “select” the variables of interest to us. The &lt;code&gt;Domestic8&lt;/code&gt; variable is the number of anti-government demonstrations, which, according to the &lt;a href=&#34;https://www.cntsdata.com/&#34;&gt;CNTS&lt;/a&gt; user manual, includes labor strikes and demonstrations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cnts19 &amp;lt;- cnts19 %&amp;gt;%
  rename(countrycode = Wbcode) %&amp;gt;%
  select(countrycode, year, domestic8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       We would want to use &lt;code&gt;left_join()&lt;/code&gt;, because it preserves the original observations even when there isn’t a match. Notice that setting the argument (by = NULL) makes sure that R uses all variables that appear in both data sets for merging, this is the so-called &lt;strong&gt;natural join&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data &amp;lt;- pwt91 %&amp;gt;%
  left_join(cnts19, by = NULL) %&amp;gt;%
  na.exclude()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Joining, by = c(&amp;quot;countrycode&amp;quot;, &amp;quot;year&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4,455 × 5
# Groups:   countrycode [162]
   countrycode country  year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454         0
 2 AGO         Angola   1991         0.451         0
 3 AGO         Angola   1992         0.449         0
 4 AGO         Angola   1993         0.448         0
 5 AGO         Angola   1994         0.445         0
 6 AGO         Angola   1995         0.442         0
 7 AGO         Angola   1996         0.439         0
 8 AGO         Angola   1997         0.438         0
 9 AGO         Angola   1998         0.437         0
10 AGO         Angola   1999         0.437         0
# … with 4,445 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 162 × 2
# Groups:   countrycode [162]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 ARM            26
 6 AUS            28
 7 AUT            28
 8 AZE            26
 9 BDI            28
10 BEL            28
# … with 152 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       According to the table above, after merging, we now have some countries with missing values for the domestic8 variable. Since this exercise is about data wrangling techniques and not about data analysis, we will continue to trim the sample further for practice, excluding countries that have missing values. We need to keep in mind that, as far as analysis is concerned, too much deletion of the data would lead to biases. Just how much bias are we able to tolerate is a whole new topic of discussion. For now, we will proceed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data &amp;lt;- new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  filter(n() == 28)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 139 × 2
# Groups:   countrycode [139]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 AUS            28
 6 AUT            28
 7 BDI            28
 8 BEL            28
 9 BEN            28
10 BFA            28
# … with 129 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have ensured that all remaining countries have values for all variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(new_data$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;139&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3,892 × 5
# Groups:   countrycode [139]
   countrycode country  year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454         0
 2 AGO         Angola   1991         0.451         0
 3 AGO         Angola   1992         0.449         0
 4 AGO         Angola   1993         0.448         0
 5 AGO         Angola   1994         0.445         0
 6 AGO         Angola   1995         0.442         0
 7 AGO         Angola   1996         0.439         0
 8 AGO         Angola   1997         0.438         0
 9 AGO         Angola   1998         0.437         0
10 AGO         Angola   1999         0.437         0
# … with 3,882 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s glimpse at which country had the single most yearly anti-government demonstrations over this 28-year period:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  arrange(desc(domestic8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3,892 × 5
# Groups:   countrycode [139]
   countrycode country               year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 IND         India                 2016         0.401       149
 2 IND         India                 2017         0.402       146
 3 IND         India                 2015         0.399       110
 4 USA         United States         2015         0.470        81
 5 SYR         Syrian Arab Republic  2011         0.237        74
 6 PAK         Pakistan              2016         0.307        55
 7 YEM         Yemen                 2011         0.185        55
 8 USA         United States         2014         0.466        50
 9 USA         United States         2011         0.457        49
10 USA         United States         2016         0.474        49
# … with 3,882 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nested-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nested Data&lt;/h2&gt;
&lt;p&gt;       Lastly, we may also present our new merged data as a nested data frame, a new structure. The nested data frame has one row per group (per country-code/country in my case). The third column, data, is a list of data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data &amp;lt;- new_data %&amp;gt;%
  group_by(countrycode, country) %&amp;gt;%
  nest()
nested_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 139 × 3
# Groups:   countrycode, country [139]
   countrycode country              data             
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                &amp;lt;list&amp;gt;           
 1 AGO         Angola               &amp;lt;tibble [28 × 3]&amp;gt;
 2 ALB         Albania              &amp;lt;tibble [28 × 3]&amp;gt;
 3 ARE         United Arab Emirates &amp;lt;tibble [28 × 3]&amp;gt;
 4 ARG         Argentina            &amp;lt;tibble [28 × 3]&amp;gt;
 5 AUS         Australia            &amp;lt;tibble [28 × 3]&amp;gt;
 6 AUT         Austria              &amp;lt;tibble [28 × 3]&amp;gt;
 7 BDI         Burundi              &amp;lt;tibble [28 × 3]&amp;gt;
 8 BEL         Belgium              &amp;lt;tibble [28 × 3]&amp;gt;
 9 BEN         Benin                &amp;lt;tibble [28 × 3]&amp;gt;
10 BFA         Burkina Faso         &amp;lt;tibble [28 × 3]&amp;gt;
# … with 129 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us examine the structure of the object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(nested_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 139
Columns: 3
Groups: countrycode, country [139]
$ countrycode &amp;lt;chr&amp;gt; &amp;quot;AGO&amp;quot;, &amp;quot;ALB&amp;quot;, &amp;quot;ARE&amp;quot;, &amp;quot;ARG&amp;quot;, &amp;quot;AUS&amp;quot;, &amp;quot;AUT&amp;quot;, &amp;quot;BDI&amp;quot;, &amp;quot;BEL&amp;quot;, &amp;quot;B…
$ country     &amp;lt;chr&amp;gt; &amp;quot;Angola&amp;quot;, &amp;quot;Albania&amp;quot;, &amp;quot;United Arab Emirates&amp;quot;, &amp;quot;Argentina&amp;quot;, …
$ data        &amp;lt;list&amp;gt; [&amp;lt;tbl_df[28 x 3]&amp;gt;], [&amp;lt;tbl_df[28 x 3]&amp;gt;], [&amp;lt;tbl_df[28 x 3]&amp;gt;…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The row numbers make sense, since we have indeed a sample of 139 countries. If we look at the first element of “data,” we see that it contains all the data for that country (in my case, Angola):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data$data[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 28 × 3
    year emp_pop_ratio domestic8
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1  1990         0.454         0
 2  1991         0.451         0
 3  1992         0.449         0
 4  1993         0.448         0
 5  1994         0.445         0
 6  1995         0.442         0
 7  1996         0.439         0
 8  1997         0.438         0
 9  1998         0.437         0
10  1999         0.437         0
# … with 18 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fifth element:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data$data[[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 28 × 3
    year emp_pop_ratio domestic8
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1  1990         0.463         0
 2  1991         0.446         0
 3  1992         0.439         0
 4  1993         0.436         0
 5  1994         0.445         0
 6  1995         0.457         0
 7  1996         0.457         1
 8  1997         0.456         0
 9  1998         0.459         0
10  1999         0.462         0
# … with 18 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       In other words, there is one data frame per country. Presenting data in this new structure can be helpful, especially when dealing with cross-sectional data where observational units are numerous and we often need to conduct transformations and fit models using only subsets of the entire data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-data-deletion-best-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is data deletion best practice?&lt;/h2&gt;
&lt;p&gt;       At the end of the post, it may be worth writing about data deletion. We began with a sample of 182 countries, which we trimmed down to 139. Judging by numbers only, the deletion process could have been worse. However, we must take note that, when it comes to missing values, the data are not missing completely at random (MCAR). This is especially true for cross-national, socio-economic data, where sampling quality reflects inequalities that are rather hard to capture and deal with. Beyond this activity, it would be interesting to explore ways of handling missing data as well as new imputation methods that have been developed over the years.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applications of Subsetting Operators in R</title>
      <link>https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#matching-and-merging-by-hand-integer-subsetting&#34;&gt;Matching and merging by hand (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-samples-and-bootstraps-integer-subsetting&#34;&gt;Random samples and bootstraps (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordering-integer-subsetting&#34;&gt;Ordering (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expanding-aggregated-counts-integer-subsetting&#34;&gt;Expanding aggregated counts (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#removing-columns-from-data-frames-character-subsetting&#34;&gt;Removing columns from data frames (character subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boolean-algebra-versus-sets-logical-and-integer-subsetting&#34;&gt;Boolean algebra versus sets (logical and integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition-of-the-operator&#34;&gt;Definition of the %% operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rondomly-permute-a-data-frame-a-technique-often-used-in-random-forests&#34;&gt;Rondomly permute a data frame (a technique often used in random forests)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-a-random-sample-of-m-rows-from-a-data-frame&#34;&gt;Selecting a random sample of m rows from a data frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordering-the-columns-in-a-data-frame-alphabetically&#34;&gt;Ordering the columns in a data frame alphabetically&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In this post, we will cover some useful applications of R’s subsetting operations. The content of this post is gleaned from Hadley Wickham’s &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;. This book is aimed at helping R users improve their programming skills beyond day-to-day data analysis. To better understand the content of this post, I recommend reading chapter 4 of Hadley’s book beforehand. Or, if you are already familiar with R’s subsetting operators, jump right in.&lt;/p&gt;
&lt;p&gt;       I wanted to document some of the content from Hadley’s book with my added commentary to help my future self as well as others who may accidentally stumble across this post. I believe that many of these examples can be extended and employed in a variety of settings, and so my goal here is to turn this post into a resource not just for myself but perhaps others in their daily use of R. With that being said, let’s get started.&lt;/p&gt;
&lt;div id=&#34;matching-and-merging-by-hand-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matching and merging by hand (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       The function &lt;code&gt;match()&lt;/code&gt; returns a vector that contains the position indices of the (first) matches of its first argument “x =” in its second “table =”. For instance, &lt;code&gt;match(x, table)&lt;/code&gt; will return the position where each element in “x” is found in “table.” This function allows us to create look-up tables. For instance, say we observe a vector of student grades in the world and a table that describe their properties. Let us say our goal is to create a data frame where each row is an observation of student grade and each column is a property associated with that letter grade. We can use a look-up table to map the properties to our vector of grades:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Grades
grades &amp;lt;- c(1, 2, 2, 3, 1)
# Info
info &amp;lt;- data.frame(
  grade = 3:1,
  desc = c(&amp;quot;Excellent&amp;quot;, &amp;quot;Good&amp;quot;, &amp;quot;Poor&amp;quot;),
  fail = c(F, F, T)
)
info&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  grade      desc  fail
1     3 Excellent FALSE
2     2      Good FALSE
3     1      Poor  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Match the grades to the &amp;quot;grade&amp;quot; column in the info table
# This is a vector indices we would later use to subset the info table
id &amp;lt;- match(x = grades, table = info[[&amp;quot;grade&amp;quot;]])
id&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 2 2 1 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Subset the info table as a matrix
# Select rows according to the order in which they appear in the index vector
info[id, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    grade      desc  fail
3       1      Poor  TRUE
2       2      Good FALSE
2.1     2      Good FALSE
1       3 Excellent FALSE
3.1     1      Poor  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we’ve selected the rows in the info table, sometimes more than once, so that each row is an observation of student grade.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;random-samples-and-bootstraps-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random samples and bootstraps (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       If we would like to randomly sample or bootstrap a vector or a data frame, we can use &lt;code&gt;sample()&lt;/code&gt; to generate a random index vector. A shortcut of the &lt;code&gt;sample()&lt;/code&gt; function: If the argument x has length 1, is a numeric vector (in the sense of &lt;code&gt;is.numeric()&lt;/code&gt;), and is &amp;gt;= 1, then sampling via &lt;code&gt;sample()&lt;/code&gt; will only return random vales from the sequence 1 to x.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create data frame
df &amp;lt;- data.frame(x = c(1, 2, 3, 1, 2), y = 5:1, z = letters[1:5])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
1 1 5 a
2 2 4 b
3 3 3 c
4 1 2 d
5 2 1 e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly reorder the rows
# Select the rows in the order they appear in the random vector created by sample()
df[sample(x = nrow(df)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
5 2 1 e
4 1 2 d
1 1 5 a
3 3 3 c
2 2 4 b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select three random rows in the order they appear in the random vector
df[sample(x = nrow(df), size = 3), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
2 2 4 b
5 2 1 e
1 1 5 a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select 8 bootstrap replicates
# Notice that replace = TRUE, which indicates that some rows will be selected more than once
df[sample(x = nrow(df), size = 8, replace = TRUE), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    x y z
2   2 4 b
5   2 1 e
4   1 2 d
4.1 1 2 d
1   1 5 a
5.1 2 1 e
2.1 2 4 b
4.2 1 2 d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we utilize the &lt;code&gt;sample()&lt;/code&gt; function to generate a random index vector, which we then use to subset the data frame. We can easily automate this bootstrapping process by writing our own function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bootstrap data frame
boots_df &amp;lt;- function(df, n, replicate) {

  # Create n index vectors
  # This returns a list of random index vectors each with size = replicate
  list_of_indices &amp;lt;- map(
    .x = 1:n,
    .f = ~ sample(
      x = 1:nrow(df),
      size = replicate,
      replace = TRUE
    )
  )

  # Pre-allocate list container
  list_of_bootstrapped_df &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = n)
  # Loop
  for (i in seq_along(1:n)) {

    # Select bootstrapped &amp;quot;rows&amp;quot; from the data frame
    list_of_bootstrapped_df[[i]] &amp;lt;- df[list_of_indices[[i]], ]
  }

  # Output is a list of &amp;quot;n&amp;quot; bootstrapped data frames, each with nrow = replicate
  list_of_bootstrapped_df
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action. Suppose we wish to produce 8 bootstrap replicates of the rows of a data frame, and we wish to do this 4 times. Using our function above, we see that the arguments are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;n = 4&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;replicate = 8&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(boots_df(df = df, n = 4, replicate = 8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 4
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 3 1 2 1 3 2 3 2
  ..$ y: int [1:8] 3 5 4 2 3 4 3 4
  ..$ z: chr [1:8] &amp;quot;c&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;d&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 3 3 1 3 1 1 3 3
  ..$ y: int [1:8] 3 3 5 3 5 5 3 3
  ..$ z: chr [1:8] &amp;quot;c&amp;quot; &amp;quot;c&amp;quot; &amp;quot;a&amp;quot; &amp;quot;c&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 1 1 2 3 2 3 1 1
  ..$ y: int [1:8] 2 2 4 3 4 3 5 2
  ..$ z: chr [1:8] &amp;quot;d&amp;quot; &amp;quot;d&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 2 2 2 3 1 1 3 1
  ..$ y: int [1:8] 4 4 1 3 5 2 3 5
  ..$ z: chr [1:8] &amp;quot;b&amp;quot; &amp;quot;b&amp;quot; &amp;quot;e&amp;quot; &amp;quot;c&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, we have a list of 4 data frames each with 8 rows of bootstrapped replicates. This function can be easily scaled to generate more bootstrap samples and more replicates per sample.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordering (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       The function &lt;code&gt;order()&lt;/code&gt; takes a vector as its input and returns an integer vector describing how to order the subsetted vector. The values in the returned integer vector are “pull” indices; that is, each order(x)[i] tells the position that each x[i] is in the “un-ordered” vector.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a character vector that is out of order
x &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;a&amp;quot;)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;a&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find the position of each alphabet in &amp;quot;x&amp;quot; and order them
order(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now select the elements from &amp;quot;x&amp;quot; in the order in which they appear in order(x)
x[order(x)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To break ties, you can supply additional variables to order(). You can also change the order from ascending to descending by using decreasing = TRUE. By default, any missing values will be put at the end of the vector; however, you can remove them with na.last = NA or put them at the front with na.last = FALSE.&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Example 2&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create &amp;quot;un-ordered&amp;quot; vector
set.seed(7)
y &amp;lt;- sample(x = 1:8, replace = TRUE)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 3 7 4 7 2 7 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find the position of each number in &amp;quot;x&amp;quot; and order them
order(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 6 8 2 4 3 5 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# According to order(y)
# Select the elements from y in this order:
y[order(y)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 2 2 3 4 7 7 7&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;For two or more dimensional objects, &lt;code&gt;order()&lt;/code&gt; and integer subsetting makes it easy to order either the rows or columns of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly reorder the rows
# Select columns 3, 2, and 1 in that order
df2 &amp;lt;- df[sample(x = 1:nrow(df)), 3:1]
df2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
3 c 3 3
4 d 2 1
2 b 4 2
1 a 5 1
5 e 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the values in column &amp;quot;x&amp;quot;
order(df2[[&amp;quot;x&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 3 5 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the rows by column x in ascension
# Select the rows based on the positions in order()
# Now the &amp;quot;x&amp;quot; column is ascending
df2[order(df2[[&amp;quot;x&amp;quot;]]), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
4 d 2 1
1 a 5 1
2 b 4 2
5 e 1 2
3 c 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the columns based on the alphabetical order of their names
df2[, order(names(df2))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
3 3 3 c
4 1 2 d
2 2 4 b
1 1 5 a
5 2 1 e&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could have sorted vectors directly with &lt;code&gt;sort()&lt;/code&gt;, or &lt;code&gt;dplyr::arrange()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using arrange() to order based on the &amp;quot;x&amp;quot; column
# The default order of arrangement is ascending
# This is equivalent to SQL&amp;#39;s ORDER BY
arrange(.data = df2, df2[[&amp;quot;x&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
1 d 2 1
2 a 5 1
3 b 4 2
4 e 1 2
5 c 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;arrange()&lt;/code&gt; orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, &lt;code&gt;arrange()&lt;/code&gt; largely ignores grouping; you need to explicitly mention grouping variables (or use .by_group = TRUE) in order to group by them.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;expanding-aggregated-counts-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expanding aggregated counts (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       First, we need to be familiar with the function &lt;code&gt;rep(x = x, times = y)&lt;/code&gt;, which repeats x[i] y[i] times. Let’s see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat each x[i] y[i] times
rep(x = c(2, 3, 4), times = c(2, 6, 5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 2 2 3 3 3 3 3 3 4 4 4 4 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat the vector object x 3 times
rep(x = c(2, 3, 4), times = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 3 4 2 3 4 2 3 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat each x[i] 3 times
rep(x = c(2, 3, 4), each = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 2 2 3 3 3 4 4 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       Sometimes you get a data frame where identical rows have been collapsed into one and a count column “n” has been added. rep() and integer subsetting make it easy to ““un-collapse”“, because we can take advantage of &lt;code&gt;rep()&lt;/code&gt;s vectorization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data frame
df &amp;lt;- data.frame(x = c(2, 4, 1), y = c(9, 11, 6), n = c(3, 5, 1))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x  y n
1 2  9 3
2 4 11 5
3 1  6 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The first row has count = 3, so repeat it 3 times
# The second row has count = 5, so repeat it 5 times
# The third row has count = 1, so do not repeat
rep(x = 1:nrow(df), times = df$n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 1 1 2 2 2 2 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select the rows in the order they appear in the rep() function
df[rep(x = 1:nrow(df), times = df$n), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    x  y n
1   2  9 3
1.1 2  9 3
1.2 2  9 3
2   4 11 5
2.1 4 11 5
2.2 4 11 5
2.3 4 11 5
2.4 4 11 5
3   1  6 1&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-columns-from-data-frames-character-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing columns from data frames (character subsetting)&lt;/h2&gt;
&lt;p&gt;There are two ways to remove columns from a data frame. You can set individual columns to NULL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame
df &amp;lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
1 1 3 a
2 2 2 b
3 3 1 c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove column z
df$z &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or you can subset to return only the columns you want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame
df &amp;lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
# Keep only columns x and y
df[c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y
1 1 3
2 2 2
3 3 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only know the columns you don’t want, use set operations to work out which columns to keep. For instance, the function &lt;code&gt;setdiff(x, y, ...)&lt;/code&gt;— x is the full set and y is a subset x. The function &lt;code&gt;setdiff()&lt;/code&gt; returns the difference between x and y; that is, it returns those elements that are &lt;em&gt;not in the subset y&lt;/em&gt; but &lt;em&gt;in the full set “x”&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Full set
names(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Exclude x
setdiff(x = names(df), y = &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Exclude x and z
setdiff(x = names(df), y = c(&amp;quot;x&amp;quot;, &amp;quot;z&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;y&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select every column except for z
df[setdiff(names(df), &amp;quot;z&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y
1 1 3
2 2 2
3 3 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other useful set operations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;intersect(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;union(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;setdiff(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;setequal(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Read the documentations to learn more about them.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;       In addition, set operations can be useful in our day-to-day use. We very often need to &lt;code&gt;rm()&lt;/code&gt; objects from the global environment that we do need anymore. It sometimes happens that there are many objects in our environment pane, and we only wish to keep a few of them. One way to do so is to list all the objects we wish to remove by name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove object we do not need
rm(list = c(&amp;quot;object1&amp;quot;, &amp;quot;object2&amp;quot;, ...))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this can be inefficient since we need to 1) figure out which objects we’d like to remove by calling &lt;code&gt;ls()&lt;/code&gt; and 2) type all of them using &lt;code&gt;c()&lt;/code&gt;. This can be too much typing and therefore very time-consuming. Alternatively, we can use &lt;code&gt;setdiff()&lt;/code&gt; to keep only the objects that we would need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Keep only objects that we meed
rm(list = setdiff(x = ls(), y = &amp;quot;object_to_be_kept_1&amp;quot;, &amp;quot;object_to_be_kept_2&amp;quot;, ...))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;boolean-algebra-versus-sets-logical-and-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boolean algebra versus sets (logical and integer subsetting)&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;which()&lt;/code&gt; gives the TRUE indices of a logical object; that is, their positions in a logical vector. Use &lt;code&gt;which.min()&lt;/code&gt; and &lt;code&gt;which.max()&lt;/code&gt; for the index of the minimum or maximum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a named logical vector
x &amp;lt;- sample(x = 1:10, replace = FALSE) &amp;lt; 4
names(x) &amp;lt;- letters[1:10]
# Convert Boolean representation to an integer representation
# Easy to see the positions of the first and last TRUE&amp;#39;s
which(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b d g 
2 4 7 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that reverses which()
unwhich &amp;lt;- function(x, n) {
  # Create a vector of all FALSE with length equal to x
  out &amp;lt;- rep_len(x = FALSE, length.out = n)
  # Select elements in &amp;quot;out&amp;quot; and convert them to TRUE
  # Since &amp;quot;x&amp;quot; is a logical index, the only elements in &amp;quot;out&amp;quot;
  # that will be selected are the TRUE values in &amp;quot;x&amp;quot;
  out[x] &amp;lt;- TRUE
  # Now &amp;quot;out&amp;quot; should be identical to &amp;quot;x&amp;quot; in terms of TRUE and FALSE
  out
}
# Reverse x from integer to Boolean
unwhich(x = x, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read the documentation to learn more about &lt;code&gt;which()&lt;/code&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-of-the-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of the %% operator&lt;/h2&gt;
&lt;p&gt;Create two logical vectors and their integer equivalents. &lt;strong&gt;Note&lt;/strong&gt;: %% indicates x mod y (“x modulo y”). The result of the %% operator is the REMAINDER of a division, Eg. 75 %% 4 = 18 Remainder 3. If the dividend is lower than the divisor, then R returns the same dividend value: Eg. 4 %% 75 = 4.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logical vector 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Example 1
1:10 %% 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 1 0 1 0 1 0 1 0 1 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Logical 1
x1 &amp;lt;- 1:10 %% 2 == 0
x1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer equivalent
x2 &amp;lt;- which(x = x1)
x2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  2  4  6  8 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Logical vector 2&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Logical 2
y1 &amp;lt;- 1:10 %% 5 == 0
y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer equivalent
y2 &amp;lt;- which(x = y1)
y2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  5 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Intersection of “x” and “y”. For the logical vectors, we wish to find the indices where both x[i] and y[i] are TRUE; for the integer vectors, we wish to find the indices where the values x[i] and y[i] are equal.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X &amp;amp; Y &amp;lt;-&amp;gt; intersect(x, y)
# Logical
x1 &amp;amp; y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
intersect(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Union of “x” and “y”. For the logical vectors, we wish to find the indices where either x[i] or y[i] or both are TRUE; for the integer vectors, we wish to find all values in x and y.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X | Y &amp;lt;-&amp;gt; union(x, y)
# Logical
x1 | y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
union(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  2  4  6  8 10  5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Set difference. For the logical, we wish to find values that are &lt;em&gt;in x1 but not in y1&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X &amp;amp; !Y &amp;lt;-&amp;gt; setdiff(x, y)
# Logical
x1 &amp;amp; !y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
setdiff(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 6 8&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;xor()&lt;/code&gt; indicates element-wise exclusive OR.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import image
knitr::include_graphics(&amp;quot;Exclusive Or.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Exclusive%20Or.png&#34; width=&#34;40%&#34; height=&#34;40%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# xor(X, Y) &amp;lt;-&amp;gt; setdiff(union(x, y), intersect(x, y))
# Logical
xor(x1, y1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
setdiff(union(x2, y2), intersect(x2, y2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 6 8 5&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;rondomly-permute-a-data-frame-a-technique-often-used-in-random-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rondomly permute a data frame (a technique often used in random forests)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly permute the columns and rows of a data frame
mtcars[
  sample(x = 1:nrow(mtcars), replace = FALSE),
  colnames(mtcars)[sample(x = 1:length(colnames(mtcars)))]
]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     qsec cyl  hp    wt  disp am  mpg carb gear vs drat
Datsun 710          18.61   4  93 2.320 108.0  1 22.8    1    4  1 3.85
Merc 450SL          17.60   8 180 3.730 275.8  0 17.3    3    3  0 3.07
Toyota Corona       20.01   4  97 2.465 120.1  0 21.5    1    3  1 3.70
Camaro Z28          15.41   8 245 3.840 350.0  0 13.3    4    3  0 3.73
Merc 230            22.90   4  95 3.150 140.8  0 22.8    2    4  1 3.92
Ferrari Dino        15.50   6 175 2.770 145.0  1 19.7    6    5  0 3.62
Dodge Challenger    16.87   8 150 3.520 318.0  0 15.5    2    3  0 2.76
Merc 240D           20.00   4  62 3.190 146.7  0 24.4    2    4  1 3.69
Maserati Bora       14.60   8 335 3.570 301.0  1 15.0    8    5  0 3.54
Cadillac Fleetwood  17.98   8 205 5.250 472.0  0 10.4    4    3  0 2.93
Lotus Europa        16.90   4 113 1.513  95.1  1 30.4    2    5  1 3.77
Mazda RX4 Wag       17.02   6 110 2.875 160.0  1 21.0    4    4  0 3.90
Merc 450SE          17.40   8 180 4.070 275.8  0 16.4    3    3  0 3.07
Pontiac Firebird    17.05   8 175 3.845 400.0  0 19.2    2    3  0 3.08
Merc 280            18.30   6 123 3.440 167.6  0 19.2    4    4  1 3.92
Merc 450SLC         18.00   8 180 3.780 275.8  0 15.2    3    3  0 3.07
Fiat 128            19.47   4  66 2.200  78.7  1 32.4    1    4  1 4.08
Honda Civic         18.52   4  52 1.615  75.7  1 30.4    2    4  1 4.93
Merc 280C           18.90   6 123 3.440 167.6  0 17.8    4    4  1 3.92
Porsche 914-2       16.70   4  91 2.140 120.3  1 26.0    2    5  0 4.43
Duster 360          15.84   8 245 3.570 360.0  0 14.3    4    3  0 3.21
Hornet Sportabout   17.02   8 175 3.440 360.0  0 18.7    2    3  0 3.15
Valiant             20.22   6 105 3.460 225.0  0 18.1    1    3  1 2.76
Volvo 142E          18.60   4 109 2.780 121.0  1 21.4    2    4  1 4.11
Chrysler Imperial   17.42   8 230 5.345 440.0  0 14.7    4    3  0 3.23
Mazda RX4           16.46   6 110 2.620 160.0  1 21.0    4    4  0 3.90
Lincoln Continental 17.82   8 215 5.424 460.0  0 10.4    4    3  0 3.00
Hornet 4 Drive      19.44   6 110 3.215 258.0  0 21.4    1    3  1 3.08
AMC Javelin         17.30   8 150 3.435 304.0  0 15.2    2    3  0 3.15
Ford Pantera L      14.50   8 264 3.170 351.0  1 15.8    4    5  0 4.22
Fiat X1-9           18.90   4  66 1.935  79.0  1 27.3    1    4  1 4.08
Toyota Corolla      19.90   4  65 1.835  71.1  1 33.9    1    4  1 4.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Second way using ncol() instead of colnames()
# Integer subsetting instead of character
mtcars[sample(x = nrow(mtcars)), sample(x = ncol(mtcars))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       wt gear vs  qsec  mpg am  hp cyl carb  disp drat
Merc 450SE          4.070    3  0 17.40 16.4  0 180   8    3 275.8 3.07
AMC Javelin         3.435    3  0 17.30 15.2  0 150   8    2 304.0 3.15
Lotus Europa        1.513    5  1 16.90 30.4  1 113   4    2  95.1 3.77
Maserati Bora       3.570    5  0 14.60 15.0  1 335   8    8 301.0 3.54
Fiat 128            2.200    4  1 19.47 32.4  1  66   4    1  78.7 4.08
Mazda RX4           2.620    4  0 16.46 21.0  1 110   6    4 160.0 3.90
Chrysler Imperial   5.345    3  0 17.42 14.7  0 230   8    4 440.0 3.23
Porsche 914-2       2.140    5  0 16.70 26.0  1  91   4    2 120.3 4.43
Volvo 142E          2.780    4  1 18.60 21.4  1 109   4    2 121.0 4.11
Merc 280C           3.440    4  1 18.90 17.8  0 123   6    4 167.6 3.92
Lincoln Continental 5.424    3  0 17.82 10.4  0 215   8    4 460.0 3.00
Mazda RX4 Wag       2.875    4  0 17.02 21.0  1 110   6    4 160.0 3.90
Merc 230            3.150    4  1 22.90 22.8  0  95   4    2 140.8 3.92
Fiat X1-9           1.935    4  1 18.90 27.3  1  66   4    1  79.0 4.08
Merc 240D           3.190    4  1 20.00 24.4  0  62   4    2 146.7 3.69
Toyota Corolla      1.835    4  1 19.90 33.9  1  65   4    1  71.1 4.22
Ford Pantera L      3.170    5  0 14.50 15.8  1 264   8    4 351.0 4.22
Honda Civic         1.615    4  1 18.52 30.4  1  52   4    2  75.7 4.93
Valiant             3.460    3  1 20.22 18.1  0 105   6    1 225.0 2.76
Hornet 4 Drive      3.215    3  1 19.44 21.4  0 110   6    1 258.0 3.08
Dodge Challenger    3.520    3  0 16.87 15.5  0 150   8    2 318.0 2.76
Ferrari Dino        2.770    5  0 15.50 19.7  1 175   6    6 145.0 3.62
Merc 450SL          3.730    3  0 17.60 17.3  0 180   8    3 275.8 3.07
Merc 450SLC         3.780    3  0 18.00 15.2  0 180   8    3 275.8 3.07
Camaro Z28          3.840    3  0 15.41 13.3  0 245   8    4 350.0 3.73
Pontiac Firebird    3.845    3  0 17.05 19.2  0 175   8    2 400.0 3.08
Toyota Corona       2.465    3  1 20.01 21.5  0  97   4    1 120.1 3.70
Datsun 710          2.320    4  1 18.61 22.8  1  93   4    1 108.0 3.85
Cadillac Fleetwood  5.250    3  0 17.98 10.4  0 205   8    4 472.0 2.93
Duster 360          3.570    3  0 15.84 14.3  0 245   8    4 360.0 3.21
Hornet Sportabout   3.440    3  0 17.02 18.7  0 175   8    2 360.0 3.15
Merc 280            3.440    4  1 18.30 19.2  0 123   6    4 167.6 3.92&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-a-random-sample-of-m-rows-from-a-data-frame&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selecting a random sample of m rows from a data frame&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that randomly selects m rows from a data frame
select_m_rows &amp;lt;- function(data, m) {

  # Warning
  if (m &amp;gt; nrow(data)) {
    abort(&amp;quot;Not enough rows in data frame&amp;quot;)
  }

  # Select rows randomly and include all columns
  data[sample(x = 1:nrow(data), size = m), , drop = FALSE]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action using the iris data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_m_rows(data = iris, m = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
149          6.2         3.4          5.4         2.3  virginica
87           6.7         3.1          4.7         1.5 versicolor
116          6.4         3.2          5.3         2.3  virginica
82           5.5         2.4          3.7         1.0 versicolor
8            5.0         3.4          1.5         0.2     setosa
81           5.5         2.4          3.8         1.1 versicolor
112          6.4         2.7          5.3         1.9  virginica
79           6.0         2.9          4.5         1.5 versicolor
43           4.4         3.2          1.3         0.2     setosa
75           6.4         2.9          4.3         1.3 versicolor&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;What if we need the first and last rows selected, but everything in between can be random?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extend the function to ensure that the first and last rows are selected
# Everything in between are random
select_m_rows_extended &amp;lt;- function(data, m) {

  # Warning
  if (m &amp;gt; nrow(data)) {
    abort(&amp;quot;Not enough rows in data frame&amp;quot;)
  }

  # Select first row and last row
  # &amp;quot;Sandwich&amp;quot; the sample() vector in between
  data[
    c(
      1,
      sample(x = 2:(nrow(data) - 1), size = (m - 2)),
      nrow(data)
    ), ,
    drop = FALSE
  ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action using the mtcars data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_m_rows_extended(data = mtcars, m = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;               mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Merc 450SLC   15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Merc 450SL    17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
Datsun 710    22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
Toyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
AMC Javelin   15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
Volvo 142E    21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Finally, what if we wish to randomly select a blocked sample, i.e., the rows have to be contiguous (an initial row, a final row, and everything in between)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Successive lines together as a blocked sample
m &amp;lt;- 10
# The starting row cannot be less than m rows from the last row of the data
# Or else there wound not be enough rows to select m successive rows from
start &amp;lt;- sample(x = 1:(nrow(mtcars) - m + 1), size = 1)
# The ending row must be m rows from the starting row
end &amp;lt;- start + m - 1
# Select the consecutive rows between random starting row
mtcars[start:end, , drop = FALSE]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Merc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
Merc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Merc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Merc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-the-columns-in-a-data-frame-alphabetically&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordering the columns in a data frame alphabetically&lt;/h2&gt;
&lt;p&gt;This can easily be done using R’s subsetting operators:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that orders the columns of data frame alphabetically
order_columns &amp;lt;- function(data) {

  # Select columns according to the indices generated by order()
  # We could also use sort()
  data[, order(x = names(data))]
}
# Test
as_tibble(order_columns(data = mtcars))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 32 × 11
      am  carb   cyl  disp  drat  gear    hp   mpg  qsec    vs    wt
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1     1     4     6  160   3.9      4   110  21    16.5     0  2.62
 2     1     4     6  160   3.9      4   110  21    17.0     0  2.88
 3     1     1     4  108   3.85     4    93  22.8  18.6     1  2.32
 4     0     1     6  258   3.08     3   110  21.4  19.4     1  3.22
 5     0     2     8  360   3.15     3   175  18.7  17.0     0  3.44
 6     0     1     6  225   2.76     3   105  18.1  20.2     1  3.46
 7     0     4     8  360   3.21     3   245  14.3  15.8     0  3.57
 8     0     2     4  147.  3.69     4    62  24.4  20       1  3.19
 9     0     2     4  141.  3.92     4    95  22.8  22.9     1  3.15
10     0     4     6  168.  3.92     4   123  19.2  18.3     1  3.44
# … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(order_columns(data = iris))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 150 × 5
   Petal.Length Petal.Width Sepal.Length Sepal.Width Species
          &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  
 1          1.4         0.2          5.1         3.5 setosa 
 2          1.4         0.2          4.9         3   setosa 
 3          1.3         0.2          4.7         3.2 setosa 
 4          1.5         0.2          4.6         3.1 setosa 
 5          1.4         0.2          5           3.6 setosa 
 6          1.7         0.4          5.4         3.9 setosa 
 7          1.4         0.3          4.6         3.4 setosa 
 8          1.5         0.2          5           3.4 setosa 
 9          1.4         0.2          4.4         2.9 setosa 
10          1.5         0.1          4.9         3.1 setosa 
# … with 140 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(order_columns(data = USArrests))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 50 × 4
   Assault Murder  Rape UrbanPop
     &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
 1     236   13.2  21.2       58
 2     263   10    44.5       48
 3     294    8.1  31         80
 4     190    8.8  19.5       50
 5     276    9    40.6       91
 6     204    7.9  38.7       78
 7     110    3.3  11.1       77
 8     238    5.9  15.8       72
 9     335   15.4  31.9       80
10     211   17.4  25.8       60
# … with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       That is it with R’s subsetting operators. Combined with other data wrangling tools from the &lt;code&gt;tidyverse&lt;/code&gt; packages, R’s subsetting operations can be powerful as far as data analysis tasks are concerned. Next up in R programming, I will write about the the &lt;code&gt;tidyverse&lt;/code&gt;’s functional programming tool— &lt;code&gt;purrr&lt;/code&gt;— which I have been using here and there in many of my posts. Having an understanding of R &lt;code&gt;functionals&lt;/code&gt; have helped me tremendously in my day-to-day use of R, and so I look forward to documenting my learning process via a post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Control Flow and Vectorization in R</title>
      <link>https://www.kenwuyang.com/en/post/control-flow-in-r/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/control-flow-in-r/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/control-flow-in-r/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#control-flow&#34;&gt;Control Flow&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#for-loop&#34;&gt;for loop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-pitfalls&#34;&gt;Common pitfalls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#while-loop&#34;&gt;while loop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#repeat-loop&#34;&gt;repeat loop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forecast-pension-growth-under-compounding-interest-for-loop&#34;&gt;Forecast pension growth under compounding interest (for loop)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#duration-of-a-fixed-payment-loan-under-monthly-compounding-interest-while-loop&#34;&gt;Duration of a fixed-payment loan under monthly compounding interest (while loop)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#piece-wise-function-loop-and-vectorization&#34;&gt;Piece-wise function (Loop and vectorization)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequence-for-loop-and-while-loop&#34;&gt;Sequence (for loop and while loop)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometric-and-harmonic-means-for-loop-and-vectorization&#34;&gt;Geometric and Harmonic Means (for loop and vectorization)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#find-the-sum-of-every-nth-element-of-a-vector&#34;&gt;Find the Sum of Every nth Element of a Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chart-the-flow-of-a-simple-program&#34;&gt;Chart the flow of a simple program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-lotka-volterra-model-for-a-predator-prey-system&#34;&gt;The Lotka-Volterra Model for a predator-prey system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#find-the-minimum-of-a-vector&#34;&gt;Find the Minimum of a Vector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;control-flow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Control Flow&lt;/h2&gt;
&lt;p&gt;       Loops are an important programming concept, enabling programmers to execute blocks of code repeatedly, usually with varying options. This post will cover three types of loops— for, while, and repeat. We will then solve some problems using loops to demonstrate the power of iteration in programming. Whenever possible, we will attempt to solve problems using different methods, including different types of loops and parallel processing. Many of R’s functions are vectorized, meaning that the function will operate on all elements of a vector without needing to loop through and act on each element one at a time. We will leverage this unique feature of R to show that many problems that seem to involve loops can actually be solved differently in R, although the programs may be harder to intuit.&lt;/p&gt;
&lt;p&gt;       For more readings on control flows in R, I suggest starting with Hadley Wickham’s &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt; and &lt;a href=&#34;https://www.amazon.com/Introduction-Scientific-Programming-Simulation-Chapman/dp/1466569999&#34;&gt;Introduction to Scientific Programming and Simulation Using R&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;for-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;for loop&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;for%20loop.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Basic syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (item in vector) perform_action&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each item in vector, perform_action is called once; updating the value of item each time. There are two ways to terminate a for loop early:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;next exits the current iteration&lt;/li&gt;
&lt;li&gt;break exits the entire for loop&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:10) {
  if (i &amp;lt; 3) {
    next
  }

  print(i)

  if (i &amp;gt; 5) {
    break
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3
[1] 4
[1] 5
[1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;common-pitfalls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common pitfalls&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use &lt;code&gt;seq_along(x)&lt;/code&gt; to generate the sequence in &lt;code&gt;for()&lt;/code&gt; since it always returns a value the same length as x, even when x is a length zero vector:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Declare variables
means &amp;lt;- c()
out &amp;lt;- vector(&amp;quot;list&amp;quot;, length(means))
# For loop
for (i in seq_along(means)) {
  out[[i]] &amp;lt;- rnorm(10, means[[i]])
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When iterating over S3 vectors, loops typically strip the attributes. Use [[ to work around this caveat:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Date
xs &amp;lt;- as.Date(c(&amp;quot;2020-01-01&amp;quot;, &amp;quot;2010-01-01&amp;quot;))
# Loop
for (i in seq_along(xs)) {
  print(xs[[i]] + 10)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;2020-01-11&amp;quot;
[1] &amp;quot;2010-01-11&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;while-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;while loop&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;while%20loop.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Basic syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;while (condition) {
  expression_1
  ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When a while command is executed, logical_expression is evaluated first. If it is true, then the group expressions in {} is executed. Control is then passed back to the start of the command: if logical_expression is still TRUE then the grouped expressions are executed again, and so on. For the loop to stop, logical_expression must eventually be FALSE. To achieve this, logical_expression usually depends on a variable that is altered within the grouped expressions.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;repeat-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;repeat loop&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;repeat%20loop.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Basic syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;repeat{
  expression_1
  ...

  if (condition) {
    break
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a simple loop that will run the same statement or a group of statements repeatedly until the stop condition has been encountered. Repeat loop does not have any condition to terminate the loop, a programmer must specifically place a condition within the loop’s body and use the declaration of a break statement to terminate this loop. If no condition is present in the body of the repeat loop then it will iterate infinitely.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;forecast-pension-growth-under-compounding-interest-for-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecast pension growth under compounding interest (for loop)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Inputs&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Annual interest rate
r &amp;lt;- 0.11
# Forecast duration (in years)
term &amp;lt;- 10
# Time between payments (in years)
period &amp;lt;- 1 / 12
# Amount deposited each period
payments &amp;lt;- 100&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Calculations. The function &lt;code&gt;ceiling()&lt;/code&gt; takes a single numeric argument x and returns a numeric vector containing the &lt;em&gt;smallest integers not less than the corresponding elements of x&lt;/em&gt;. On the other hand, &lt;code&gt;floor()&lt;/code&gt; takes a single numeric argument x and returns a numeric vector containing the &lt;em&gt;largest integers not greater than the corresponding elements of x&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of payments
n &amp;lt;- floor(term / period)
# Pre-allocate pension container
pension &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = n)
# Object size
lobstr::obj_size(pension)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1,008 B&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use seq_along
seq_along(pension)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
 [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
[109] 109 110 111 112 113 114 115 116 117 118 119 120&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For loop (compounded monthly)
for (i in seq_along(pension)) {
  pension[[i + 1]] &amp;lt;- pension[[i]] * (1 + r * period) + payments
}
# New object size
lobstr::obj_size(pension)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1,016 B&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Graph the output&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Time
time &amp;lt;- (0:n) * period
# Plot
ggplot(data = tibble(time, pension), mapping = aes(x = time, y = pension)) +
  geom_point(color = &amp;quot;orange&amp;quot;) +
  labs(
    title = &amp;quot;Forecast of Pension Value&amp;quot;,
    x = &amp;quot;Time (years)&amp;quot;, y = &amp;quot;Pension Value ($)&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/control-flow-in-r/index.en_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;duration-of-a-fixed-payment-loan-under-monthly-compounding-interest-while-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Duration of a fixed-payment loan under monthly compounding interest (while loop)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Inputs&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Annual interest rate
r &amp;lt;- 0.11
# Time between repayments (in years)
period &amp;lt;- 1 / 12
# Initial principal
initial_principal &amp;lt;- 1000
# Fixed payment amount
payments &amp;lt;- 12&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Calculations&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Initialize variables
time &amp;lt;- 0
principal &amp;lt;- initial_principal
# While loop
while (principal &amp;gt; 0) {
  # Time (in years)
  time &amp;lt;- time + period
  # Principal payments
  principal &amp;lt;- principal * (1 + r * period) - payments
}&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Output&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Fixed-payment loan will be repaid in&amp;quot;, time, &amp;quot;years\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fixed-payment loan will be repaid in 13.25 years&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;piece-wise-function-loop-and-vectorization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Piece-wise function (Loop and vectorization)&lt;/h2&gt;
&lt;p&gt;Consider the function &lt;span class=&#34;math inline&#34;&gt;\(y=f(x)\)&lt;/span&gt; defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\leq 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x)=-x^{3}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\in(0,1]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x)=x^{2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x&amp;gt;1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\sqrt{x}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Implement the function using for loop:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define x
x_vals &amp;lt;- seq.int(from = -2, to = 2, by = 0.1)
# Initialize sequence
seq &amp;lt;- seq_along(x_vals)
# Pre-allocate container for y values
y_vals &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = length(x_vals))
# For loop
for (i in seq) {

  # Set x values
  x &amp;lt;- x_vals[[i]]

  if (x &amp;lt;= 0) {
    y &amp;lt;- -x^3
  } else if (x &amp;gt; 0 &amp;amp; x &amp;lt;= 1) {
    y &amp;lt;- x^2
  } else if (x &amp;gt; 1) {
    y &amp;lt;- sqrt(x)
  }

  # Compute y values and store in the container vector
  y_vals[[i]] &amp;lt;- y
}
# Plot the function
ggplot(data = tibble(x_vals, y_vals)) +
  geom_line(mapping = aes(x = x_vals, y = y_vals), color = &amp;quot;blue&amp;quot;) +
  labs(
    title = &amp;quot;Piecewise Function&amp;quot;,
    x = &amp;quot;x&amp;quot;, y = &amp;quot;y&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/control-flow-in-r/index.en_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement the function using &lt;code&gt;case_when()&lt;/code&gt; (Note that the function is &lt;span class=&#34;math inline&#34;&gt;\(-x^3\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x \leq 0\)&lt;/span&gt;; hence the negative sign in front of x)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vectorization
y_vals_vectorized &amp;lt;- case_when(
  x_vals &amp;lt;= 0 ~ -x_vals^3,
  x_vals &amp;gt; 0 &amp;amp; x_vals &amp;lt;= 1 ~ x_vals^2,
  x_vals &amp;gt; 1 ~ sqrt(x_vals)
)
y_vals_vectorized&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 8.000000 6.859000 5.832000 4.913000 4.096000 3.375000 2.744000 2.197000
 [9] 1.728000 1.331000 1.000000 0.729000 0.512000 0.343000 0.216000 0.125000
[17] 0.064000 0.027000 0.008000 0.001000 0.000000 0.010000 0.040000 0.090000
[25] 0.160000 0.250000 0.360000 0.490000 0.640000 0.810000 1.000000 1.048809
[33] 1.095445 1.140175 1.183216 1.224745 1.264911 1.303840 1.341641 1.378405
[41] 1.414214&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-for-loop-and-while-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sequence (for loop and while loop)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(h(x, n)=1+x+x^{2}+\cdots+x^{n}=\sum_{i=0}^{n} x^{i}\)&lt;/span&gt;. Let us implement this sum of a geometric sequence using a for loop:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
sum_of_sequence_for_loop &amp;lt;- function(x, n) {

  # Initialize sequence
  seq &amp;lt;- 0:n
  # Pre-allocate container
  terms &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = (n + 1))
  # Loop
  for (i in seq) {
    terms[[i + 1]] &amp;lt;- x^i
  }

  # Sum
  sum(terms)
}
# Test
sum_of_sequence_for_loop(x = 0.3, n = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.428571&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_for_loop(x = 6.6, n = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4243336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_for_loop(x = 1, n = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Using a while loop:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
sum_of_sequence_while_loop &amp;lt;- function(x, n) {

  # Initialize i
  i &amp;lt;- 0
  # Pre-allocate container
  terms &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = (n + 1))
  # Loop
  while (i &amp;lt;= n) {
    terms[[i + 1]] &amp;lt;- x^i
    i &amp;lt;- i + 1
  }

  # Sum
  sum(terms)
}
# Test
sum_of_sequence_while_loop(x = 0.3, n = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.428571&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_while_loop(x = 6.6, n = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4243336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_while_loop(x = 1, n = 46)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 47&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Implement using parallel processing— vectorization&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
sum_of_sequence_vectorized &amp;lt;- function(x, n) {

  # Create vector of x
  vector_of_x &amp;lt;- rep(x = x, times = n + 1)

  # Create vector of exponents
  vector_of_exponents &amp;lt;- seq.int(from = 0, to = n, by = 1)

  # Create vector of terms in the sequence
  vector_of_terms &amp;lt;- vector_of_x^vector_of_exponents

  # Find the sum
  sum(vector_of_terms)
}
# Test
sum_of_sequence_vectorized(x = 0.3, n = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.428571&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_vectorized(x = 6.6, n = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4243336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_of_sequence_vectorized(x = 1, n = 46)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 47&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;geometric-and-harmonic-means-for-loop-and-vectorization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometric and Harmonic Means (for loop and vectorization)&lt;/h2&gt;
&lt;p&gt;The geometric mean of a vector is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\left(\prod_{i=1}^{n} x_{i}\right)^{\frac{1}{n}}=\sqrt[n]{x_{1} x_{2} \cdots x_{n}}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Geometric mean (for loop)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geometric_for_loop &amp;lt;- function(x) {

  # Length of vector
  n &amp;lt;- length(x)

  # Warning
  if (is.numeric(x) == FALSE) {
    rlang::abort(&amp;quot;Vector is of the wrong type; input must be numeric&amp;quot;)
  } else if (n &amp;lt; 2) {
    rlang::abort(&amp;quot;Input vector must contain more than 1 element&amp;quot;)
  }

  # Initialize first term (as.double() ensures no integer overflow)
  x_val &amp;lt;- as.double(x[[1]])
  # Iterate over the sequence 1:(n - 1)
  # The algorithm involves multiplying the current element i by the next (i + 1) element in x
  # Setting (n - 1) as the last item safeguards against out-of-bounds subsetting of &amp;quot;x&amp;quot;
  seq &amp;lt;- 1:(n - 1)
  # Iterate
  for (i in seq) {
    x_val &amp;lt;- x_val * x[[i + 1]]
  }


  # Geometric mean
  (x_val)^(1 / n)
}
# Test
# Create a random vector
x &amp;lt;- sample(x = 1:45, size = 200, replace = TRUE)
# A function from the psych package
psych::geometric.mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 19.10415&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Our custom function
geometric_for_loop(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 19.10415&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Geometric mean (vectorization)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geometric_vectorization &amp;lt;- function(x) {

  # Length of vector
  n &amp;lt;- length(x)

  # Warning
  if (is.numeric(x) == FALSE) {
    rlang::abort(&amp;quot;Vector is of the wrong type; input must be numeric&amp;quot;)
  } else if (n &amp;lt; 2) {
    rlang::abort(&amp;quot;Input vector must contain more than 1 element&amp;quot;)
  }

  # Product of vector elements
  # The function prod() is primitive
  prod &amp;lt;- prod(x)
  # Geometric mean
  prod^(1 / n)
}
# Test
geometric_vectorization(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 19.10415&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Harmonic mean (for loop)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;harmonic_for_loop &amp;lt;- function(x) {

  # Length of vector
  n &amp;lt;- length(x)

  # Warning
  if (is.numeric(x) == FALSE) {
    rlang::abort(&amp;quot;Vector is of the wrong type; input must be numeric&amp;quot;)
  } else if (n &amp;lt; 2) {
    rlang::abort(&amp;quot;Input vector must contain more than 1 element&amp;quot;)
  }

  # Initialize x value
  x_val &amp;lt;- as.double(1 / x[[1]])
  # Create sequence
  seq &amp;lt;- 1:(n - 1)
  # Iterate
  for (i in seq) {
    x_val &amp;lt;- x_val + (1 / x[[i + 1]])
  }

  # Harmonic mean
  n / x_val
}
# Test
# A function from the psych package
psych::harmonic.mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12.33392&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Our custom function
harmonic_for_loop(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12.33392&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Harmonic mean (vectorization)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;harmonic_vectorization &amp;lt;- function(x) {

  # Length of vector
  n &amp;lt;- length(x)

  # Warning
  if (is.numeric(x) == FALSE) {
    rlang::abort(&amp;quot;Vector is of the wrong type; input must be numeric&amp;quot;)
  } else if (n &amp;lt; 2) {
    rlang::abort(&amp;quot;Input vector must contain more than 1 element&amp;quot;)
  }

  # Find element-wise reciprocals
  x_reciprical &amp;lt;- 1 / x
  # Sum the reciprocals
  sum &amp;lt;- sum(x_reciprical)
  # Harmonic mean
  n / sum
}
# Test
harmonic_vectorization(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12.33392&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;find-the-sum-of-every-nth-element-of-a-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Find the Sum of Every nth Element of a Vector&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Using for loop&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
every_nth_element_for_loop &amp;lt;- function(x, n) {

  # Define the nth term
  n &amp;lt;- n
  # Initialize sequence
  seq &amp;lt;- seq_along(x)
  # Initialize counter
  counter &amp;lt;- 0
  # Pre-allocate container
  new_x &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = length(x))
  # Loop
  for (i in seq) {

    # Count the term
    counter &amp;lt;- counter + 1

    # If counter gets to n, copy that term to the container
    if (counter == n) {
      new_x[[i]] &amp;lt;- x[[i]]

      # Reinitialize counter to zero
      counter &amp;lt;- 0
    }
  }

  # Sum
  new_x
}
# Test vector
x &amp;lt;- sample(x = 1:203, size = 100, replace = TRUE)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1]  34 190 201 193 127 176  56 134 137  27  36  22  24  96 129  63  50 146
 [19] 119 203  63  34  17   5 147  89  79  49  79  14 198  31 131 146 165  50
 [37] 129  44  64 123  59  37  63  82  86  40  47  59 100  41  31  66 122  82
 [55]  90 203  81  56  43 136  60 179  88  69   4  49 162 111 146  61  58  69
 [73] 127  88 161  31 198  14  77   9  91 158  19 156  19 107  12  67 171  61
 [91] 113  38 129  77 151  36 157 139  45  56&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A vector that contains every thirteenth element of a vector
every_nth_element_for_loop(x = x, n = 13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1]   0   0   0   0   0   0   0   0   0   0   0   0  24   0   0   0   0   0
 [19]   0   0   0   0   0   0   0  89   0   0   0   0   0   0   0   0   0   0
 [37]   0   0  64   0   0   0   0   0   0   0   0   0   0   0   0  66   0   0
 [55]   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0
 [73]   0   0   0   0   0  14   0   0   0   0   0   0   0   0   0   0   0   0
 [91] 113   0   0   0   0   0   0   0   0   0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find sum
sum(every_nth_element_for_loop(x = x, n = 13))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 374&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Using while loop&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
every_nth_element_while_loop &amp;lt;- function(x, n) {

  # Length of vector
  length &amp;lt;- length(x)
  # Initial value
  value &amp;lt;- 0
  # Initialize counter
  counter &amp;lt;- n
  # Loop
  # Use modulo to ensure that, whenver the counter gets to the nth element, the logical evaluates to true
  while (counter %% n == 0) {

    # Extract the element from x using the index &amp;quot;counter&amp;quot;
    # This counter is every nth element in the vector or the logical above wouldn&amp;#39;t have evaluated to true
    # Alter the value by add the nth term
    value &amp;lt;- value + x[[counter]]

    # Increase the counter by n
    # Now the logical above will again evaluate to true
    counter &amp;lt;- counter + n

    # Exit condition
    if (counter &amp;gt; length) {
      break
    }
  }

  # Sum
  value
}
# Test (This result should corroborate with that of the function above)
every_nth_element_while_loop(x = x, n = 13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 374&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Using subsetting and &lt;code&gt;seq()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function
every_nth_element_subsetting &amp;lt;- function(x, n) {

  # Define the nth term
  n &amp;lt;- n
  # Create a sequence of indices for subsetting
  seq &amp;lt;- seq.int(from = n, to = length(x), by = n)
  # Sum
  sum(x[seq])
}
# Test
every_nth_element_subsetting(x = x, n = 13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 374&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;chart-the-flow-of-a-simple-program&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chart the flow of a simple program&lt;/h2&gt;
&lt;p&gt;       Charting the flow of the following program is a good way to see how for loops work in R. We will write out the program line by line so as to understand what it is doing &lt;em&gt;exactly&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3 # line 1
for (i in 1:3) { # line 2
  show(x) # line 3
  if (x[[i]] %% 2 == 0) { # line 4
    x[[i + 1]] &amp;lt;- x[[i]] / 2 # line 5
  } else { # line 6
    x[[i + 1]] &amp;lt;- 3 * x[[i]] + 1 # line 7
  } # line 8
} # line 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3
[1]  3 10
[1]  3 10  5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(x) # line 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  3 10  5 16&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;line 1: Set x equal to 3, which is a double vector of length 1.&lt;/li&gt;
&lt;li&gt;line 2: Set i to 1.&lt;/li&gt;
&lt;li&gt;line 3: Show x to the screen.&lt;/li&gt;
&lt;li&gt;line 4: Take the first element of x and divide by 2; by default, r returns the dividend if the divisor is larger than the dividend (i.e. 2 &amp;gt; 1 and so 1 %% 2 is 1).
Therefore, (x[[i]] %% 2 == 0) evaluates to FALSE. Proceed to line 7.&lt;/li&gt;
&lt;li&gt;line 7: Carry out the sub-assignment by setting the second element of x to 10.&lt;/li&gt;
&lt;li&gt;line 8: End of else action.&lt;/li&gt;
&lt;li&gt;line 9: End of for loop and return to line 2.&lt;/li&gt;
&lt;li&gt;line 2: Set i to 2.&lt;/li&gt;
&lt;li&gt;line 3: Show x, which is now a length-2 vector &lt;span class=&#34;math inline&#34;&gt;\(\langle3,10\rangle\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;line 4: The expression (x[[2]] %% 2 == 0) evaluates to TRUE, since 10 divided by 2 is 5 remainder 0. Proceed to line 5.&lt;/li&gt;
&lt;li&gt;line 5: Sub-assign the third element in x as 5; x[[2]] is 10 and 10 divided by 2 is 5.&lt;/li&gt;
&lt;li&gt;line 6: End of if statement and return to line 2.&lt;/li&gt;
&lt;li&gt;line 2: Set i to 3.&lt;/li&gt;
&lt;li&gt;line 3: Show x, which is now a length-3 vector &lt;span class=&#34;math inline&#34;&gt;\(\langle3,10,5\rangle\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;line 4: (x[[3]] %% 2 == 0) evaluates to FALSE since x[[3]] is 5 and 5 %% 2 is 2 remainder 1. Proceed to line 7&lt;/li&gt;
&lt;li&gt;line 7: Sub-assign the fourth element of x to 16 since &lt;span class=&#34;math inline&#34;&gt;\((3\times5)+ 1=16\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;line 8: End of else action.&lt;/li&gt;
&lt;li&gt;line 9: End of for loop. The sequence is exhausted. Proceed to line 10.&lt;/li&gt;
&lt;li&gt;line 10: Show x, which is now a length-4 vector &lt;span class=&#34;math inline&#34;&gt;\(\langle3,10,5,16\rangle\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;the-lotka-volterra-model-for-a-predator-prey-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Lotka-Volterra Model for a predator-prey system&lt;/h2&gt;
&lt;p&gt;       We suppose that &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; is the number of prey animals at the start of a year &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; (rabbits) and &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt; is the number of predators (foxes), then the Lotka-Volterra model is:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
x(t+1) &amp;amp;=x(t)+b_{r} \cdot x(t)-d_{r} \cdot x(t) \cdot y(t) \\
y(t+1) &amp;amp;=y(t)+b_{f} \cdot d_{r} \cdot x(t) \cdot y(t)-d_{f} \cdot y(t)
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the parameters are defined by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_{r}\)&lt;/span&gt; is the natural birth rate of rabbits in the absence of predation;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(d_{r}\)&lt;/span&gt; is the death rate per encounter of rabbits due to predation;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(d_{f}\)&lt;/span&gt; is the natural death rate of foxes in the absence of food (rabbits);&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_{f}\)&lt;/span&gt; is the efficiency of turning predated rabbits into foxes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Growth rate of rabbits
br &amp;lt;- 0.04
# Death rate of rabbits due to predation
dr &amp;lt;- 0.0005
# Death rate of foxes in the absence of of food
df &amp;lt;- 0.2
# Efficiency of turning predated rabbits into foxes
bf &amp;lt;- 0.1
# Initial predator/prey populations
x &amp;lt;- 4200
y &amp;lt;- 100
# Model output
while (x &amp;gt; 3900) { # line 1
  cat(&amp;quot;x =&amp;quot;, x, &amp;quot; y =&amp;quot;, y, &amp;quot;\n&amp;quot;) # line 2
  x.new &amp;lt;- (1 + br) * x - dr * x * y # line 3
  y.new &amp;lt;- (1 - df) * y + bf * dr * x * y # line 4
  x &amp;lt;- x.new # line 5
  y &amp;lt;- y.new # line 6
} # line 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x = 4200  y = 100 
x = 4158  y = 101 
x = 4114.341  y = 101.7979 
x = 4069.499  y = 102.3799 
x = 4023.962  y = 102.7356 
x = 3978.218  y = 102.8587 
x = 3932.749  y = 102.7467 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;line 1: The initial population of rabbits is &lt;span class=&#34;math inline&#34;&gt;\(x = 4000\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\((x &amp;gt; 3900)\)&lt;/span&gt; evaluates to TRUE. Proceed to line 2.&lt;/li&gt;
&lt;li&gt;line 2: Concatenate and print the populations of predator and prey at state one. The “dash n” in &lt;code&gt;cat&lt;/code&gt; means start a new line, ensuring that the printed output are printed lines by line successively instead of just one line.&lt;/li&gt;
&lt;li&gt;line 3: Compute the new population of rabbits and bind that object value to the name x.new.&lt;/li&gt;
&lt;li&gt;line 4: Compute the new population foxes and bind that object value to the name y.new.&lt;/li&gt;
&lt;li&gt;line 5: Bind x.new to x.&lt;/li&gt;
&lt;li&gt;line 6: Bind y.new to y.&lt;/li&gt;
&lt;li&gt;line 7: End of while loop. Return to line 1.&lt;/li&gt;
&lt;li&gt;line 1: If (x &amp;gt; 3900) still evaluates to TRUE, repeat as above for state two, three, and so on. If not, end of program.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;find-the-minimum-of-a-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Find the Minimum of a Vector&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;find_min_max &amp;lt;- function(x, summary_stat) {

  # Find minimum or maximum
  if (summary_stat == &amp;quot;min&amp;quot;) {

    # Initialize minimum value
    x_min &amp;lt;- x[[1]]
    # Loop
    for (i in 2:length(x)) {
      if (x_min &amp;gt; x[[i]]) {
        x_min &amp;lt;- x[[i]]
      }
    }
    # Output
    x_min
  } else if (summary_stat == &amp;quot;max&amp;quot;) {

    # Initialize minimum value
    x_max &amp;lt;- x[[1]]
    # Loop
    for (i in 2:length(x)) {
      if (x_max &amp;lt; x[[i]]) {
        x_max &amp;lt;- x[[i]]
      }
    }
    # Output
    x_max
  } else {

    # Warning
    rlang::abort(message = &amp;quot;summary_stat must either be min or max&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       The function above uses if statements and for loops; there may be a need to benchmark performance. However, we are not creating copies each time we create a binding from the name “x_min” to a new vector object. This is because the new vector object only has a single name bound to it, and so R applies the modify-in-place optimization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test vector
x &amp;lt;- sample(x = 20:1923, size = 1000, replace = FALSE)
# Find min and max
find_min_max(x, summary_stat = &amp;quot;min&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;find_min_max(x, summary_stat = &amp;quot;max&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1918&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Confirm using base R functions
min(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       That is it for control flows in R! Hopefully, this was helpful.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Mathematical Concepts in R</title>
      <link>https://www.kenwuyang.com/en/post/implementing-mathematical-concepts-in-r/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/implementing-mathematical-concepts-in-r/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/implementing-mathematical-concepts-in-r/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-binomial-theorem&#34;&gt;The Binomial Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pascals-triangle&#34;&gt;Pascal’s Triangle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In my first post, we will be implementing some interesting mathematical objects in R.&lt;/p&gt;
&lt;p&gt;Why? Why not? I believe it’s a good way to practice solving problems in R.&lt;/p&gt;
&lt;div id=&#34;the-binomial-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Binomial Theorem&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    (x+y)^n=\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Expanding:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    (x+y)^n=\binom{n}{0}x^{0}y^{n-0}+\binom{n}{1}x^{1}y^{n-1}+\binom{n}{2}x^{2}y^{n-2}+... \\ +\binom{n}{n-1}x^{n-1}y^{n-(n-1)}+\binom{n}{n}x^{n}y^{n-n}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R, we can tackle the implementation of the Binomial Theorem in three parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the binomial coefficient&lt;/li&gt;
&lt;li&gt;raising the real number &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to the vector of powers &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;raising the real number &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to the vector of powers &lt;span class=&#34;math inline&#34;&gt;\(n-k\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;binomial_theorem &amp;lt;- function(x, y, n) {

  # Create a sequence from k = 0 to k = n
  seq_k_n &amp;lt;- seq.int(from = 0, to = n, by = 1)


  # Pre-allocate container for storing coefficients
  binom_coeffs &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = n + 1)
  # Binomial coefficients
  binom_coeffs &amp;lt;- purrr::map_dbl(.x = seq_k_n, .f = choose, n = n)


  # Pre-allocate container for storing the x&amp;#39;s
  vector_of_x &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = n + 1)
  # Raise x to the power of y
  vector_of_x &amp;lt;- x^(seq_k_n)


  # Pre-allocate container for storing the y&amp;#39;s
  vector_of_y &amp;lt;- vector(mode = &amp;quot;double&amp;quot;, length = n + 1)
  # Raise y to the power of n-k
  vector_of_y &amp;lt;- y^(n - (seq_k_n))


  # Product of the two vectors and their coefficients
  prod &amp;lt;- binom_coeffs * vector_of_x * vector_of_y

  # Summation operator
  result &amp;lt;- sum(prod)
  result
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Let’s see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test
x &amp;lt;- 924
y &amp;lt;- 23
n &amp;lt;- 39
# Compute by hand
(x + y)^n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.195774e+116&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute using custom function
binomial_theorem(x = x, y = y, n = n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.195774e+116&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, the results are exactly the same.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;pascals-triangle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pascal’s Triangle&lt;/h2&gt;
&lt;p&gt;Directly related to the Binomial coefficient is Pascal’s triangle, whose entries in each row are usually staggered relative to the numbers in the adjacent rows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Pascal&#39;s Triangle.png&#34; width=&#34;50%&#34; height=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To implement Pascal’s triangle, we will use a for loop. Our goal is to write a program that finds the next row of Pascal’s triangle, given the previous rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finding the (n + 1)th row of a Pascal&amp;#39;s triangle given n rows that precede it
pascal_triangle_n_plus_1 &amp;lt;- function(x) {
  if (!is.list(x)) {
    rlang::abort(message = &amp;quot;The input object must a be a list containing the rows of Pascal&amp;#39;s Triangle.&amp;quot;)
  }

  # Set n equal to depth of the input list &amp;quot;x&amp;quot;, that is, the number of elements in x, where each represents a row
  n &amp;lt;- length(x)
  # Extract the last element (the nth row) from the input list &amp;quot;x&amp;quot; and store it as a new variable x_n
  # Use [[ to extract the value rather than a sub-list
  x_n &amp;lt;- x[[n]]
  # Repeat the integer &amp;quot;1&amp;quot; (n + 1) times
  # Note that the (n + 1)th  row has (n + 1) elements beginning and ending with 1
  x_n_plus_1 &amp;lt;- rep(x = 1, times = n + 1)

  # Loop to add all adjacent pairs in the nth row to obtain the (n + 1)th row
  # Start with the second element and end with second to last element of each row
  # This is because the first and last numbers in any given row are always 1
  if (n &amp;gt; 1) {
    # This is the prefix form of for loop
    `for`(
      var = i,
      seq = 2:n,
      action = x_n_plus_1[[i]] &amp;lt;- x_n[[i - 1]] + x_n[[i]]
    )
  }

  # Append the (n + 1)th row to the list object
  base::append(x, values = list(x_n_plus_1))
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Let’s see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a Pascal&amp;#39;s Triangle with 4 rows
x &amp;lt;- list(c(1), c(1, 1), c(1, 2, 1), c(1, 3, 3, 1))
# Row 5
x &amp;lt;- pascal_triangle_n_plus_1(x = x)
x[[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 4 6 4 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Row 6
x &amp;lt;- pascal_triangle_n_plus_1(x = x)
x[[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1  5 10 10  5  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We know that these row entries can be computed are the binomial coefficients 5 choose 0 thru 5 choose 5
purrr::map2_dbl(
  .x = rep(x = 5, times = 6),
  .y = seq.int(from = 0, to = 5, by = 1),
  .f = choose
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1  5 10 10  5  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for my first post! I’ll update this post to include more interesting topics for my future self and perhaps others who may stumble across this.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
