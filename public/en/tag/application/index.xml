<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Application | Yang (Ken) Wu</title>
    <link>https://www.kenwuyang.com/en/tag/application/</link>
      <atom:link href="https://www.kenwuyang.com/en/tag/application/index.xml" rel="self" type="application/rss+xml" />
    <description>Application</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Yang Wu</copyright><lastBuildDate>Sat, 04 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.kenwuyang.com/media/Sharing.png</url>
      <title>Application</title>
      <link>https://www.kenwuyang.com/en/tag/application/</link>
    </image>
    
    <item>
      <title>Statistical Factories in R (Part II)</title>
      <link>https://www.kenwuyang.com/en/post/statistical-factories-part-ii/</link>
      <pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/statistical-factories-part-ii/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#box-cox-transformation&#34;&gt;Box-Cox Transformation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bootstrap-generator&#34;&gt;Bootstrap Generator&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories-1&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-using-function-factories-2&#34;&gt;Implementation using function factories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In my &lt;a href=&#34;https://www.kenwuyang.com/en/post/function-factories-part-i/&#34;&gt;post on function factories&lt;/a&gt;, we discussed the R data structure that powers function factories— environments. Now, we will focus on some applications of function factories in statistics. Again, the content of this post is inspired by &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;; those who may be interested in learning more could turn to Hadley’s book for more information. Here we go!&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;box-cox-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Cox Transformation&lt;/h2&gt;
&lt;p&gt;       The Box-Cox procedure is used in statistical analysis to identify a transformation from the family of power transformations on a univariate variable, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, in order to address the skewness of its distribution. The family of power transformation is of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Y^{\prime}=Y^{\lambda}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a parameter to be determined from the data or taken as given. For univariate data &lt;span class=&#34;math inline&#34;&gt;\(\{Y_{1}, Y_{2},...,Y_{n}\}\)&lt;/span&gt;, the transformation has the following form:
&lt;span class=&#34;math display&#34;&gt;\[
Y_{i}(\lambda)=\left\{\begin{array}{ll}
\frac{Y_{i}^{\lambda}-1}{\lambda}, &amp;amp;  \lambda \neq 0 \\
\log_{e} Y_{i}, &amp;amp; \lambda=0
\end{array}\right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       Here is the function factory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;box_cox_factory &amp;lt;- function(lambda) {
  if (!is.double(lambda)) {
    rlang::abort(message = &amp;quot;The argument lambda must be a numeric vector.&amp;quot;)
  }

  if (lambda == 0) {
    function(y) log(y, base = exp(1))
  } else {
    function(y) ((y^lambda) - 1) / lambda
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a transformation function where lambda = 5
box_cox_5 &amp;lt;- box_cox_factory(lambda = 5)
# Create a vector of non-normal data
y &amp;lt;- rbeta(n = 500, shape1 = 6, shape2 = 1)
# Visualize
ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &amp;quot;black&amp;quot;, fill = &amp;quot;orange&amp;quot;
  ) +
  labs(title = &amp;quot;Pre-transformation&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# After transformation
ggplot(data = tibble::tibble(&amp;quot;new_y&amp;quot; = box_cox_5(y)), mapping = aes(x = new_y)) +
  geom_histogram(
    binwidth = function(y) (max(y) - min(y)) / nclass.FD(y),
    color = &amp;quot;black&amp;quot;, fill = &amp;quot;orange&amp;quot;
  ) +
  labs(title = &amp;quot;Post-transformation&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       This is by no means the &lt;em&gt;optimal&lt;/em&gt; transformation, but it allows us to see the significant change to the distribution of our data. An added benefit of using the function factory is that we can visually explore how different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; transform our data. This can be carried out with the help of &lt;code&gt;ggplot2::stat_function&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;box_cox_plot_function &amp;lt;- function(lambda) {
  # Change line color based on lambda value
  ggplot2::stat_function(
    mapping = aes(color = lambda),
    # Use our function factory, which will return different manufactured functions based on &amp;quot;lambda&amp;quot;
    fun = box_cox_factory(lambda = lambda),
    size = 1
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see how different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\{0, 1, 2, 3, 4, 5\}\)&lt;/span&gt; change our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble::tibble(y), mapping = aes(x = y)) +
  purrr::map(.x = c(0, 1, 2, 3, 4, 5), .f = box_cox_plot_function) +
  scale_color_viridis_c(limits = c(0, 5)) +
  labs(
    y = &amp;quot;Transformed Y&amp;quot;,
    x = &amp;quot;Original Y&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The transformation can be applied in linear regression analysis as a remedial measure when model conditions— linearity, constancy of error variance— are not met. I will also cover this topic in my future posts as well.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-generator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrap Generator&lt;/h2&gt;
&lt;p&gt;       The bootstrap is a powerful statistical technique that allows us to quantify the uncertainty associated with a given estimator or statistic. Ideally, to quantify the uncertainty of an estimator or statistic, we would obtain new samples of data from the population, obtaining estimators or statistics for each individual sample. In reality, however, obtaining repeated samples from a given population can be costly. The bootstrap method emulates the process of obtaining new samples, allowing us to estimate the variability of estimators or statistics without actually generating additional samples. &lt;em&gt;Rather than repeatedly obtaining independent samples from the population, the method instead obtains distinct samples by repeatedly sampling observations from the original sample.&lt;/em&gt; The following diagram illustrates the essence of bootstrapping:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;bootstrap.png&#34; alt=&#34;Diagram from [An Introduction to Statistical Learning](https://www.statlearning.com/)&#34; width=&#34;489&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Diagram from &lt;a href=&#34;https://www.statlearning.com/&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;       Let us examine an application. Suppose we have a data set with two variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;; we wish to fit the simple linear regression model (&lt;span class=&#34;math inline&#34;&gt;\(E\{Y\}=\beta_{0}+\beta_{1}X\)&lt;/span&gt;) to the data. The point estimator we are interested is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt; and we wish to quantify the variability of this estimator. We can solve analytically for the point estimator of the variance of the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat{\sigma}^{2}\{\hat{\hat{\beta}_{1}}\}=\frac{MSE}{\sum(X_{1}-\bar{X})^{2}}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, we are not satisfied with just one point estimator. We would like to generate bootstrap samples of the data, fit the simple regression model to each sample, and obtain a point estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt; for each of the models. Then, we will empirically obtain the variance of the sampling distribution of all of these estimators, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       To solve this problem in R, we can combine function factories and functionals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate a random data frame
data &amp;lt;- tibble::tibble(
  y = sample(x = 1:200, size = 100, replace = FALSE),
  x = sample(x = 1:200, size = 100, replace = FALSE)
)
# Find the point estimator of the variance of the sampling distribution of beta hat
lm(y ~ x, data = data) %&amp;gt;%
  summary() %&amp;gt;%
  purrr::pluck(.x = ., &amp;quot;coefficients&amp;quot;) %&amp;gt;%
  `[`(4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1024499&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create a function factory, and it takes as input a given data frame and a variable from which the bootstrap sample is to be generated. Ultimately, the function factory will return a manufactured function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bootstrap_factory &amp;lt;- function(df, var) {

  # Initialize n
  n &amp;lt;- nrow(df)
  # Force evaluation of var
  base::force(var)

  # Manufactured function
  function() {
    # Select the variables and modify them
    # Use &amp;quot;drop&amp;quot; to prevent data frame from being simplified to a matrix
    df[var] &amp;lt;- df[var][sample(x = 1:n, replace = TRUE), , drop = FALSE]
    df
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       The benefit of creating a function factory is that this bootstrap generator is data frame and variable-agnostic. If we wish to create a bootstrapping function for another data frame or other variables, we could simply create another manufactured function for that task. In the simple linear regression case with one independent variable, it may not be obvious why a function factory is beneficial. However, imagine now we have a 20-variable data frame that we need to bootstrap. This function factory will save us a lot of time in terms of copying-and-pasting, minimizing code length and the amount of typing. Next, let us generate 1000 bootstrapped data frames based on the original data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a list of 1000 data frames
# Each list element is a bootstrapped data frame
list_of_data_frames &amp;lt;- purrr::map(
  .x = 1:1000,
  # Create a manufactured function
  .f = ~ bootstrap_factory(df = data, var = &amp;quot;y&amp;quot;)()
)
# Next fit the model to each of the bootstrapped data frames and extract the coefficient
vector_of_betas &amp;lt;- list_of_data_frames %&amp;gt;%
  # Fit the model to each of the 1000 data frames
  # This is a list of &amp;quot;lm&amp;quot; model objects
  purrr::map(.x = ., .f = ~ lm(y ~ x, data = .x)) %&amp;gt;%
  # Now extract the estimator of the coefficient on x from each of the model summary output
  purrr::map_dbl(.x = ., .f = ~ stats::coef(.x)[[2]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       As can be seen, the whole process only takes about 6 lines of code. Here’s the sampling distribution of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = tibble::tibble(x = vector_of_betas), mapping = aes(x = x)) +
  geom_histogram(
    binwidth = function(x) (max(x) - min(x)) / nclass.FD(x),
    color = &amp;quot;black&amp;quot;,
    fill = &amp;quot;orange&amp;quot;
  ) +
  theme(
    panel.background = element_rect(fill = &amp;quot;white&amp;quot;),
    panel.grid = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/statistical-factories-part-ii/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the standard deviation of this sampling distribution is 0.102395. Does this confirm our standard error calculation from earlier or refute it?&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;       Another application of function factories is the estimation of the parameters of the normal error regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Y_{i}=\beta_{0}+\beta_{1}X_{i} + \varepsilon_{i}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this model, each &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; observation is normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The method of maximum likelihood uses the density of the probability distribution at &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; as a measure of consistency for the observation &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt;. In general, the density of an observation &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; for the normal error model is given as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
f_{i}=\frac{1}{\sqrt{2\sigma}}\text{exp}\bigg[-\frac{1}{2}\bigg(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma}\bigg)^2\bigg]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;       In R, this density can be computed using the &lt;code&gt;dnorm&lt;/code&gt; function. We will specify the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y=Y_{i}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{expected_y}=E\{Y\}=\beta_{0}+\beta_{1}X_{i}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{sigma}=\sigma\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dnorm(x = y, mean = expected_y, sd = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The maximum likelihood function uses the product of the densities of all the &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}\)&lt;/span&gt; observations as the measure of consistency of the parameter values with the sample data.&lt;/strong&gt; In other words, the likelihood function for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations &lt;span class=&#34;math inline&#34;&gt;\(Y_{1},Y_{2},...Y_{n}\)&lt;/span&gt; is the product of the individual densities &lt;span class=&#34;math inline&#34;&gt;\(f_{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    L(\beta_{o},\beta_{1},\sigma)=\prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may work with the natural log of &lt;span class=&#34;math inline&#34;&gt;\(L(\beta_{o},\beta_{1},\sigma)\)&lt;/span&gt; since both &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log_{e}L\)&lt;/span&gt; are maximized for the same values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{o},\beta_{1},\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Plus, we can use the product rule such that the natural log of a product of is the sum of the natural logs, &lt;span class=&#34;math inline&#34;&gt;\(\ln(xyz)=\ln(x)+\ln(y)+\ln(z)\)&lt;/span&gt;. Therefore, taking the natural log of the likelihood function means that the right hand side of the equation becomes the sum of the log densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \log_{e}L(\beta_{o},\beta_{1},\sigma)=\sum_{i = 1}^{n}\log_{e}\Bigg[ \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left[-\frac{1}{2}(\frac{Y_{i}-\beta_{0}-\beta_{1}X_{i}}{\sigma})^2\right]\Bigg]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will simplify our implementation in R.&lt;/p&gt;
&lt;div id=&#34;implementation-using-function-factories-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation using function factories&lt;/h3&gt;
&lt;p&gt;       For R implementation, we first need to generate some random data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate random data
data &amp;lt;- tibble(
  x = rnorm(n = 150, mean = 2, sd = 24),
  y = rnorm(n = 150, mean = 45, sd = 7)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For comparison, let us also obtain the estimators of the parameters using the least squares method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model
model &amp;lt;- lm(y ~ x, data = data) %&amp;gt;%
  summary()
# Least squares estimators
purrr::pluck(.x = model, &amp;quot;coefficients&amp;quot;) %&amp;gt;%
  `[`(c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 44.572055917  0.001569285&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sigma
purrr::pluck(.x = model, &amp;quot;sigma&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7.168166&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, create the function factory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;likelihood_factory &amp;lt;- function(x, y) {

  # Initialize y and x
  y &amp;lt;- force(y)
  x &amp;lt;- force(x)

  # Manufactured function
  function(beta0, beta1, sigma) {

    # Linear regression model
    # The value of &amp;quot;x&amp;quot; is scoped from the enclosing environment of this manufactured function
    expected_y &amp;lt;- beta0 + beta1 * x
    # Negative log-likelihood function
    # The value of &amp;quot;y&amp;quot; is scoped from the enclosing environment of this manufactured function
    # Negatively scale the log-likelihood values since we want to maximize
    -sum(dnorm(x = y, mean = expected_y, sd = sigma, log = TRUE))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       The advantage of creating a function factory is that we initialize “x” and “y” in the execution environment of &lt;code&gt;likelihood_factory&lt;/code&gt;, which is the enclosing environment of the manufactured function and where it scopes for the values of “x” and “y”. Without the captured and encapsulated environment of a factory function, “x” and “y” will have to be stored in the global environment. Here they can be overwritten or deleted as well as interfere with other bindings. So, in a sense, the function factory here provides an extra layer of safeguarding. To find the maximum likelihood estimators, we supply a manufactured function to the &lt;code&gt;bbmle::mle2&lt;/code&gt; function from the &lt;code&gt;bbmle&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), 
            start = list(beta0 = 0, beta1 = 0, sigma = 50))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
bbmle::mle2(minuslogl = likelihood_factory(data$x, data$y), start = list(beta0 = 0, 
    beta1 = 0, sigma = 50))

Coefficients:
       beta0        beta1        sigma 
44.572057288  0.001569382  7.120216813 

Log-likelihood: -507.28 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do the maximum likelihood estimators compare to those of the least squares method? (Hint: For normal data, they should be consistent with each other.)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;       And that is all for this post. We have covered three interesting applications of function factories in statistics. Among all of R’s functional programming tool-kits, function factories are perhaps the least utilized features as far as data science is concerned. This is likely because functional factories tend to have more of a mathematical or statistical flavor to them. However, as we have seen in this post and &lt;a href=&#34;https://www.kenwuyang.com/post/function-factory-estimating-probabilities-of-returns/&#34;&gt;others&lt;/a&gt;, there is a class of problems, particular in mathematical statistics, to which function factories could offer elegant solutions in R. Therefore, having them in our R toolkit may certainly reap some long term benefits, allowing us to solve a variety of problems beyond those that are typically encountered in data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applications of Subsetting Operators in R</title>
      <link>https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/applications-of-subsetting-operators-in-r/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#matching-and-merging-by-hand-integer-subsetting&#34;&gt;Matching and merging by hand (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-samples-and-bootstraps-integer-subsetting&#34;&gt;Random samples and bootstraps (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordering-integer-subsetting&#34;&gt;Ordering (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expanding-aggregated-counts-integer-subsetting&#34;&gt;Expanding aggregated counts (integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#removing-columns-from-data-frames-character-subsetting&#34;&gt;Removing columns from data frames (character subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boolean-algebra-versus-sets-logical-and-integer-subsetting&#34;&gt;Boolean algebra versus sets (logical and integer subsetting)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition-of-the-operator&#34;&gt;Definition of the %% operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rondomly-permute-a-data-frame-a-technique-often-used-in-random-forests&#34;&gt;Rondomly permute a data frame (a technique often used in random forests)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-a-random-sample-of-m-rows-from-a-data-frame&#34;&gt;Selecting a random sample of m rows from a data frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordering-the-columns-in-a-data-frame-alphabetically&#34;&gt;Ordering the columns in a data frame alphabetically&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In this post, we will cover some useful applications of R’s subsetting operations. The content of this post is gleaned from Hadley Wickham’s &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advance R&lt;/a&gt;. This book is aimed at helping R users improve their programming skills beyond day-to-day data analysis. To better understand the content of this post, I recommend reading chapter 4 of Hadley’s book beforehand. Or, if you are already familiar with R’s subsetting operators, jump right in.&lt;/p&gt;
&lt;p&gt;       I wanted to document some of the content from Hadley’s book with my added commentary to help my future self as well as others who may accidentally stumble across this post. I believe that many of these examples can be extended and employed in a variety of settings, and so my goal here is to turn this post into a resource not just for myself but perhaps others in their daily use of R. With that being said, let’s get started.&lt;/p&gt;
&lt;div id=&#34;matching-and-merging-by-hand-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matching and merging by hand (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       The function &lt;code&gt;match()&lt;/code&gt; returns a vector that contains the position indices of the (first) matches of its first argument “x =” in its second “table =”. For instance, &lt;code&gt;match(x, table)&lt;/code&gt; will return the position where each element in “x” is found in “table.” This function allows us to create look-up tables. For instance, say we observe a vector of student grades in the world and a table that describe their properties. Let us say our goal is to create a data frame where each row is an observation of student grade and each column is a property associated with that letter grade. We can use a look-up table to map the properties to our vector of grades:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Grades
grades &amp;lt;- c(1, 2, 2, 3, 1)
# Info
info &amp;lt;- data.frame(
  grade = 3:1,
  desc = c(&amp;quot;Excellent&amp;quot;, &amp;quot;Good&amp;quot;, &amp;quot;Poor&amp;quot;),
  fail = c(F, F, T)
)
info&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  grade      desc  fail
1     3 Excellent FALSE
2     2      Good FALSE
3     1      Poor  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Match the grades to the &amp;quot;grade&amp;quot; column in the info table
# This is a vector indices we would later use to subset the info table
id &amp;lt;- match(x = grades, table = info[[&amp;quot;grade&amp;quot;]])
id&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 2 2 1 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Subset the info table as a matrix
# Select rows according to the order in which they appear in the index vector
info[id, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    grade      desc  fail
3       1      Poor  TRUE
2       2      Good FALSE
2.1     2      Good FALSE
1       3 Excellent FALSE
3.1     1      Poor  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we’ve selected the rows in the info table, sometimes more than once, so that each row is an observation of student grade.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;random-samples-and-bootstraps-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random samples and bootstraps (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       If we would like to randomly sample or bootstrap a vector or a data frame, we can use &lt;code&gt;sample()&lt;/code&gt; to generate a random index vector. A shortcut of the &lt;code&gt;sample()&lt;/code&gt; function: If the argument x has length 1, is a numeric vector (in the sense of &lt;code&gt;is.numeric()&lt;/code&gt;), and is &amp;gt;= 1, then sampling via &lt;code&gt;sample()&lt;/code&gt; will only return random vales from the sequence 1 to x.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create data frame
df &amp;lt;- data.frame(x = c(1, 2, 3, 1, 2), y = 5:1, z = letters[1:5])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
1 1 5 a
2 2 4 b
3 3 3 c
4 1 2 d
5 2 1 e&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly reorder the rows
# Select the rows in the order they appear in the random vector created by sample()
df[sample(x = nrow(df)), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
5 2 1 e
4 1 2 d
1 1 5 a
3 3 3 c
2 2 4 b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select three random rows in the order they appear in the random vector
df[sample(x = nrow(df), size = 3), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
2 2 4 b
5 2 1 e
1 1 5 a&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select 8 bootstrap replicates
# Notice that replace = TRUE, which indicates that some rows will be selected more than once
df[sample(x = nrow(df), size = 8, replace = TRUE), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    x y z
2   2 4 b
5   2 1 e
4   1 2 d
4.1 1 2 d
1   1 5 a
5.1 2 1 e
2.1 2 4 b
4.2 1 2 d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we utilize the &lt;code&gt;sample()&lt;/code&gt; function to generate a random index vector, which we then use to subset the data frame. We can easily automate this bootstrapping process by writing our own function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bootstrap data frame
boots_df &amp;lt;- function(df, n, replicate) {

  # Create n index vectors
  # This returns a list of random index vectors each with size = replicate
  list_of_indices &amp;lt;- map(
    .x = 1:n,
    .f = ~ sample(
      x = 1:nrow(df),
      size = replicate,
      replace = TRUE
    )
  )

  # Pre-allocate list container
  list_of_bootstrapped_df &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = n)
  # Loop
  for (i in seq_along(1:n)) {

    # Select bootstrapped &amp;quot;rows&amp;quot; from the data frame
    list_of_bootstrapped_df[[i]] &amp;lt;- df[list_of_indices[[i]], ]
  }

  # Output is a list of &amp;quot;n&amp;quot; bootstrapped data frames, each with nrow = replicate
  list_of_bootstrapped_df
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action. Suppose we wish to produce 8 bootstrap replicates of the rows of a data frame, and we wish to do this 4 times. Using our function above, we see that the arguments are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;n = 4&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;replicate = 8&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(boots_df(df = df, n = 4, replicate = 8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 4
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 3 1 2 1 3 2 3 2
  ..$ y: int [1:8] 3 5 4 2 3 4 3 4
  ..$ z: chr [1:8] &amp;quot;c&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;d&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 3 3 1 3 1 1 3 3
  ..$ y: int [1:8] 3 3 5 3 5 5 3 3
  ..$ z: chr [1:8] &amp;quot;c&amp;quot; &amp;quot;c&amp;quot; &amp;quot;a&amp;quot; &amp;quot;c&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 1 1 2 3 2 3 1 1
  ..$ y: int [1:8] 2 2 4 3 4 3 5 2
  ..$ z: chr [1:8] &amp;quot;d&amp;quot; &amp;quot;d&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; ...
 $ :&amp;#39;data.frame&amp;#39;:   8 obs. of  3 variables:
  ..$ x: num [1:8] 2 2 2 3 1 1 3 1
  ..$ y: int [1:8] 4 4 1 3 5 2 3 5
  ..$ z: chr [1:8] &amp;quot;b&amp;quot; &amp;quot;b&amp;quot; &amp;quot;e&amp;quot; &amp;quot;c&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, we have a list of 4 data frames each with 8 rows of bootstrapped replicates. This function can be easily scaled to generate more bootstrap samples and more replicates per sample.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordering (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       The function &lt;code&gt;order()&lt;/code&gt; takes a vector as its input and returns an integer vector describing how to order the subsetted vector. The values in the returned integer vector are “pull” indices; that is, each order(x)[i] tells the position that each x[i] is in the “un-ordered” vector.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a character vector that is out of order
x &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;a&amp;quot;)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;a&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find the position of each alphabet in &amp;quot;x&amp;quot; and order them
order(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now select the elements from &amp;quot;x&amp;quot; in the order in which they appear in order(x)
x[order(x)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To break ties, you can supply additional variables to order(). You can also change the order from ascending to descending by using decreasing = TRUE. By default, any missing values will be put at the end of the vector; however, you can remove them with na.last = NA or put them at the front with na.last = FALSE.&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Example 2&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create &amp;quot;un-ordered&amp;quot; vector
set.seed(7)
y &amp;lt;- sample(x = 1:8, replace = TRUE)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 3 7 4 7 2 7 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find the position of each number in &amp;quot;x&amp;quot; and order them
order(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 6 8 2 4 3 5 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# According to order(y)
# Select the elements from y in this order:
y[order(y)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 2 2 3 4 7 7 7&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;For two or more dimensional objects, &lt;code&gt;order()&lt;/code&gt; and integer subsetting makes it easy to order either the rows or columns of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly reorder the rows
# Select columns 3, 2, and 1 in that order
df2 &amp;lt;- df[sample(x = 1:nrow(df)), 3:1]
df2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
3 c 3 3
4 d 2 1
2 b 4 2
1 a 5 1
5 e 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the values in column &amp;quot;x&amp;quot;
order(df2[[&amp;quot;x&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 3 5 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the rows by column x in ascension
# Select the rows based on the positions in order()
# Now the &amp;quot;x&amp;quot; column is ascending
df2[order(df2[[&amp;quot;x&amp;quot;]]), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
4 d 2 1
1 a 5 1
2 b 4 2
5 e 1 2
3 c 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Order the columns based on the alphabetical order of their names
df2[, order(names(df2))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
3 3 3 c
4 1 2 d
2 2 4 b
1 1 5 a
5 2 1 e&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could have sorted vectors directly with &lt;code&gt;sort()&lt;/code&gt;, or &lt;code&gt;dplyr::arrange()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using arrange() to order based on the &amp;quot;x&amp;quot; column
# The default order of arrangement is ascending
# This is equivalent to SQL&amp;#39;s ORDER BY
arrange(.data = df2, df2[[&amp;quot;x&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  z y x
1 d 2 1
2 a 5 1
3 b 4 2
4 e 1 2
5 c 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;arrange()&lt;/code&gt; orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, &lt;code&gt;arrange()&lt;/code&gt; largely ignores grouping; you need to explicitly mention grouping variables (or use .by_group = TRUE) in order to group by them.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;expanding-aggregated-counts-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expanding aggregated counts (integer subsetting)&lt;/h2&gt;
&lt;p&gt;       First, we need to be familiar with the function &lt;code&gt;rep(x = x, times = y)&lt;/code&gt;, which repeats x[i] y[i] times. Let’s see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat each x[i] y[i] times
rep(x = c(2, 3, 4), times = c(2, 6, 5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 2 2 3 3 3 3 3 3 4 4 4 4 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat the vector object x 3 times
rep(x = c(2, 3, 4), times = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 3 4 2 3 4 2 3 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Repeat each x[i] 3 times
rep(x = c(2, 3, 4), each = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 2 2 3 3 3 4 4 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       Sometimes you get a data frame where identical rows have been collapsed into one and a count column “n” has been added. rep() and integer subsetting make it easy to ““un-collapse”“, because we can take advantage of &lt;code&gt;rep()&lt;/code&gt;s vectorization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data frame
df &amp;lt;- data.frame(x = c(2, 4, 1), y = c(9, 11, 6), n = c(3, 5, 1))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x  y n
1 2  9 3
2 4 11 5
3 1  6 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The first row has count = 3, so repeat it 3 times
# The second row has count = 5, so repeat it 5 times
# The third row has count = 1, so do not repeat
rep(x = 1:nrow(df), times = df$n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 1 1 2 2 2 2 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select the rows in the order they appear in the rep() function
df[rep(x = 1:nrow(df), times = df$n), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    x  y n
1   2  9 3
1.1 2  9 3
1.2 2  9 3
2   4 11 5
2.1 4 11 5
2.2 4 11 5
2.3 4 11 5
2.4 4 11 5
3   1  6 1&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-columns-from-data-frames-character-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing columns from data frames (character subsetting)&lt;/h2&gt;
&lt;p&gt;There are two ways to remove columns from a data frame. You can set individual columns to NULL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame
df &amp;lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y z
1 1 3 a
2 2 2 b
3 3 1 c&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove column z
df$z &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or you can subset to return only the columns you want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame
df &amp;lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])
# Keep only columns x and y
df[c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y
1 1 3
2 2 2
3 3 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you only know the columns you don’t want, use set operations to work out which columns to keep. For instance, the function &lt;code&gt;setdiff(x, y, ...)&lt;/code&gt;— x is the full set and y is a subset x. The function &lt;code&gt;setdiff()&lt;/code&gt; returns the difference between x and y; that is, it returns those elements that are &lt;em&gt;not in the subset y&lt;/em&gt; but &lt;em&gt;in the full set “x”&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Full set
names(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Exclude x
setdiff(x = names(df), y = &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Exclude x and z
setdiff(x = names(df), y = c(&amp;quot;x&amp;quot;, &amp;quot;z&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;y&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select every column except for z
df[setdiff(names(df), &amp;quot;z&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  x y
1 1 3
2 2 2
3 3 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other useful set operations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;intersect(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;union(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;setdiff(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;setequal(x, y, …)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Read the documentations to learn more about them.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;       In addition, set operations can be useful in our day-to-day use. We very often need to &lt;code&gt;rm()&lt;/code&gt; objects from the global environment that we do need anymore. It sometimes happens that there are many objects in our environment pane, and we only wish to keep a few of them. One way to do so is to list all the objects we wish to remove by name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove object we do not need
rm(list = c(&amp;quot;object1&amp;quot;, &amp;quot;object2&amp;quot;, ...))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this can be inefficient since we need to 1) figure out which objects we’d like to remove by calling &lt;code&gt;ls()&lt;/code&gt; and 2) type all of them using &lt;code&gt;c()&lt;/code&gt;. This can be too much typing and therefore very time-consuming. Alternatively, we can use &lt;code&gt;setdiff()&lt;/code&gt; to keep only the objects that we would need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Keep only objects that we meed
rm(list = setdiff(x = ls(), y = &amp;quot;object_to_be_kept_1&amp;quot;, &amp;quot;object_to_be_kept_2&amp;quot;, ...))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;boolean-algebra-versus-sets-logical-and-integer-subsetting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boolean algebra versus sets (logical and integer subsetting)&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;which()&lt;/code&gt; gives the TRUE indices of a logical object; that is, their positions in a logical vector. Use &lt;code&gt;which.min()&lt;/code&gt; and &lt;code&gt;which.max()&lt;/code&gt; for the index of the minimum or maximum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a named logical vector
x &amp;lt;- sample(x = 1:10, replace = FALSE) &amp;lt; 4
names(x) &amp;lt;- letters[1:10]
# Convert Boolean representation to an integer representation
# Easy to see the positions of the first and last TRUE&amp;#39;s
which(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;b d g 
2 4 7 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that reverses which()
unwhich &amp;lt;- function(x, n) {
  # Create a vector of all FALSE with length equal to x
  out &amp;lt;- rep_len(x = FALSE, length.out = n)
  # Select elements in &amp;quot;out&amp;quot; and convert them to TRUE
  # Since &amp;quot;x&amp;quot; is a logical index, the only elements in &amp;quot;out&amp;quot;
  # that will be selected are the TRUE values in &amp;quot;x&amp;quot;
  out[x] &amp;lt;- TRUE
  # Now &amp;quot;out&amp;quot; should be identical to &amp;quot;x&amp;quot; in terms of TRUE and FALSE
  out
}
# Reverse x from integer to Boolean
unwhich(x = x, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read the documentation to learn more about &lt;code&gt;which()&lt;/code&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-of-the-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of the %% operator&lt;/h2&gt;
&lt;p&gt;Create two logical vectors and their integer equivalents. &lt;strong&gt;Note&lt;/strong&gt;: %% indicates x mod y (“x modulo y”). The result of the %% operator is the REMAINDER of a division, Eg. 75 %% 4 = 18 Remainder 3. If the dividend is lower than the divisor, then R returns the same dividend value: Eg. 4 %% 75 = 4.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logical vector 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Example 1
1:10 %% 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 1 0 1 0 1 0 1 0 1 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Logical 1
x1 &amp;lt;- 1:10 %% 2 == 0
x1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer equivalent
x2 &amp;lt;- which(x = x1)
x2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  2  4  6  8 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Logical vector 2&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Logical 2
y1 &amp;lt;- 1:10 %% 5 == 0
y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer equivalent
y2 &amp;lt;- which(x = y1)
y2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  5 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Intersection of “x” and “y”. For the logical vectors, we wish to find the indices where both x[i] and y[i] are TRUE; for the integer vectors, we wish to find the indices where the values x[i] and y[i] are equal.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X &amp;amp; Y &amp;lt;-&amp;gt; intersect(x, y)
# Logical
x1 &amp;amp; y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
intersect(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Union of “x” and “y”. For the logical vectors, we wish to find the indices where either x[i] or y[i] or both are TRUE; for the integer vectors, we wish to find all values in x and y.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X | Y &amp;lt;-&amp;gt; union(x, y)
# Logical
x1 | y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
union(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  2  4  6  8 10  5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Set difference. For the logical, we wish to find values that are &lt;em&gt;in x1 but not in y1&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# X &amp;amp; !Y &amp;lt;-&amp;gt; setdiff(x, y)
# Logical
x1 &amp;amp; !y1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
setdiff(x2, y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 6 8&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The function &lt;code&gt;xor()&lt;/code&gt; indicates element-wise exclusive OR.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import image
knitr::include_graphics(&amp;quot;Exclusive Or.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Exclusive%20Or.png&#34; width=&#34;40%&#34; height=&#34;40%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# xor(X, Y) &amp;lt;-&amp;gt; setdiff(union(x, y), intersect(x, y))
# Logical
xor(x1, y1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Integer
setdiff(union(x2, y2), intersect(x2, y2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 4 6 8 5&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;rondomly-permute-a-data-frame-a-technique-often-used-in-random-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rondomly permute a data frame (a technique often used in random forests)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Randomly permute the columns and rows of a data frame
mtcars[
  sample(x = 1:nrow(mtcars), replace = FALSE),
  colnames(mtcars)[sample(x = 1:length(colnames(mtcars)))]
]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     qsec cyl  hp    wt  disp am  mpg carb gear vs drat
Datsun 710          18.61   4  93 2.320 108.0  1 22.8    1    4  1 3.85
Merc 450SL          17.60   8 180 3.730 275.8  0 17.3    3    3  0 3.07
Toyota Corona       20.01   4  97 2.465 120.1  0 21.5    1    3  1 3.70
Camaro Z28          15.41   8 245 3.840 350.0  0 13.3    4    3  0 3.73
Merc 230            22.90   4  95 3.150 140.8  0 22.8    2    4  1 3.92
Ferrari Dino        15.50   6 175 2.770 145.0  1 19.7    6    5  0 3.62
Dodge Challenger    16.87   8 150 3.520 318.0  0 15.5    2    3  0 2.76
Merc 240D           20.00   4  62 3.190 146.7  0 24.4    2    4  1 3.69
Maserati Bora       14.60   8 335 3.570 301.0  1 15.0    8    5  0 3.54
Cadillac Fleetwood  17.98   8 205 5.250 472.0  0 10.4    4    3  0 2.93
Lotus Europa        16.90   4 113 1.513  95.1  1 30.4    2    5  1 3.77
Mazda RX4 Wag       17.02   6 110 2.875 160.0  1 21.0    4    4  0 3.90
Merc 450SE          17.40   8 180 4.070 275.8  0 16.4    3    3  0 3.07
Pontiac Firebird    17.05   8 175 3.845 400.0  0 19.2    2    3  0 3.08
Merc 280            18.30   6 123 3.440 167.6  0 19.2    4    4  1 3.92
Merc 450SLC         18.00   8 180 3.780 275.8  0 15.2    3    3  0 3.07
Fiat 128            19.47   4  66 2.200  78.7  1 32.4    1    4  1 4.08
Honda Civic         18.52   4  52 1.615  75.7  1 30.4    2    4  1 4.93
Merc 280C           18.90   6 123 3.440 167.6  0 17.8    4    4  1 3.92
Porsche 914-2       16.70   4  91 2.140 120.3  1 26.0    2    5  0 4.43
Duster 360          15.84   8 245 3.570 360.0  0 14.3    4    3  0 3.21
Hornet Sportabout   17.02   8 175 3.440 360.0  0 18.7    2    3  0 3.15
Valiant             20.22   6 105 3.460 225.0  0 18.1    1    3  1 2.76
Volvo 142E          18.60   4 109 2.780 121.0  1 21.4    2    4  1 4.11
Chrysler Imperial   17.42   8 230 5.345 440.0  0 14.7    4    3  0 3.23
Mazda RX4           16.46   6 110 2.620 160.0  1 21.0    4    4  0 3.90
Lincoln Continental 17.82   8 215 5.424 460.0  0 10.4    4    3  0 3.00
Hornet 4 Drive      19.44   6 110 3.215 258.0  0 21.4    1    3  1 3.08
AMC Javelin         17.30   8 150 3.435 304.0  0 15.2    2    3  0 3.15
Ford Pantera L      14.50   8 264 3.170 351.0  1 15.8    4    5  0 4.22
Fiat X1-9           18.90   4  66 1.935  79.0  1 27.3    1    4  1 4.08
Toyota Corolla      19.90   4  65 1.835  71.1  1 33.9    1    4  1 4.22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Second way using ncol() instead of colnames()
# Integer subsetting instead of character
mtcars[sample(x = nrow(mtcars)), sample(x = ncol(mtcars))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       wt gear vs  qsec  mpg am  hp cyl carb  disp drat
Merc 450SE          4.070    3  0 17.40 16.4  0 180   8    3 275.8 3.07
AMC Javelin         3.435    3  0 17.30 15.2  0 150   8    2 304.0 3.15
Lotus Europa        1.513    5  1 16.90 30.4  1 113   4    2  95.1 3.77
Maserati Bora       3.570    5  0 14.60 15.0  1 335   8    8 301.0 3.54
Fiat 128            2.200    4  1 19.47 32.4  1  66   4    1  78.7 4.08
Mazda RX4           2.620    4  0 16.46 21.0  1 110   6    4 160.0 3.90
Chrysler Imperial   5.345    3  0 17.42 14.7  0 230   8    4 440.0 3.23
Porsche 914-2       2.140    5  0 16.70 26.0  1  91   4    2 120.3 4.43
Volvo 142E          2.780    4  1 18.60 21.4  1 109   4    2 121.0 4.11
Merc 280C           3.440    4  1 18.90 17.8  0 123   6    4 167.6 3.92
Lincoln Continental 5.424    3  0 17.82 10.4  0 215   8    4 460.0 3.00
Mazda RX4 Wag       2.875    4  0 17.02 21.0  1 110   6    4 160.0 3.90
Merc 230            3.150    4  1 22.90 22.8  0  95   4    2 140.8 3.92
Fiat X1-9           1.935    4  1 18.90 27.3  1  66   4    1  79.0 4.08
Merc 240D           3.190    4  1 20.00 24.4  0  62   4    2 146.7 3.69
Toyota Corolla      1.835    4  1 19.90 33.9  1  65   4    1  71.1 4.22
Ford Pantera L      3.170    5  0 14.50 15.8  1 264   8    4 351.0 4.22
Honda Civic         1.615    4  1 18.52 30.4  1  52   4    2  75.7 4.93
Valiant             3.460    3  1 20.22 18.1  0 105   6    1 225.0 2.76
Hornet 4 Drive      3.215    3  1 19.44 21.4  0 110   6    1 258.0 3.08
Dodge Challenger    3.520    3  0 16.87 15.5  0 150   8    2 318.0 2.76
Ferrari Dino        2.770    5  0 15.50 19.7  1 175   6    6 145.0 3.62
Merc 450SL          3.730    3  0 17.60 17.3  0 180   8    3 275.8 3.07
Merc 450SLC         3.780    3  0 18.00 15.2  0 180   8    3 275.8 3.07
Camaro Z28          3.840    3  0 15.41 13.3  0 245   8    4 350.0 3.73
Pontiac Firebird    3.845    3  0 17.05 19.2  0 175   8    2 400.0 3.08
Toyota Corona       2.465    3  1 20.01 21.5  0  97   4    1 120.1 3.70
Datsun 710          2.320    4  1 18.61 22.8  1  93   4    1 108.0 3.85
Cadillac Fleetwood  5.250    3  0 17.98 10.4  0 205   8    4 472.0 2.93
Duster 360          3.570    3  0 15.84 14.3  0 245   8    4 360.0 3.21
Hornet Sportabout   3.440    3  0 17.02 18.7  0 175   8    2 360.0 3.15
Merc 280            3.440    4  1 18.30 19.2  0 123   6    4 167.6 3.92&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-a-random-sample-of-m-rows-from-a-data-frame&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selecting a random sample of m rows from a data frame&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that randomly selects m rows from a data frame
select_m_rows &amp;lt;- function(data, m) {

  # Warning
  if (m &amp;gt; nrow(data)) {
    abort(&amp;quot;Not enough rows in data frame&amp;quot;)
  }

  # Select rows randomly and include all columns
  data[sample(x = 1:nrow(data), size = m), , drop = FALSE]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action using the iris data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_m_rows(data = iris, m = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
149          6.2         3.4          5.4         2.3  virginica
87           6.7         3.1          4.7         1.5 versicolor
116          6.4         3.2          5.3         2.3  virginica
82           5.5         2.4          3.7         1.0 versicolor
8            5.0         3.4          1.5         0.2     setosa
81           5.5         2.4          3.8         1.1 versicolor
112          6.4         2.7          5.3         1.9  virginica
79           6.0         2.9          4.5         1.5 versicolor
43           4.4         3.2          1.3         0.2     setosa
75           6.4         2.9          4.3         1.3 versicolor&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;What if we need the first and last rows selected, but everything in between can be random?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extend the function to ensure that the first and last rows are selected
# Everything in between are random
select_m_rows_extended &amp;lt;- function(data, m) {

  # Warning
  if (m &amp;gt; nrow(data)) {
    abort(&amp;quot;Not enough rows in data frame&amp;quot;)
  }

  # Select first row and last row
  # &amp;quot;Sandwich&amp;quot; the sample() vector in between
  data[
    c(
      1,
      sample(x = 2:(nrow(data) - 1), size = (m - 2)),
      nrow(data)
    ), ,
    drop = FALSE
  ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see it in action using the mtcars data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_m_rows_extended(data = mtcars, m = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;               mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Merc 450SLC   15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Merc 450SL    17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
Datsun 710    22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
Toyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
AMC Javelin   15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
Volvo 142E    21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Finally, what if we wish to randomly select a blocked sample, i.e., the rows have to be contiguous (an initial row, a final row, and everything in between)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Successive lines together as a blocked sample
m &amp;lt;- 10
# The starting row cannot be less than m rows from the last row of the data
# Or else there wound not be enough rows to select m successive rows from
start &amp;lt;- sample(x = 1:(nrow(mtcars) - m + 1), size = 1)
# The ending row must be m rows from the starting row
end &amp;lt;- start + m - 1
# Select the consecutive rows between random starting row
mtcars[start:end, , drop = FALSE]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Merc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
Merc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Merc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Merc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-the-columns-in-a-data-frame-alphabetically&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordering the columns in a data frame alphabetically&lt;/h2&gt;
&lt;p&gt;This can easily be done using R’s subsetting operators:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A function that orders the columns of data frame alphabetically
order_columns &amp;lt;- function(data) {

  # Select columns according to the indices generated by order()
  # We could also use sort()
  data[, order(x = names(data))]
}
# Test
as_tibble(order_columns(data = mtcars))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 32 × 11
      am  carb   cyl  disp  drat  gear    hp   mpg  qsec    vs    wt
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1     1     4     6  160   3.9      4   110  21    16.5     0  2.62
 2     1     4     6  160   3.9      4   110  21    17.0     0  2.88
 3     1     1     4  108   3.85     4    93  22.8  18.6     1  2.32
 4     0     1     6  258   3.08     3   110  21.4  19.4     1  3.22
 5     0     2     8  360   3.15     3   175  18.7  17.0     0  3.44
 6     0     1     6  225   2.76     3   105  18.1  20.2     1  3.46
 7     0     4     8  360   3.21     3   245  14.3  15.8     0  3.57
 8     0     2     4  147.  3.69     4    62  24.4  20       1  3.19
 9     0     2     4  141.  3.92     4    95  22.8  22.9     1  3.15
10     0     4     6  168.  3.92     4   123  19.2  18.3     1  3.44
# … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(order_columns(data = iris))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 150 × 5
   Petal.Length Petal.Width Sepal.Length Sepal.Width Species
          &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  
 1          1.4         0.2          5.1         3.5 setosa 
 2          1.4         0.2          4.9         3   setosa 
 3          1.3         0.2          4.7         3.2 setosa 
 4          1.5         0.2          4.6         3.1 setosa 
 5          1.4         0.2          5           3.6 setosa 
 6          1.7         0.4          5.4         3.9 setosa 
 7          1.4         0.3          4.6         3.4 setosa 
 8          1.5         0.2          5           3.4 setosa 
 9          1.4         0.2          4.4         2.9 setosa 
10          1.5         0.1          4.9         3.1 setosa 
# … with 140 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(order_columns(data = USArrests))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 50 × 4
   Assault Murder  Rape UrbanPop
     &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
 1     236   13.2  21.2       58
 2     263   10    44.5       48
 3     294    8.1  31         80
 4     190    8.8  19.5       50
 5     276    9    40.6       91
 6     204    7.9  38.7       78
 7     110    3.3  11.1       77
 8     238    5.9  15.8       72
 9     335   15.4  31.9       80
10     211   17.4  25.8       60
# … with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       That is it with R’s subsetting operators. Combined with other data wrangling tools from the &lt;code&gt;tidyverse&lt;/code&gt; packages, R’s subsetting operations can be powerful as far as data analysis tasks are concerned. Next up in R programming, I will write about the the &lt;code&gt;tidyverse&lt;/code&gt;’s functional programming tool— &lt;code&gt;purrr&lt;/code&gt;— which I have been using here and there in many of my posts. Having an understanding of R &lt;code&gt;functionals&lt;/code&gt; have helped me tremendously in my day-to-day use of R, and so I look forward to documenting my learning process via a post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
