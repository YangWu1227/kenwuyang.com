<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Economics | Yang (Ken) Wu</title>
    <link>https://www.kenwuyang.com/en/tag/economics/</link>
      <atom:link href="https://www.kenwuyang.com/en/tag/economics/index.xml" rel="self" type="application/rss+xml" />
    <description>Economics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Yang Wu</copyright><lastBuildDate>Fri, 05 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.kenwuyang.com/media/Sharing.png</url>
      <title>Economics</title>
      <link>https://www.kenwuyang.com/en/tag/economics/</link>
    </image>
    
    <item>
      <title>Gender Pay Gap in Professional Sports</title>
      <link>https://www.kenwuyang.com/en/project/pay-in-the-wnba/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/project/pay-in-the-wnba/</guid>
      <description>&lt;p&gt;“To put it really simple: If you don’t respect women’s basketball, you’re a joke. You’re a joke, man.”&lt;/p&gt;
&lt;p&gt;                                                                                         — Isiah Thomas, Former NBA All-Star&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;“WNBA Players Should Stop Complaining. If Anything, They’re Overpaid”&lt;/p&gt;
&lt;p&gt;                                                                      — Tho Bishop, Assistant Editor for the Mises Wire&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;       The quotes above seem to embody the sentiments that typify the social discourse surrounding the issue of equity of pay in sports. A major drawback in today’s discussions pertaining to the issue of fair pay is that it is often scrutinized as a political issue. &lt;em&gt;It is&lt;/em&gt;. But framing the issue as entirely political omits the fact that it is also economic.&lt;/p&gt;
&lt;p&gt;       While the arms of normative economics wrestle with the question of fairness, positive economics is concerned with the description, quantification and explanation of economic issues. The goal of this paper is to add to the current discourse regarding fair pay by examining the talent market of the Women’s National Basketball Association as a case study. Framing the issue of equitable compensation as a study of economic efficiency, I focus my analysis primarily on entertaining the “what is” question. Is there an inefficiency in the labor market for the WNBA players in terms of player pay?&lt;/p&gt;
&lt;p&gt;       The finished paper can be found &lt;a href=&#34;https://www.kenwuyang.com/project/Pay%20in%20the%20WNBA/Yang_Wu_Writing_Sample.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. And, for those interested, here&amp;rsquo;s the &lt;a href=&#34;https://www.kenwuyang.com/project/Pay%20in%20the%20WNBA/yang_wu_WNBA_data.xlsx&#34; target=&#34;_blank&#34;&gt;data&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling with Economics Data in R</title>
      <link>https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/data-wrangling-with-economics-data/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-importation&#34;&gt;Data Importation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrangling&#34;&gt;Wrangling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#merging-data-sets&#34;&gt;Merging Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nested-data&#34;&gt;Nested Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#is-data-deletion-best-practice&#34;&gt;Is data deletion best practice?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In this post, I experiment with some data wrangling techniques. I would be using the &lt;a href=&#34;https://www.rug.nl/ggdc/productivity/pwt/?lang=en&#34;&gt;Penn World Table&lt;/a&gt; version 9.1 (PWT 9.1) and the &lt;a href=&#34;https://www.cntsdata.com/&#34;&gt;Cross National Time Series&lt;/a&gt; (CNTS) data set to practice these techniques.&lt;/p&gt;
&lt;div id=&#34;data-importation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Importation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- read_csv(&amp;quot;pwt91.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 12376 Columns: 52&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;── Column specification ────────────────────────────────────────────────────────
Delimiter: &amp;quot;,&amp;quot;
chr  (8): countrycode, country, currency_unit, i_cig, i_xm, i_xr, i_outlier,...
dbl (44): year, rgdpe, rgdpo, pop, emp, avh, hc, ccon, cda, cgdpe, cgdpo, cn...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrangling&lt;/h2&gt;
&lt;p&gt;       We first need to “mutate” some variables. The total employment numbers do not offer as much context as the employment–population ratios; we create a new variable by dividing total employment by population. Next, we “select” the variables that are of interest to us. Suppose we wish to keep the following variables: country-code, country name, year, employment–population ratios:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  mutate(emp_pop_ratio = emp / pop) %&amp;gt;%
  select(countrycode, country, year, emp_pop_ratio)
pwt91&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 12,376 × 4
   countrycode country  year emp_pop_ratio
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1 ABW         Aruba    1950            NA
 2 ABW         Aruba    1951            NA
 3 ABW         Aruba    1952            NA
 4 ABW         Aruba    1953            NA
 5 ABW         Aruba    1954            NA
 6 ABW         Aruba    1955            NA
 7 ABW         Aruba    1956            NA
 8 ABW         Aruba    1957            NA
 9 ABW         Aruba    1958            NA
10 ABW         Aruba    1959            NA
# … with 12,366 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s do some simple counting to get a good sense of our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(pwt91$country)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;182&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of years covered in this data set
cat(length(unique(pwt91$year)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;68&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make sure that country and country-code are consistent
cat(length(unique(pwt91$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;182&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First and last year
cat(min(pwt91$year))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1950&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(max(pwt91$year))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2017&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       Notice that there are lots of NA’s, i.e., for some combination of country and year, there are no data. More succinctly put, the further back we go in time, the more we observe missing values for countries. Also, I expect that some countries would have more missing values than others. Before we trim the sample down, we want to examine the number missing values by year and by country to see if this is indeed a widespread trend:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(year) %&amp;gt;%
  count(is.na(emp_pop_ratio))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 136 × 3
# Groups:   year [68]
    year `is.na(emp_pop_ratio)`     n
   &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;                  &amp;lt;int&amp;gt;
 1  1950 FALSE                     50
 2  1950 TRUE                     132
 3  1951 FALSE                     54
 4  1951 TRUE                     128
 5  1952 FALSE                     55
 6  1952 TRUE                     127
 7  1953 FALSE                     57
 8  1953 TRUE                     125
 9  1954 FALSE                     61
10  1954 TRUE                     121
# … with 126 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       After examining the table above, I find that the count confirms my thinking. &lt;code&gt;True&lt;/code&gt; indicates the count for missing values, and &lt;code&gt;False&lt;/code&gt; indicates the opposite. The first two decades over this period of 68 years (1950-2017) have many countries with missing values— more than 100 countries in our sample of 182 have missing values. As we go into the 1990’s, the number of countries with missing values drastically decreases— less than 10 countries have missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(country) %&amp;gt;%
  count(is.na(emp_pop_ratio))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 312 × 3
# Groups:   country [182]
   country             `is.na(emp_pop_ratio)`     n
   &amp;lt;chr&amp;gt;               &amp;lt;lgl&amp;gt;                  &amp;lt;int&amp;gt;
 1 Albania             FALSE                     48
 2 Albania             TRUE                      20
 3 Algeria             FALSE                     58
 4 Algeria             TRUE                      10
 5 Angola              FALSE                     48
 6 Angola              TRUE                      20
 7 Anguilla            FALSE                     29
 8 Anguilla            TRUE                      39
 9 Antigua and Barbuda FALSE                      9
10 Antigua and Barbuda TRUE                      59
# … with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       OECD members such as the US, UK, Australia, Canada, France, Finland, Germany, etc., have no missing values over this period. Sub-Saharan African countries, in particular, have relatively more missing values. Then, some Latin American countries have even more missing values. This is to be expected with socio-economic data; we confirm that the sampling quality introduces some region bias. Deleting missing observations can result in biased parameters and estimates and reduce the statistical power of the analyses. However, in this case, I would be using list-wise deletion, where all observations that have missing values are deleted. This means that, if I were to continue on with my analysis, the models would only be trained on data from countries that have a more complete set of data. While there may be better ways to handle biased samples, for this activity, I would simply use the year 1990 as a cut off since a reasonable number of countries would have values from 1990 to 2017:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  filter(year &amp;gt;= 1990) %&amp;gt;%
  na.exclude()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We count the number of rows for each country to see which countries do not have all 28 years (1990-2017) worth of data, and we drop those countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 &amp;lt;- pwt91 %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  filter(n() == 28)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a sanity check to make sure we are on track:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91 %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 169 × 2
# Groups:   countrycode [169]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 ARM            28
 6 AUS            28
 7 AUT            28
 8 AZE            28
 9 BDI            28
10 BEL            28
# … with 159 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(pwt91$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen, all remaining countries have values for the 28-year period; we now have a sample of 169 countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt91&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4,732 × 4
# Groups:   countrycode [169]
   countrycode country  year emp_pop_ratio
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454
 2 AGO         Angola   1991         0.451
 3 AGO         Angola   1992         0.449
 4 AGO         Angola   1993         0.448
 5 AGO         Angola   1994         0.445
 6 AGO         Angola   1995         0.442
 7 AGO         Angola   1996         0.439
 8 AGO         Angola   1997         0.438
 9 AGO         Angola   1998         0.437
10 AGO         Angola   1999         0.437
# … with 4,722 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;merging-data-sets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Merging Data Sets&lt;/h2&gt;
&lt;p&gt;       Suppose after we completed our analysis, we found another data set containing some other relevant variables. We would like to “join” the two data sets by a common variable. In my case, that common variable is “country-code.” We start by loading in the second data set, which I’ve converted to a .csv file for convenience:&lt;/p&gt;
&lt;p&gt;       First we need to make sure that the “key” variables share the same names across these two data sets. Then, we “select” the variables of interest to us. The &lt;code&gt;Domestic8&lt;/code&gt; variable is the number of anti-government demonstrations, which, according to the &lt;a href=&#34;https://www.cntsdata.com/&#34;&gt;CNTS&lt;/a&gt; user manual, includes labor strikes and demonstrations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cnts19 &amp;lt;- cnts19 %&amp;gt;%
  rename(countrycode = Wbcode) %&amp;gt;%
  select(countrycode, year, domestic8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       We would want to use &lt;code&gt;left_join()&lt;/code&gt;, because it preserves the original observations even when there isn’t a match. Notice that setting the argument (by = NULL) makes sure that R uses all variables that appear in both data sets for merging, this is the so-called &lt;strong&gt;natural join&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data &amp;lt;- pwt91 %&amp;gt;%
  left_join(cnts19, by = NULL) %&amp;gt;%
  na.exclude()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Joining, by = c(&amp;quot;countrycode&amp;quot;, &amp;quot;year&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4,455 × 5
# Groups:   countrycode [162]
   countrycode country  year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454         0
 2 AGO         Angola   1991         0.451         0
 3 AGO         Angola   1992         0.449         0
 4 AGO         Angola   1993         0.448         0
 5 AGO         Angola   1994         0.445         0
 6 AGO         Angola   1995         0.442         0
 7 AGO         Angola   1996         0.439         0
 8 AGO         Angola   1997         0.438         0
 9 AGO         Angola   1998         0.437         0
10 AGO         Angola   1999         0.437         0
# … with 4,445 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 162 × 2
# Groups:   countrycode [162]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 ARM            26
 6 AUS            28
 7 AUT            28
 8 AZE            26
 9 BDI            28
10 BEL            28
# … with 152 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       According to the table above, after merging, we now have some countries with missing values for the domestic8 variable. Since this exercise is about data wrangling techniques and not about data analysis, we will continue to trim the sample further for practice, excluding countries that have missing values. We need to keep in mind that, as far as analysis is concerned, too much deletion of the data would lead to biases. Just how much bias are we able to tolerate is a whole new topic of discussion. For now, we will proceed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data &amp;lt;- new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  filter(n() == 28)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  group_by(countrycode) %&amp;gt;%
  count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 139 × 2
# Groups:   countrycode [139]
   countrycode     n
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 AGO            28
 2 ALB            28
 3 ARE            28
 4 ARG            28
 5 AUS            28
 6 AUT            28
 7 BDI            28
 8 BEL            28
 9 BEN            28
10 BFA            28
# … with 129 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have ensured that all remaining countries have values for all variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of countries
cat(length(unique(new_data$countrycode)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;139&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3,892 × 5
# Groups:   countrycode [139]
   countrycode country  year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 AGO         Angola   1990         0.454         0
 2 AGO         Angola   1991         0.451         0
 3 AGO         Angola   1992         0.449         0
 4 AGO         Angola   1993         0.448         0
 5 AGO         Angola   1994         0.445         0
 6 AGO         Angola   1995         0.442         0
 7 AGO         Angola   1996         0.439         0
 8 AGO         Angola   1997         0.438         0
 9 AGO         Angola   1998         0.437         0
10 AGO         Angola   1999         0.437         0
# … with 3,882 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s glimpse at which country had the single most yearly anti-government demonstrations over this 28-year period:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  arrange(desc(domestic8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3,892 × 5
# Groups:   countrycode [139]
   countrycode country               year emp_pop_ratio domestic8
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 IND         India                 2016         0.401       149
 2 IND         India                 2017         0.402       146
 3 IND         India                 2015         0.399       110
 4 USA         United States         2015         0.470        81
 5 SYR         Syrian Arab Republic  2011         0.237        74
 6 PAK         Pakistan              2016         0.307        55
 7 YEM         Yemen                 2011         0.185        55
 8 USA         United States         2014         0.466        50
 9 USA         United States         2011         0.457        49
10 USA         United States         2016         0.474        49
# … with 3,882 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nested-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nested Data&lt;/h2&gt;
&lt;p&gt;       Lastly, we may also present our new merged data as a nested data frame, a new structure. The nested data frame has one row per group (per country-code/country in my case). The third column, data, is a list of data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data &amp;lt;- new_data %&amp;gt;%
  group_by(countrycode, country) %&amp;gt;%
  nest()
nested_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 139 × 3
# Groups:   countrycode, country [139]
   countrycode country              data             
   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                &amp;lt;list&amp;gt;           
 1 AGO         Angola               &amp;lt;tibble [28 × 3]&amp;gt;
 2 ALB         Albania              &amp;lt;tibble [28 × 3]&amp;gt;
 3 ARE         United Arab Emirates &amp;lt;tibble [28 × 3]&amp;gt;
 4 ARG         Argentina            &amp;lt;tibble [28 × 3]&amp;gt;
 5 AUS         Australia            &amp;lt;tibble [28 × 3]&amp;gt;
 6 AUT         Austria              &amp;lt;tibble [28 × 3]&amp;gt;
 7 BDI         Burundi              &amp;lt;tibble [28 × 3]&amp;gt;
 8 BEL         Belgium              &amp;lt;tibble [28 × 3]&amp;gt;
 9 BEN         Benin                &amp;lt;tibble [28 × 3]&amp;gt;
10 BFA         Burkina Faso         &amp;lt;tibble [28 × 3]&amp;gt;
# … with 129 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us examine the structure of the object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(nested_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 139
Columns: 3
Groups: countrycode, country [139]
$ countrycode &amp;lt;chr&amp;gt; &amp;quot;AGO&amp;quot;, &amp;quot;ALB&amp;quot;, &amp;quot;ARE&amp;quot;, &amp;quot;ARG&amp;quot;, &amp;quot;AUS&amp;quot;, &amp;quot;AUT&amp;quot;, &amp;quot;BDI&amp;quot;, &amp;quot;BEL&amp;quot;, &amp;quot;B…
$ country     &amp;lt;chr&amp;gt; &amp;quot;Angola&amp;quot;, &amp;quot;Albania&amp;quot;, &amp;quot;United Arab Emirates&amp;quot;, &amp;quot;Argentina&amp;quot;, …
$ data        &amp;lt;list&amp;gt; [&amp;lt;tbl_df[28 x 3]&amp;gt;], [&amp;lt;tbl_df[28 x 3]&amp;gt;], [&amp;lt;tbl_df[28 x 3]&amp;gt;…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The row numbers make sense, since we have indeed a sample of 139 countries. If we look at the first element of “data,” we see that it contains all the data for that country (in my case, Angola):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data$data[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 28 × 3
    year emp_pop_ratio domestic8
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1  1990         0.454         0
 2  1991         0.451         0
 3  1992         0.449         0
 4  1993         0.448         0
 5  1994         0.445         0
 6  1995         0.442         0
 7  1996         0.439         0
 8  1997         0.438         0
 9  1998         0.437         0
10  1999         0.437         0
# … with 18 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fifth element:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data$data[[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 28 × 3
    year emp_pop_ratio domestic8
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1  1990         0.463         0
 2  1991         0.446         0
 3  1992         0.439         0
 4  1993         0.436         0
 5  1994         0.445         0
 6  1995         0.457         0
 7  1996         0.457         1
 8  1997         0.456         0
 9  1998         0.459         0
10  1999         0.462         0
# … with 18 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;       In other words, there is one data frame per country. Presenting data in this new structure can be helpful, especially when dealing with cross-sectional data where observational units are numerous and we often need to conduct transformations and fit models using only subsets of the entire data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-data-deletion-best-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is data deletion best practice?&lt;/h2&gt;
&lt;p&gt;       At the end of the post, it may be worth writing about data deletion. We began with a sample of 182 countries, which we trimmed down to 139. Judging by numbers only, the deletion process could have been worse. However, we must take note that, when it comes to missing values, the data are not missing completely at random (MCAR). This is especially true for cross-national, socio-economic data, where sampling quality reflects inequalities that are rather hard to capture and deal with. Beyond this activity, it would be interesting to explore ways of handling missing data as well as new imputation methods that have been developed over the years.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
