<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Visualization | Yang (Ken) Wu</title>
    <link>https://www.kenwuyang.com/en/tag/data-visualization/</link>
      <atom:link href="https://www.kenwuyang.com/en/tag/data-visualization/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Visualization</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Yang Wu</copyright><lastBuildDate>Tue, 07 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.kenwuyang.com/media/Sharing.png</url>
      <title>Data Visualization</title>
      <link>https://www.kenwuyang.com/en/tag/data-visualization/</link>
    </image>
    
    <item>
      <title>Visualizing My Life In Months</title>
      <link>https://www.kenwuyang.com/en/post/my-life-in-months/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/my-life-in-months/</guid>
      <description>&lt;h3 id=&#34;how-much-time-do-i-have-left&#34;&gt;How much time do I have left?&lt;/h3&gt;
&lt;img src=&#34;my_life_in_months.png&#34; width=&#34;500&#34; height=&#34;250&#34;&gt;
&lt;p&gt;Inspired by &lt;a href=&#34;https://waitbutwhy.com/2014/05/life-weeks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tim Urban&lt;/a&gt; and &lt;a href=&#34;https://github.com/isabellabenabaye/life-chart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Isabella Benabaye&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Estimating Probabilities of Asset Returns in R</title>
      <link>https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-problem&#34;&gt;The problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#not-theoretical-but-empirical&#34;&gt;Not theoretical, but empirical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-empirical-cumulative-distribution-function&#34;&gt;The Empirical Cumulative Distribution Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-probabilities-in-r&#34;&gt;Estimating probabilities in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-words-of-caution&#34;&gt;Some words of caution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;       In a previous &lt;a href=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/&#34;&gt;post&lt;/a&gt;, we covered some techniques for visualizing financial data— specifically, asset returns— using various types of visualization and R graphics libraries. We explored line charts, histograms, and density plots, which are all very useful ways to visualize asset returns. Nevertheless, we were left with a question— &lt;em&gt;how could one estimate the probabilities of these asset returns?&lt;/em&gt; We are interested in this question since stakeholders may need more information than what visualizations could provide. Simply put, stakeholders may need us to provide some ways to ascertain uncertainty, as difficult as this may seem, and we must answer the call.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;       The last time we examined financial data, we focused on a class of assets called the Exchange Traded Funds. We took a sample of daily prices from 2012-12-31 to 2021-7-31 and converted them to monthly log returns. This ultimately left us with a sample of 103 monthly log returns. The last section of my &lt;a href=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/&#34;&gt;post&lt;/a&gt; on asset visualization ended with this following visualization:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;densities.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       We also established that &lt;strong&gt;the heights of the curve (the y-axis) do not represent probabilities.&lt;/strong&gt; To convert to an actual probability, we need to find the area under the curve for a specific interval of returns. One way of accomplishing this task is to fit some type of probability distribution to our data, and the options are plentiful. The difficulty, however, is that data from the world of finance are often messy and none of these options are ever consistently adequate. The problem, as &lt;a href=&#34;https://www.investopedia.com/contributors/46/&#34;&gt;David Harper&lt;/a&gt; once described in a blog post, is precisely this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finance, a social science, is not as clean as physical sciences. Gravity, for example, has an elegant formula that we can depend on, time and again. Financial asset returns, on the other hand, cannot be replicated so consistently. A staggering amount of money has been lost over the years by clever people who confused the accurate distributions (i.e., as if derived from physical sciences) with the messy, unreliable approximations that try to depict financial returns. In finance, probability distributions are little more than crude pictorial representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;       I quote David’s blog post here simply to emphasize that it is generally difficult to capture uncertainty accurately and consistently in finance. But this is not a reason to give up, and, in my humble opinion, we certainly should not disregard all the mathematical and statistical models in &lt;em&gt;toto&lt;/em&gt;. Why? Perhaps, it is because having some information is usually more helpful than having no information. As the aphorism goes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All models are wrong, but some are useful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In light of this spirit, we will try to tackle the problem set forth in my last post to the best of our abilities.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;not-theoretical-but-empirical&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Not theoretical, but empirical&lt;/h2&gt;
&lt;p&gt;       A common practice is to use the normal distribution to approximate returns data, but empirical evidence often suggest sub-optimal goodness-of-fit due to skewness and excess kurtosis. Examine the skewness measures and excess kurtosis of our sample of returns data:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skewness&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Skewness
asset_returns_tq %&amp;gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::skewness,
    method = &amp;quot;moment&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       SPY        EFA        DIA        QQQ        AGG 
-0.7011322 -0.5213137 -0.7557169 -0.2351347 -0.0665614 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Excess Kurtosis&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Excess Kurtosis
asset_returns_tq %&amp;gt;%
  # Apply the function from the PerformanceAnalytics package to each ETF
  purrr::map_dbl(
    .x = .,
    .f = PerformanceAnalytics::kurtosis,
    method = &amp;quot;excess&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      SPY       EFA       DIA       QQQ       AGG 
2.1505474 2.1227652 2.3110488 0.3138191 0.3451339 &lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;       In addition, take a look at the results for a test of normality:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-6&#34;&gt;Table 1: &lt;/span&gt;Anderson-Darling Test of Normality
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
SPY
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
EFA
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
DIA
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
QQQ
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
AGG
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3e-04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0886
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7e-04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.129
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.6993
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Controlling the Type I error rate at &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1\)&lt;/span&gt;, only the returns from the Invesco QQQ and iShares Core U.S. Aggregate Bond ETF’s could be considered normally distributed for this particular sample. All of the empirical evidence above suggest that, in practice, the normal distribution may not be a good fit for asset returns. Further, violation of the assumptions that the monthly returns are independently and identically distributed are frequent. Examine the shapes of the densities above, are there any good reasons to believe that these shapes are constant across time and that each random draw of monthly return will come from these &lt;em&gt;exact&lt;/em&gt; distributions? Plus, we usually cannot argue &lt;strong&gt;a priori&lt;/strong&gt; that monthly asset returns are independent.&lt;/p&gt;
&lt;p&gt;       Various other theoretical distributions such as the log-normal distribution, beta distribution, t-distribution have been used as alternatives. To the extent that these theoretical distributions work well with specific random samples of returns data, it may be helpful to employ them in our estimations of probabilities. Nevertheless, we may choose a normal or a log-normal distribution and discover later on that it misled us on the likelihood of left-tail losses. Sometimes we employ a skewed distribution that fits the sample very well only to have the data in the next period prove us wrong. Consider the returns series for the SPDR Dow Jones Industrial Average ETF. Let us plot the skewness-kurtosis graph:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;summary statistics
------
min:  -0.1464204   max:  0.1147059 
median:  0.01368791 
mean:  0.0113744 
estimated sd:  0.0398583 
estimated skewness:  -0.7669313 
estimated kurtosis:  5.487882 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examine the blue dot (labeled &lt;strong&gt;observation&lt;/strong&gt;) in the plot above, does it look like our sample of returns is anywhere close to any of the theoretical distributions? The orange dots, which represent bootstrapped samples of the DIA returns series, are scattered all over the place; we simply cannot be certain if any of the theoretical distributions would be a good fit for our data. And we only have one sample in this case!&lt;/p&gt;
&lt;p&gt;       So for all of the reasons above, I opt to approach the task of estimating returns probabilities empirically. This approach entails the use of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Empirical_distribution_function&#34;&gt;empirical distribution function&lt;/a&gt;, commonly referred to as the eCDF. One advantage of this nonparametric approach is that we are depending on our data for estimation. Simply put, an empirical distribution is determined by the sample, whereas a theoretical distribution can only determine samples drawn from it. When the parametric conditions of validity are far from being met, we have to use the data itself to create a cumulative distribution in order to estimate probabilities.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;the-empirical-cumulative-distribution-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Empirical Cumulative Distribution Function&lt;/h2&gt;
&lt;p&gt;       Before we discuss the R implementations, however, a bit of theory on the eCDF and its properties certainly would not hurt. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(x_{1}, \ldots, x_{n}\)&lt;/span&gt; is a batch of observations (the word batch implies no commitment to an i.i.d stochastic model). The empirical cumulative distribution function is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
F_{n}(x)=\frac{1}{n}\left(\text{number of} x_{i} \leq x\right)
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next, we order the batch of observations by &lt;span class=&#34;math inline&#34;&gt;\(x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}\)&lt;/span&gt;, so&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x&amp;lt;x_{(1)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x_{(1)} \leq x&amp;lt;x_{(2)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=\frac{1}{n}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(x_{(k)} \leq x&amp;lt;x_{(k+1)}\)&lt;/span&gt;, the probability is defined as &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)=\frac{k}{n}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there is a single observation with value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; has a jump of height &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;; if there are &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; observations with the same value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; has a jump of height &lt;span class=&#34;math inline&#34;&gt;\(\frac{t}{n}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The eCDF is analogue to the cumulative distribution function of a random variable in a sense— &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; gives the probability that &lt;span class=&#34;math inline&#34;&gt;\(X \leq x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)\)&lt;/span&gt; gives the proportion of observations less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;       In the case where &lt;span class=&#34;math inline&#34;&gt;\(X_{1}, \ldots, X_{n}\)&lt;/span&gt; is a random sample from a continuous distribution function, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, we can express &lt;span class=&#34;math inline&#34;&gt;\(F_{n}\)&lt;/span&gt; as follows:
&lt;span class=&#34;math display&#34;&gt;\[
F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I_{(-\infty, x]}\left(X_{i}\right)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp;amp; \text { if } X_{i} \leq x \\
0, &amp;amp; \text { if } X_{i}&amp;gt;x
\end{array}\right.
\]&lt;/span&gt;
By the definition of CDF, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}\leq x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; and the probability of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}&amp;gt;x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1-F(x)\)&lt;/span&gt;. &lt;strong&gt;Note&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; is the true unknown cdf we intend to estimate. The random variables &lt;span class=&#34;math inline&#34;&gt;\(I_{(-\infty, x]}\left(X_{i}\right)\)&lt;/span&gt; are independent Bernoulli random variables:
&lt;span class=&#34;math display&#34;&gt;\[
I_{(-\infty, x]}\left(X_{i}\right)=\left\{\begin{array}{ll}
1, &amp;amp; \text { with probability } F(x) \\
0, &amp;amp; \text { with probability } 1-F(x)
\end{array}\right.
\]&lt;/span&gt;
Thus, &lt;span class=&#34;math inline&#34;&gt;\(n F_{n}(x)\)&lt;/span&gt; is a binomial random variable with
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E\left[F_{n}(x)\right] &amp;amp;=F(x) \\
\operatorname{Var}\left[F_{n}(x)\right] &amp;amp;=\frac{1}{n} F(x)[1-F(x)]
\end{aligned}
\]&lt;/span&gt;
In other words, &lt;span class=&#34;math inline&#34;&gt;\(F_{n}(x)\)&lt;/span&gt; is an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; and it has a maximum variance at the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(F(x)=.5\)&lt;/span&gt;, which is the empirical median. As &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; becomes very large or very small, the variance tends toward zero.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-probabilities-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating probabilities in R&lt;/h2&gt;
&lt;p&gt;       The &lt;code&gt;stats&lt;/code&gt; package provides a &lt;a href=&#34;https://www.kenwuyang.com/post/function-factories-part-i/&#34;&gt;function factory&lt;/a&gt;, &lt;code&gt;ecdf&lt;/code&gt;, that returns an empirical cumulative distribution function given a vector of observations. Let us create a manufactured function using the returns series of the SPDR Dow Jones Industrial Average ETF. As can be seen from earlier sections, the returns series for this ETF is far from normal, and, for that matter, from any other theoretical distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a function using the function factory
ecdf_DIA &amp;lt;- stats::ecdf(x = asset_returns_tq[[&amp;quot;DIA&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we may plot the eCDF using &lt;code&gt;ggplot2&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
tibble::tibble(
  x = asset_returns_tq[[&amp;quot;DIA&amp;quot;]],
  y = ecdf_DIA(v = asset_returns_tq[[&amp;quot;DIA&amp;quot;]])
) %&amp;gt;% 
  # Plot using geom_step
  ggplot(data = ., mapping = aes(x = x, y = y)) +
  geom_step(color = &amp;quot;orange&amp;quot;) +
  labs(title = &amp;quot;Empirical Cumulative Distribution&amp;quot;,
       x = &amp;quot;Monthly Log Returns&amp;quot;,
       y = latex2exp::TeX(string = &amp;quot;$\\F_{n}(X)$&amp;quot;)) + 
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = &amp;quot;white&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/function-factory-estimating-probabilities-of-returns/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The advantage of using a function factory is that we may now compute probabilities based on the eCDF. For instance, we may wish to know the probability that a monthly log return would fall below &lt;span class=&#34;math inline&#34;&gt;\(0\%\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Probability P(x &amp;lt; 0.0)
ecdf_DIA(v = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2815534&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, there is about a &lt;span class=&#34;math inline&#34;&gt;\(28\%\)&lt;/span&gt; chance that a monthly log return would fall below &lt;span class=&#34;math inline&#34;&gt;\(0\%\)&lt;/span&gt;. The probabilities that monthly log returns would fall within other intervals of values could be computed using the same method. &lt;strong&gt;Note:&lt;/strong&gt; We could never estimate the probability that a monthly log return would take on a &lt;em&gt;specific&lt;/em&gt; value. Because asset (or portfolio) returns are continuous, the probability that a monthly log return would take on any one specific value is zero.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;some-words-of-caution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some words of caution&lt;/h2&gt;
&lt;p&gt;       Viola! We now have a method, though an imperfect one, for estimating probabilities of asset returns. The empirical approach is not without its limitations. For instance, it is usually the case that a large amount of data is needed to accurately estimate a distribution nonparametrically, especially a continuous one. In this post, we employ of sample of 103 monthly returns from 2012 to 2021. For better accuracy, we could certainly switch to weekly or even daily frequency. The availability of historical data also ensures that we can expand our sample easily.&lt;/p&gt;
&lt;p&gt;       Still, some questions remain as we often need to make assumptions in order to interpolate between observed values of returns (What if we have yearly, weekly, or even daily frequencies?) and extrapolate outside the observed data range (What about the probabilities of returns intervals that are beyond the minimum and maximum values of our sample?). In addition, the reliability and convergence rate of the empirical approach in multivariate analyses decrease as data become more scattered in higher dimensions. Fortunately, in the context of asset returns, we often find ourselves in the case of univariate analysis.&lt;/p&gt;
&lt;p&gt;       In short, there is no perfect approach to estimating probabilities of asset returns. And, in reality, the focus is often placed on analyzing portfolio returns. To this end, there are are many alternative ways to quantify uncertainty outside of simple probabilities— value-at-risk, expected shortfalls, downside deviation, etc. In this post, we have demonstrated that perhaps no approach is ever completely &lt;em&gt;correct&lt;/em&gt; but we must always try our best to do what we can to bring value. Fortunately, there is a variety of topics to cover in those avenues and we will certainly tackle them in future posts.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Asset Returns in R Using ggplot2 and highcharter</title>
      <link>https://www.kenwuyang.com/en/post/visualizing-asset-returns/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.kenwuyang.com/en/post/visualizing-asset-returns/</guid>
      <description>
&lt;script src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-asset-returns-in-xts&#34;&gt;Visualization Asset returns in xts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-log-returns-highcharts&#34;&gt;Monthly log returns highcharts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-log-returns-histograms&#34;&gt;Monthly log returns Histograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualizations-in-the-tidyvserse&#34;&gt;Visualizations in the tidyvserse&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histogram&#34;&gt;Histogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#densty&#34;&gt;Densty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;visualization-asset-returns-in-xts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization Asset returns in xts&lt;/h2&gt;
&lt;p&gt;       In this post, we will explore some visualizations of asset returns. Similar to the previous &lt;a href=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/&#34;&gt;post&lt;/a&gt;, we import our data from Yahoo Finance. The five assets under examination are Exchange Traded Funds, which are funds that can be traded on an exchange like a stock. Exchange-traded funds are a type of investment fund that offers the best attributes of two popular assets: they have the diversification benefits of mutual funds and the ease with which stocks are traded. We will take a sample of daily prices from 2012-12-31 to 2021-7-31, converting them to monthly returns. This leaves us with a sample of 103 monthly returns. If you are interested in exploring a different sample, you could expand or shorten the time horizon upon importing the data. The visualization methods in this post are agnostic to the underlying data; the interpretation of these plots, however, will be different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a vector of ticker symbols
symbols &amp;lt;- c(&amp;quot;SPY&amp;quot;, &amp;quot;EFA&amp;quot;, &amp;quot;DIA&amp;quot;, &amp;quot;QQQ&amp;quot;, &amp;quot;AGG&amp;quot;)
# Load data from 2012 to today
prices &amp;lt;- quantmod::getSymbols(
  Symbols = symbols,
  src = &amp;quot;yahoo&amp;quot;,
  from = &amp;quot;2012-12-31&amp;quot;,
  to = &amp;quot;2021-7-31&amp;quot;,
  auto.assign = TRUE,
  warnings = FALSE
) %&amp;gt;%
  # The map function takes an anonymous function and will return a list of five
  # The Ad() function extracts the adjusted price series for each ETF
  purrr::map(.f = ~ quantmod::Ad(get(x = .x))) %&amp;gt;%
  # Use reduce() to merge the elements of .x consecutively
  purrr::reduce(.f = merge) %&amp;gt;%
  # Use a replacement function to set column names as ticker symbols
  # This function is in prefix form
  # It is equivalent to colnames(x = prices) &amp;lt;- value
  `colnames&amp;lt;-`(value = symbols)
# Remove all objects but price series and ticker symbol vector
rm(list = setdiff(x = ls(), y = c(&amp;quot;prices&amp;quot;, &amp;quot;symbols&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;       Since we will not be aggregating asset returns to compute portfolio returns, we choose log returns, i.e., the continuously compounded rate of returns, over the simple returns. Continuously compounded rate of returns should be used in statistical analysis (and visualizations) because unlike simple returns they are not positively biased. In addition, we opt to convert daily prices to monthly returns by finding the relative change of prices between the last day of each month. We could have easily chosen to use the first day of each month, and the values of the monthly returns will be different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Keep only the last price reading of each month
asset_returns_xts &amp;lt;- xts::to.monthly(
  x = prices,
  drop.time = TRUE,
  indexAt = &amp;quot;lastof&amp;quot;,
  OHLC = FALSE
) %&amp;gt;%
  # Compute log returns
  PerformanceAnalytics::Return.calculate(method = &amp;quot;log&amp;quot;) %&amp;gt;%
  # Drop the first row since we lose one observation in 12/31/2012
  stats::na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monthly-log-returns-highcharts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly log returns highcharts&lt;/h3&gt;
&lt;p&gt;       The &lt;code&gt;highcharter&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/highcharter/highcharter.pdf&#34;&gt;package&lt;/a&gt; is a wrapper for the “Highcharts” Library, which has an amazing visualization infrastructure for time series and financial data. The &lt;code&gt;highcharter&lt;/code&gt; package houses functions that accept xts objects (R’s time series object class) as arguments, making it seamless to move from time series data to visualizations. The plot below displays the line chart for a subset of the ETF’s. We could have easily plotted all five ETF’s on the same line chart, but it would be harder for our eyes to compare, contrast, and identify patterns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Crate Highstock widget
highchart(type = &amp;quot;stock&amp;quot;) %&amp;gt;%
  # Add chart main title
  hc_title(text = &amp;quot;Monthly Log Returns for SPY, QQQ, DIA&amp;quot;) %&amp;gt;%
  # Add returns series to highchart objects
  # We use &amp;quot;symbols&amp;quot; to reference series since we may need to add/remove ETF&amp;#39;s in the future
  # Use matrix sub-setting and character indexing to select returns by column
  hc_add_series(
    data = asset_returns_xts[, symbols[[1]]],
    name = symbols[[1]]
  ) %&amp;gt;%
  hc_add_series(
    data = asset_returns_xts[, symbols[[4]]],
    name = symbols[[4]]
  ) %&amp;gt;%
  hc_add_series(
    data = asset_returns_xts[, symbols[[3]]],
    name = symbols[[3]]
  ) %&amp;gt;%
  # Add theme to highchart object
  # More themes to be found in the vignette
  hc_add_theme(hc_thm = hc_theme_flat()) %&amp;gt;%
  # Navigator
  hc_navigator(enabled = TRUE) %&amp;gt;%
  # Scrollbar
  hc_scrollbar(enabled = TRUE) %&amp;gt;%
  # Exporting
  hc_exporting(enabled = TRUE) %&amp;gt;%
  # Add legend
  hc_legend(enabled = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Line_chart.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The navigator is a small series below the main series, displaying a view of the entire data set. It provides tools to zoom in and out on parts of the data as well as panning across the data-set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The scroll-bar is a means of panning over the X axis of a stock chart. Scroll-bars can also be applied to other types of axes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;Highcharter&lt;/code&gt; plots are highly interactive; that is, an user can hover over the line chart to interact with it. However, due to internal conflicts with &lt;code&gt;blogdown&lt;/code&gt; themes, this &lt;code&gt;Highcharter&lt;/code&gt; plot cannot be rendered dynamically. Please navigate to this &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;link&lt;/a&gt; to view the interactive version of this line chart.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-log-returns-histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monthly log returns Histograms&lt;/h3&gt;
&lt;p&gt;       Create a function that returns a uni-variate histogram given a series of returns. The function will also take several other arguments— an xts object of returns, a vector of ticker symbols, a symbol index, and a color for plotting. Internally, the function creates a list of histogram components: counts, density, bin breaks, etc. Then, the function &lt;code&gt;hchart()&lt;/code&gt; is called on the histogram list object to plot the uni-variate histogram; this is the final output of the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc_hist_fun &amp;lt;- function(xts_obj, tickers, symbol_index, color) {

  # Check for invalid input
  if (!is.xts(xts_obj) || !rlang::is_character(color) || !rlang::is_character(tickers)) {
    rlang::abort(
      message = &amp;quot;Invalid input type for xts_object, tickers, and/or color arguments&amp;quot;
    )
  }

  # Create histogram list object with 6 elements
  hc_hist &amp;lt;- graphics::hist(xts_obj[, tickers[[symbol_index]]],
    breaks = &amp;quot;Freedman-Diaconis&amp;quot;,
    plot = FALSE
  )

  # Call hchart on the histogram list object
  hchart(object = hc_hist, color = color) %&amp;gt;%
    hc_title(
      text =
        paste(tickers[[symbol_index]], &amp;quot;Log Returns Distribution&amp;quot;, sep = &amp;quot; &amp;quot;)
    ) %&amp;gt;%
    hc_add_theme(hc_thm = hc_theme_flat()) %&amp;gt;%
    hc_exporting(enabled = TRUE) %&amp;gt;%
    hc_legend(enabled = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Now, we utilize the functional programming tool from &lt;code&gt;purrr&lt;/code&gt; to apply the function above to each of the five uni-variate returns series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map the histogram function to each of the returns series
list_of_histogram &amp;lt;- purrr::map(
  .x = 1:5,
  .f = ~ hc_hist_fun(
    xts_obj = asset_returns_xts,
    tickers = symbols,
    symbol_index = .x,
    color = &amp;quot;cornflowerblue&amp;quot;
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;SPY.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;EFA.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DIA.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;QQQ.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;AGG.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please navigate to this &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;link&lt;/a&gt; to view the interactive version of these histograms.&lt;/p&gt;
&lt;p&gt;       As can be seen, most of our assets are negatively skewed, indicating that there were a few &lt;em&gt;really bad months&lt;/em&gt;. For the iShares Core US Aggregate Bond ETF (AGG), most months have returns that may be statistically indistinguishable from zero. From a sheer numbers perspective, investors who wish to maximize gains may consider such an asset undesirable. However, other performance factors such as risk, diversifier effect, and time horizon are important. The iShares Core US Aggregate Bond ETF seeks to track the investment results of an index composed of the total U.S. investment-grade bond market. And we expect bonds to produce lower returns for investors because they are also considered less volatile than stocks.&lt;/p&gt;
&lt;p&gt;The Invesco QQQ Trust Series 1 ETF (QQQ) stands out as a strong performing asset. Out of 103 months, about &lt;span class=&#34;math inline&#34;&gt;\(22%\)&lt;/span&gt; of its monthly returns fall between &lt;span class=&#34;math inline&#34;&gt;\(2\%\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(4\%\)&lt;/span&gt;. Hover over the histograms &lt;a href=&#34;https://rpubs.com/yangwu1227/interactive_plots&#34;&gt;here&lt;/a&gt; to see the counts and break points of the returns distribution.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations-in-the-tidyvserse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations in the tidyvserse&lt;/h2&gt;
&lt;p&gt;       Similarly, we could plot our asset returns using &lt;code&gt;ggplot2&lt;/code&gt;, which implements the layered grammar of graphics approach. For efficiency, we will convert the xts object into the &lt;em&gt;long tidy&lt;/em&gt; format that the tidyverse functions are designed to work well with. For another method of data importation that automatically converts the data into a &lt;em&gt;tidy&lt;/em&gt; format, please see this &lt;a href=&#34;https://www.kenwuyang.com/en/post/portfolio-optimization-and-returns/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;asset_returns_dplyr &amp;lt;- xts::to.monthly(
  x = prices,
  drop.time = TRUE,
  indexAt = &amp;quot;lastof&amp;quot;,
  OHLC = FALSE
) %&amp;gt;%
  # Create a new &amp;quot;date&amp;quot; variable by extracting the date indices from the xts object
  base::data.frame(&amp;quot;date&amp;quot; = zoo::index(x = .)) %&amp;gt;%
  # Coerce to tibble
  dplyr::as_tibble() %&amp;gt;%
  # Create a key column &amp;quot;asset&amp;quot; that contains the column names, i.e. ticker symbols
  # Create a value column that contains all the cells associated with each column
  # We convert to long format since it is easier to compute the returns using lag()
  tidyr::pivot_longer(
    cols = 1:5,
    names_to = &amp;quot;asset&amp;quot;,
    values_to = &amp;quot;returns&amp;quot;
  ) %&amp;gt;%
  # Group by ticker symbol
  dplyr::group_by(asset) %&amp;gt;%
  # Compute log returns manually
  dplyr::mutate(
    &amp;quot;returns&amp;quot; = (
      log(x = returns, base = exp(1)) - log(x = dplyr::lag(x = returns), base = exp(1))
    )
  ) %&amp;gt;%
  # Remove NA_double_ readings for 12/31/2021
  na.omit()
# See the results
head(asset_returns_dplyr, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 3
# Groups:   asset [5]
  date       asset  returns
  &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
1 2013-01-31 SPY    0.0499 
2 2013-01-31 EFA    0.0366 
3 2013-01-31 DIA    0.0593 
4 2013-01-31 QQQ    0.0264 
5 2013-01-31 AGG   -0.00623&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;histogram&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histogram&lt;/h3&gt;
&lt;p&gt;       Here are the histograms. Notice that we can either overlay the histograms on top of each other or show them in separate panels. I recommend using the panel approach for studying the shapes (spread, central tendency, skewness, tailed-ness, etc.) of the uni-variate distributions, and employ the overlaying histograms for making comparisons between these distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute Freedman-Diaconis bin numbers
bins_fd &amp;lt;- function(vec) {
  diff(range(vec)) / (2 * IQR(vec) / length(vec)^(1 / 3))
}
# Histogram
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histogram with panels
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  facet_wrap(~asset) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;densty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Densty&lt;/h3&gt;
&lt;p&gt;       We could also plot the probability density functions of these historical returns. Take a look at the y-axis of these plots and compare them to those of the histograms. This is an important distinction between these otherwise similar visualizations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Density plot
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_density(mapping = aes(color = asset)) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Density plot with panels
ggplot(data = asset_returns_dplyr, mapping = aes(x = returns)) +
  geom_density(mapping = aes(color = asset)) +
  geom_histogram(
    alpha = 0.5,
    mapping = aes(fill = asset),
    bins = bins_fd(asset_returns_dplyr[[&amp;quot;returns&amp;quot;]])
  ) +
  facet_wrap(~asset) +
  ggtitle(&amp;quot;Distributions of Monthly Log Returns&amp;quot;) +
  theme(
    panel.background = element_rect(fill = &amp;quot;grey97&amp;quot;),
    panel.grid = element_blank(),
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.kenwuyang.com/en/post/visualizing-asset-returns/index.en_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;       The smoothed densities can be useful for estimating the probabilities of returns, or an infinitesimal interval of returns, to be mathematically precise. Take caution when interpreting these kernel densities as it is important to understand that &lt;strong&gt;the heights of the curve (the y-axis) do not represent probabilities.&lt;/strong&gt; The y-axis in a density plot is the probability density function for the kernel density estimation. To convert to an actual probability, we need to find the area under the curve for a specific interval of returns. It is generally difficult to estimate probabilities from densities and we will have to tackle this problem in another post.&lt;/p&gt;
&lt;p&gt;       For now, we have equipped ourselves with some nice visualization techniques in R. These are not the only ways to visualize financial data by any means. In future posts, I will explore other aspects of financial analytics and portfolio analytics.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
